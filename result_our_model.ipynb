{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b1e4aaa269f42a1b452c8e16a531faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f44be6e500144adbf9ac23b4fc281ff",
              "IPY_MODEL_4ae4a6213e594fe4ab87e64dfb2dfbd2",
              "IPY_MODEL_bdc325b0304e4d3e86696fa18f87b7a5"
            ],
            "layout": "IPY_MODEL_67000fc1f945470c86326f682723b942"
          }
        },
        "8f44be6e500144adbf9ac23b4fc281ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483b484e1e5a4ab5b0db193a1eb3a752",
            "placeholder": "​",
            "style": "IPY_MODEL_257c73981c484558a681fe3cb04d7747",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4ae4a6213e594fe4ab87e64dfb2dfbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31dc782270e8475e90cf7186483fceca",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_529d59e38e5141aa94523e9eba4fced4",
            "value": 48
          }
        },
        "bdc325b0304e4d3e86696fa18f87b7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5875f60bf0374be7a1329428b4dca9e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f8ce9509299a4db894199c41c23b9103",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.38kB/s]"
          }
        },
        "67000fc1f945470c86326f682723b942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483b484e1e5a4ab5b0db193a1eb3a752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257c73981c484558a681fe3cb04d7747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31dc782270e8475e90cf7186483fceca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529d59e38e5141aa94523e9eba4fced4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5875f60bf0374be7a1329428b4dca9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ce9509299a4db894199c41c23b9103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "613c9619666a4a45b870d3f2d5815c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_613add5fb0b94c2fbcaaf6e818997a69",
              "IPY_MODEL_92b5a4d1314a4de2ad10471ebebdd217",
              "IPY_MODEL_793db10ccf144c10b7eaad8e01cc62ed"
            ],
            "layout": "IPY_MODEL_64ed9078a3c64a9f877b84083d1d27af"
          }
        },
        "613add5fb0b94c2fbcaaf6e818997a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0181fbba39b54e499114f48d5d098b60",
            "placeholder": "​",
            "style": "IPY_MODEL_81f5d5c720a340cc902be5032a21cda3",
            "value": "config.json: 100%"
          }
        },
        "92b5a4d1314a4de2ad10471ebebdd217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041878effed0407082f6d005b8e107ef",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c02fed0594a44775a812820ac61ba679",
            "value": 570
          }
        },
        "793db10ccf144c10b7eaad8e01cc62ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7f8c1f9def4c128aafab95273fda41",
            "placeholder": "​",
            "style": "IPY_MODEL_60b99912686d43f1adc370cbc2a0dcb1",
            "value": " 570/570 [00:00&lt;00:00, 68.3kB/s]"
          }
        },
        "64ed9078a3c64a9f877b84083d1d27af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0181fbba39b54e499114f48d5d098b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f5d5c720a340cc902be5032a21cda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "041878effed0407082f6d005b8e107ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02fed0594a44775a812820ac61ba679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e7f8c1f9def4c128aafab95273fda41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b99912686d43f1adc370cbc2a0dcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b517c8ebf62845d99a37b1ef4afaefff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_065520d93ec34a16a7654dab1ec37939",
              "IPY_MODEL_d34c1181bfc741c4ab99334e450b210d",
              "IPY_MODEL_6bf447e13c0041bf8ca55b530f9f7ee9"
            ],
            "layout": "IPY_MODEL_8ec64f55b39b402185f00fbc74785ec5"
          }
        },
        "065520d93ec34a16a7654dab1ec37939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_931bab3a75484b28933cb20be22689ee",
            "placeholder": "​",
            "style": "IPY_MODEL_9bd5da951e594a2e9688a7b95af36c1e",
            "value": "vocab.txt: 100%"
          }
        },
        "d34c1181bfc741c4ab99334e450b210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8501d6ba016448e9ff0998f39353da7",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10ee829191824bc994dac22715c13783",
            "value": 231508
          }
        },
        "6bf447e13c0041bf8ca55b530f9f7ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da39dc695fe147a690adaf6c09b61724",
            "placeholder": "​",
            "style": "IPY_MODEL_8c2606724754402c84b12d8e1cad14bd",
            "value": " 232k/232k [00:00&lt;00:00, 13.4MB/s]"
          }
        },
        "8ec64f55b39b402185f00fbc74785ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931bab3a75484b28933cb20be22689ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd5da951e594a2e9688a7b95af36c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8501d6ba016448e9ff0998f39353da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ee829191824bc994dac22715c13783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da39dc695fe147a690adaf6c09b61724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2606724754402c84b12d8e1cad14bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc986c7eb08e4ce7b512cd4b3fab0811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8941192a49ee425594558a6cf43eb793",
              "IPY_MODEL_9843de38e3224bccad34c1bdee071a38",
              "IPY_MODEL_be5cece29fca441bba80ff8ea07cb36c"
            ],
            "layout": "IPY_MODEL_dcaa17a91a874c98ad3b6baadfcb2ec2"
          }
        },
        "8941192a49ee425594558a6cf43eb793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d76dea8d42b49668fb4bc41bff3b363",
            "placeholder": "​",
            "style": "IPY_MODEL_899de7e330124fa5bfd88c7bcf1d8741",
            "value": "tokenizer.json: 100%"
          }
        },
        "9843de38e3224bccad34c1bdee071a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc939cd396714f1e8bb31ffb830a4eda",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f37d604ef7c1487e9b516a9d83c541b9",
            "value": 466062
          }
        },
        "be5cece29fca441bba80ff8ea07cb36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96848e17815481082ace5138314aa78",
            "placeholder": "​",
            "style": "IPY_MODEL_6adda259908141668cdfe33eaf4abcaa",
            "value": " 466k/466k [00:00&lt;00:00, 2.12MB/s]"
          }
        },
        "dcaa17a91a874c98ad3b6baadfcb2ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d76dea8d42b49668fb4bc41bff3b363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899de7e330124fa5bfd88c7bcf1d8741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc939cd396714f1e8bb31ffb830a4eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37d604ef7c1487e9b516a9d83c541b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f96848e17815481082ace5138314aa78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6adda259908141668cdfe33eaf4abcaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22444258554740d49673ff2d45f89d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67b0a191196d4eaea0a152eb3e1854a2",
              "IPY_MODEL_dae77240d5fe4f27b8a4f5ffae6beef4",
              "IPY_MODEL_0595b200744045aebf5fd7bbd89d7a26"
            ],
            "layout": "IPY_MODEL_ad7ad8672ace41acace0a5cc19f0bd60"
          }
        },
        "67b0a191196d4eaea0a152eb3e1854a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5ba8c79fbb4870be89f8503606bf01",
            "placeholder": "​",
            "style": "IPY_MODEL_e579d30bf21b42a998b00985f417390a",
            "value": "model.safetensors: 100%"
          }
        },
        "dae77240d5fe4f27b8a4f5ffae6beef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5396e080a38c41c8b91e12b8b5c8b055",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_975ea9ea2a6d45669420b1cfd9a1e2e6",
            "value": 440449768
          }
        },
        "0595b200744045aebf5fd7bbd89d7a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_774fa74fbe7c4e6d81ee85f73612824a",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb7d529cd834b5a9b0cef9f723aaf83",
            "value": " 440M/440M [00:00&lt;00:00, 483MB/s]"
          }
        },
        "ad7ad8672ace41acace0a5cc19f0bd60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5ba8c79fbb4870be89f8503606bf01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e579d30bf21b42a998b00985f417390a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5396e080a38c41c8b91e12b8b5c8b055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975ea9ea2a6d45669420b1cfd9a1e2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "774fa74fbe7c4e6d81ee85f73612824a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb7d529cd834b5a9b0cef9f723aaf83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZhXQBoQonUj",
        "outputId": "f16c7dd8-baaf-4ce6-f1da-2e0b957b58e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wqBDNY6lvjg",
        "outputId": "f24044f9-2f45-4646-e3cd-b0af801b66f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT model used: bert-base-uncased\n",
            "Hierarchical LSTM: Sentence-level hidden dim 256, Sample-level hidden dim 512, Dropout 0.3\n",
            "Projector output dimension (Stream processor audio/video input): 1024\n",
            "Max BERT length for context/punchline part: 512\n",
            "Stream processor internal text LSTM hidden size: 256\n",
            "Attention token dimension: 32, Tokens per modality: 16\n",
            "Stream processor attention heads: 1, Stream processor text FC Dropout rate: 0.3\n",
            "Final fusion stage attention heads: 1, MLP hidden dimension: 256\n",
            "Training parameters: Batch size 16, Learning rate 8e-05, Epochs 8\n",
            "Contrastive loss: Temperature 0.5, Weight 0.03\n",
            "\n",
            " !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \n",
            "\n",
            "Loading raw data pickle files...\n",
            "Raw data loading complete.\n",
            "Extracting features and labels...\n",
            "Feature and label extraction complete.\n",
            "Structuring data for new dataset format (context/punchline split)...\n",
            "Data structuring complete.\n",
            "Initializing BERT tokenizer...\n",
            "Actual BERT hidden size: 768\n",
            "Creating CustomFeatureDatasetContextPunchline instances...\n",
            "Dataloaders created. Train batches: 475, Val batches: 62, Test batches: 63\n",
            "\n",
            "--- Starting processing for model config: AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA... Total 8 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.7756, BCE: 0.6656, SimCLR: 3.6674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 0.6255, Accuracy: 0.6602, F1: 0.7213\n",
            "Epoch 1 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA): New best validation accuracy: 0.6602 (F1: 0.7213). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.7000, BCE: 0.5982, SimCLR: 3.3956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 0.6158, Accuracy: 0.6694, F1: 0.6029\n",
            "Epoch 2 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA): New best validation accuracy: 0.6694 (F1: 0.6029). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.6602, BCE: 0.5616, SimCLR: 3.2874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 0.5842, Accuracy: 0.6888, F1: 0.6852\n",
            "Epoch 3 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA): New best validation accuracy: 0.6888 (F1: 0.6852). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.5996, BCE: 0.5017, SimCLR: 3.2648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 0.6821, Accuracy: 0.6745, F1: 0.6251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.4191, BCE: 0.3208, SimCLR: 3.2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 0.8775, Accuracy: 0.6551, F1: 0.6150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.1975, BCE: 0.0991, SimCLR: 3.2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 1.2554, Accuracy: 0.6439, F1: 0.6520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.1325, BCE: 0.0348, SimCLR: 3.2585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 1.4936, Accuracy: 0.6531, F1: 0.6488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Train Avg Loss: 0.1110, BCE: 0.0134, SimCLR: 3.2509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Val Avg BCE: 1.5601, Accuracy: 0.6469, F1: 0.6342\n",
            "Training for AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA complete. Best validation accuracy for this config: 0.6888 (corresponding F1: 0.6852)\n",
            "\n",
            "Starting test phase for AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA...\n",
            "Loading best model state from best_model_AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA) Test Avg BCE: 0.5723, Accuracy: 0.7032, F1: 0.7065\n",
            "Final test results for AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA -> Avg BCE Loss: 0.5723, Accuracy: 0.7032, F1: 0.7065\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA\n",
            "  Best Validation Accuracy: 0.6888 (Corresponding Val F1: 0.6852)\n",
            "  Test Set Accuracy: 0.7032\n",
            "  Test Set F1 Score: 0.7065\n",
            "  Test Set Loss: 0.5723\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Single-head attention, new loss function, remove last self-attention layer\n",
        "# Import pickle module, used for serializing and deserializing Python object structures\n",
        "import pickle\n",
        "# Import numpy library, used for scientific computing, especially array operations\n",
        "import numpy as np\n",
        "# Import PyTorch library, an open-source machine learning framework\n",
        "import torch\n",
        "# Import PyTorch's neural network module\n",
        "import torch.nn as nn\n",
        "# Import PyTorch's neural network functional library\n",
        "import torch.nn.functional as F\n",
        "# Import Dataset and DataLoader classes from PyTorch, used for data loading\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Import auto tokenizer, auto model, and learning rate scheduler from transformers library\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "# Import AdamW optimizer from PyTorch\n",
        "from torch.optim import AdamW\n",
        "# Import accuracy and F1 score calculation functions from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Import copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "import copy\n",
        "# Import tqdm library, used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# Import os module, used for file path operations, etc.\n",
        "import os\n",
        "# Import functions for handling variable-length sequences from PyTorch's RNN utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "# Google Drive mount path (example)\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\"\n",
        "# Base path where feature files are located (example)\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\")\n",
        "\n",
        "# Path to the dataset split file\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "# Path to the OpenFace feature file\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "# Path to the COVAREP feature file\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "# Path to the language feature file\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "# Path to the humor label file\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "\n",
        "# Audio word-level feature dimension constant\n",
        "_AUDIO_WORD_DIM_CONST = 81\n",
        "# Video word-level feature dimension constant\n",
        "_VIDEO_WORD_DIM_CONST = 371\n",
        "# Hidden dimension of sentence-level LSTM in Hierarchical LSTM (Modified to align with Script_B's configuration idea)\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "# Hidden dimension of sample-level LSTM in Hierarchical LSTM (also its output dimension, projector layer input dimension) (Modified to align with Script_B)\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "# Helper function to load pickle files\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        # Open file in binary read mode\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            # Load pickle data\n",
        "            return pickle.load(f)\n",
        "    # Handle possible UnicodeDecodeError\n",
        "    except UnicodeDecodeError:\n",
        "        # If UnicodeDecodeError occurs, try opening with latin1 encoding\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    # Handle other possible exceptions\n",
        "    except Exception as e:\n",
        "        print(f'Cannot load data {pickle_file}: {e}')\n",
        "        # Raise exception\n",
        "        raise\n",
        "\n",
        "# Helper function to safely prepare feature data for np.array()\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    # If input data is None, return an empty list\n",
        "    if feature_data is None: return []\n",
        "    # If input data is a numpy array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's an empty numpy array, return an empty list\n",
        "        if feature_data.size == 0: return []\n",
        "        # Return non-empty numpy array\n",
        "        return feature_data\n",
        "    # If input data is a list\n",
        "    if isinstance(feature_data, list):\n",
        "        # If it's an empty list, return an empty list\n",
        "        if not feature_data: return []\n",
        "        # Return non-empty list\n",
        "        return feature_data\n",
        "    # Other unexpected types, return an empty list (can add a warning)\n",
        "    return []\n",
        "\n",
        "# Function to extract features and labels\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    # Initialize lists to store various features and labels\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "    # Iterate through the ID list\n",
        "    for hid in id_list:\n",
        "        # Add punchline text\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        # Add context text list\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP (audio) feature processing\n",
        "        # Prepare COVAREP features for the punchline\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline audio features to a float32 numpy array and add\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp, dtype=np.float32))\n",
        "        # Process context COVAREP features (one feature array per sentence)\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp, dtype=np.float32))\n",
        "        # Add the list of processed context audio features\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace (video) feature processing\n",
        "        # Prepare OpenFace features for the punchline\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline video features to a float32 numpy array and add\n",
        "        of_p_list.append(np.array(prepared_punchline_of, dtype=np.float32))\n",
        "        # Process context OpenFace features\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of, dtype=np.float32))\n",
        "        # Add the list of processed context video features\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        # Add labels\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    # Return all extracted features and labels, specifying the dtype for numpy arrays\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object), np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object), np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object), np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "# Prepare data for the new dataset structure: output a list of samples, each sample is a dictionary containing all sentence features/texts\n",
        "# Among them, the features/text of the punchline will be the last item in the corresponding modality list\n",
        "def concatenate_multimodal_data_for_dataset(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    # Get the number of samples (based on the number of context sentences)\n",
        "    num_samples = len(cs)\n",
        "    # List to store all sample data\n",
        "    all_samples_data = []\n",
        "    # Iterate through each sample\n",
        "    for i in range(num_samples):\n",
        "        # Data dictionary for a single sample, containing 'audio', 'video', 'text' keys\n",
        "        sample_data = {'audio': [], 'video': [], 'text': []}\n",
        "\n",
        "        # Audio data processing\n",
        "        # Extract context audio features, ensuring they are valid numpy arrays (word count > 0, correct dimension)\n",
        "        current_sample_audio = [s for s in list(cvp_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _AUDIO_WORD_DIM_CONST]\n",
        "        # Get punchline audio features\n",
        "        punchline_audio = cvp_p[i]\n",
        "        # Append punchline audio features to the end of the list, if valid\n",
        "        if isinstance(punchline_audio, np.ndarray) and punchline_audio.ndim == 2 and punchline_audio.shape[0] > 0 and punchline_audio.shape[1] == _AUDIO_WORD_DIM_CONST:\n",
        "            current_sample_audio.append(punchline_audio)\n",
        "        # If the current audio list is empty (both context and punchline are invalid or missing), add a placeholder for the punchline (single sample, correct dimension)\n",
        "        elif not current_sample_audio:\n",
        "            current_sample_audio.append(np.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        # Store the processed audio feature list into the sample data dictionary\n",
        "        sample_data['audio'] = current_sample_audio\n",
        "\n",
        "        # Video data processing (logic same as audio)\n",
        "        current_sample_video = [s for s in list(of_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _VIDEO_WORD_DIM_CONST]\n",
        "        punchline_video = of_p[i]\n",
        "        if isinstance(punchline_video, np.ndarray) and punchline_video.ndim == 2 and punchline_video.shape[0] > 0 and punchline_video.shape[1] == _VIDEO_WORD_DIM_CONST:\n",
        "            current_sample_video.append(punchline_video)\n",
        "        elif not current_sample_video:\n",
        "            current_sample_video.append(np.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        sample_data['video'] = current_sample_video\n",
        "\n",
        "        # Text data processing\n",
        "        # Extract context sentence text list\n",
        "        current_sample_text = [s for s in list(cs[i]) if isinstance(s, str)]\n",
        "        # Get punchline text\n",
        "        punchline_text_str = ps[i]\n",
        "        # If the punchline text is a string, append it\n",
        "        if isinstance(punchline_text_str, str):\n",
        "            current_sample_text.append(punchline_text_str)\n",
        "        # If the current text list is empty (both context and punchline are invalid or missing), add an empty string as a punchline placeholder\n",
        "        elif not current_sample_text:\n",
        "            current_sample_text.append(\"\")\n",
        "        sample_data['text'] = current_sample_text\n",
        "\n",
        "        # Add the current sample's data dictionary to the total list\n",
        "        all_samples_data.append(sample_data)\n",
        "    # Return the list containing all sample data\n",
        "    return all_samples_data\n",
        "\n",
        "\n",
        "# --- Dataset Class: Modified for Context/Punchline Splitting ---\n",
        "class CustomFeatureDatasetContextPunchline(Dataset):\n",
        "    # Initialization function\n",
        "    def __init__(self, list_of_sample_data_dicts, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len_for_part=512,\n",
        "                 audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST):\n",
        "\n",
        "        # List of sample data dictionaries (each element is a sample, containing 'audio', 'video', 'text' keys)\n",
        "        self.list_of_sample_data_dicts = list_of_sample_data_dicts\n",
        "        # List of labels, converted to torch.long type\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long)\n",
        "        # BERT tokenizer\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        # Maximum BERT length for each part (context/punchline)\n",
        "        self.max_bert_len_for_part = max_bert_len_for_part\n",
        "        # Audio word feature dimension\n",
        "        self.audio_word_dim = audio_word_dim\n",
        "        # Video word feature dimension\n",
        "        self.video_word_dim = video_word_dim\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    # Helper function to tokenize the text part\n",
        "    def _tokenize_text_part(self, text_sentences_list):\n",
        "        # If the text list is empty\n",
        "        if not text_sentences_list:\n",
        "            # If the tokenizer has a pad token, use it, otherwise an empty string might be tokenized into special tokens\n",
        "            processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "        else:\n",
        "            # Join all sentences in the sentence list with spaces\n",
        "            processed_text = \" \".join(text_sentences_list)\n",
        "            # If it's only whitespace or empty after joining\n",
        "            if not processed_text.strip():\n",
        "                processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "\n",
        "        # Call the tokenizer to tokenize\n",
        "        bert_inputs = self.tokenizer(\n",
        "            processed_text, add_special_tokens=True, return_attention_mask=True, # Add special tokens, return attention_mask\n",
        "            max_length=self.max_bert_len_for_part, padding='max_length', truncation=True, # Max length, pad to max length, truncate\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        )\n",
        "        # Return input_ids and attention_mask, and remove the batch dimension (because this is single sample processing)\n",
        "        return bert_inputs[\"input_ids\"].squeeze(0), bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    # Helper function to process audio/video parts\n",
        "    # all_sentences_features_for_sample: List of all sentence features for the entire sample (list of numpy arrays)\n",
        "    # part_sentences_indices: Indices in the total sentence list that the current part (context or punchline) should contain\n",
        "    # word_dim: Word feature dimension for audio or video\n",
        "    def _process_av_part(self, all_sentences_features_for_sample, part_sentences_indices, word_dim):\n",
        "        # List to store feature tensors of all sentences in this part\n",
        "        part_features_list = []\n",
        "        # If the sample itself does not have any sentence features (e.g., the entire sample is empty)\n",
        "        if not all_sentences_features_for_sample:\n",
        "            # Add a placeholder tensor (1 word, specified dimension)\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "            return part_features_list\n",
        "\n",
        "        # Iterate through the sentence indices of the specified part\n",
        "        for sent_idx in part_sentences_indices:\n",
        "            # Ensure the index is within the valid range\n",
        "            if 0 <= sent_idx < len(all_sentences_features_for_sample):\n",
        "                # Get features of a single sentence (numpy array)\n",
        "                sent_feat = all_sentences_features_for_sample[sent_idx]\n",
        "                # Validate feature validity: is a numpy array, 2D, word count > 0, correct dimension\n",
        "                if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0 and sent_feat.shape[1] == word_dim:\n",
        "                    # Convert to PyTorch tensor and add to the list\n",
        "                    part_features_list.append(torch.as_tensor(sent_feat, dtype=torch.float32))\n",
        "\n",
        "        # If this part is empty after processing (e.g., all sentences are invalid or indices are out of range, or the specified index list is empty)\n",
        "        if not part_features_list:\n",
        "            # Add a placeholder tensor for this part\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "        return part_features_list\n",
        "\n",
        "\n",
        "    # Method to get single sample data\n",
        "    def __getitem__(self, index):\n",
        "        # Get the sample data dictionary for the current index\n",
        "        sample_data = self.list_of_sample_data_dicts[index]\n",
        "        # Audio: list of numpy arrays (sentence features)\n",
        "        audio_all_sents_raw = sample_data['audio']\n",
        "        # Video: list of numpy arrays (sentence features)\n",
        "        video_all_sents_raw = sample_data['video']\n",
        "        # Text: list of sentence strings\n",
        "        text_all_sents_str = sample_data['text']\n",
        "        # Get label\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        # Determine the total number of sentences based on the number of text sentences\n",
        "        n_total_sents = len(text_all_sents_str)\n",
        "\n",
        "        # Prepare placeholder input_ids and attention_mask for empty text parts\n",
        "        empty_ids, empty_mask = self._tokenize_text_part([])\n",
        "\n",
        "        # Case 1: If the sample has no sentences at all (n_total_sents == 0)\n",
        "        if n_total_sents == 0:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is empty/placeholder\n",
        "            pl_audio_part = self._process_av_part([], [], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = empty_ids, empty_mask\n",
        "\n",
        "        # Case 2: If there is only one sentence, treat it as only punchline, context is empty\n",
        "        elif n_total_sents == 1:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty index list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is this one sentence (index 0)\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, [0], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, [0], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[0]])\n",
        "\n",
        "        # Case 3: If there are multiple sentences, split into context and punchline\n",
        "        else:\n",
        "            # Context sentence indices: from 0 to the second to last\n",
        "            ctx_indices = list(range(n_total_sents - 1))\n",
        "            # Punchline sentence index: only the last one\n",
        "            pl_indices = [n_total_sents - 1]\n",
        "\n",
        "            # Process context part\n",
        "            ctx_audio_part = self._process_av_part(audio_all_sents_raw, ctx_indices, self.audio_word_dim)\n",
        "            ctx_video_part = self._process_av_part(video_all_sents_raw, ctx_indices, self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in ctx_indices])\n",
        "\n",
        "            # Process punchline part\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, pl_indices, self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, pl_indices, self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in pl_indices])\n",
        "\n",
        "        # Return a tuple of context data, punchline data, and label\n",
        "        return (ctx_audio_part, ctx_video_part, ctx_input_ids, ctx_attention_mask,\n",
        "                pl_audio_part, pl_video_part, pl_input_ids, pl_attention_mask,\n",
        "                label)\n",
        "\n",
        "# --- Custom Collate Function for Context/Punchline Data ---\n",
        "def custom_collate_fn_context_punchline(batch):\n",
        "    # batch is a list where each element is the tuple returned by __getitem__\n",
        "    # Unpack batch data into respective lists\n",
        "    (ctx_audio_list, ctx_video_list, ctx_ids_list, ctx_mask_list,\n",
        "     pl_audio_list, pl_video_list, pl_ids_list, pl_mask_list,\n",
        "     labels_list) = zip(*batch)\n",
        "\n",
        "    # Directly stack text IDs, masks, and labels (they are already fixed-size tensors)\n",
        "    batched_ctx_ids = torch.stack(ctx_ids_list)\n",
        "    batched_ctx_masks = torch.stack(ctx_mask_list)\n",
        "    batched_pl_ids = torch.stack(pl_ids_list)\n",
        "    batched_pl_masks = torch.stack(pl_mask_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Helper function to process a list of audio/video data for a part (e.g., context audio)\n",
        "    # part_data_list: A list of samples, where each sample is a list of sentence tensors\n",
        "    # word_dim_const: Word feature dimension of this modality\n",
        "    def _collate_av_part(part_data_list, word_dim_const):\n",
        "        # Get the number of sentences in each sample\n",
        "        sample_lengths = [len(sample) for sample in part_data_list]\n",
        "        # Maximum number of sentences in the batch, 0 if empty\n",
        "        max_sents = max(sample_lengths) if sample_lengths else 0\n",
        "\n",
        "        # Get the word count of each sentence and find the maximum word count\n",
        "        sentence_word_counts_flat = []\n",
        "        for sample in part_data_list: # Iterate through each sample\n",
        "            for sentence_tensor in sample: # Iterate through each sentence tensor in the sample\n",
        "                sentence_word_counts_flat.append(sentence_tensor.shape[0]) # Add the word count of this sentence\n",
        "        # Maximum number of words in the batch, 0 if empty\n",
        "        max_words = max(sentence_word_counts_flat) if sentence_word_counts_flat else 0\n",
        "\n",
        "        # Ensure max_words and max_sents are at least 1 to avoid zero dimensions in tensors\n",
        "        max_words = max(1, max_words)\n",
        "        max_sents = max(1, max_sents)\n",
        "\n",
        "        # Create padded feature tensor and length tensor\n",
        "        # padded_features: (batch_size, max_sentences, max_words, feature_dimension)\n",
        "        # sentence_lengths_tensor: (batch_size, max_sentences) - records the actual word count of each sentence\n",
        "        padded_features = torch.zeros(len(part_data_list), max_sents, max_words, word_dim_const)\n",
        "        sentence_lengths_tensor = torch.zeros(len(part_data_list), max_sents, dtype=torch.long)\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, sample in enumerate(part_data_list):\n",
        "            # Iterate through each sentence tensor in the sample\n",
        "            for j, sentence_tensor in enumerate(sample):\n",
        "                # Word count of the current sentence\n",
        "                num_words = sentence_tensor.shape[0]\n",
        "                # Pad only if there are words\n",
        "                if num_words > 0:\n",
        "                    # Pad features into the padded_features tensor\n",
        "                    padded_features[i, j, :num_words, :] = sentence_tensor\n",
        "                    # Record the actual word count into the sentence_lengths_tensor tensor\n",
        "                    sentence_lengths_tensor[i, j] = num_words\n",
        "        # Return padded features, list of sentence counts per sample (as tensor), and word counts per sentence tensor\n",
        "        return padded_features, torch.tensor(sample_lengths, dtype=torch.long), sentence_lengths_tensor\n",
        "\n",
        "    # Process audio and video data for context and punchline separately\n",
        "    # ctx_padded_audio: (B, S_ctx_max, W_ctx_max, D_audio)\n",
        "    # ctx_audio_sl: (B,) - Actual number of sentences per sample for context\n",
        "    # ctx_audio_ssl: (B, S_ctx_max) - Actual word count of each sentence per sample for context\n",
        "    ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl = _collate_av_part(ctx_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    ctx_padded_video, ctx_video_sl, ctx_video_ssl = _collate_av_part(ctx_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "    pl_padded_audio, pl_audio_sl, pl_audio_ssl = _collate_av_part(pl_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    pl_padded_video, pl_video_sl, pl_video_ssl = _collate_av_part(pl_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "\n",
        "    # Return all processed batch data\n",
        "    return (ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl, # Context audio (features, sample sentence count, words per sentence)\n",
        "            ctx_padded_video, ctx_video_sl, ctx_video_ssl, # Context video\n",
        "            batched_ctx_ids, batched_ctx_masks,             # Context text\n",
        "            pl_padded_audio, pl_audio_sl, pl_audio_ssl,     # Punchline audio\n",
        "            pl_padded_video, pl_video_sl, pl_video_ssl,     # Punchline video\n",
        "            batched_pl_ids, batched_pl_masks,               # Punchline text\n",
        "            batched_labels)                                 # Labels\n",
        "\n",
        "\n",
        "# --- Hierarchical LSTM Aggregator ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Hidden dimension of sentence-level LSTM\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        # Hidden dimension of sample-level LSTM\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        # Sentence-level LSTM: input word embeddings, output sentence representation\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed, output dimension will become 2*hidden_dim\n",
        "\n",
        "        # If sentence LSTM is bidirectional, the input dimension of sample LSTM needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        # Sample-level LSTM: input sentence representations, output sample representation\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sentences, max_words, word_dimension)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sentences) - actual word count per sentence\n",
        "\n",
        "        # Get the shape of the feature tensor\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "        # Final output dimension of sample LSTM (considering bidirectional case)\n",
        "        final_output_dim_sample = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "\n",
        "        # Handle the extreme case where all inputs in the batch are empty\n",
        "        # If max_sentences or max_words is 0, or batch_size is 0, or all sample_lengths are 0\n",
        "        if max_sents == 0 or max_words == 0 or batch_size == 0 or torch.all(sample_lengths == 0):\n",
        "            # Return a zero tensor with shape (batch_size, final_output_dim_sample)\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sentence dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        # If all sentences are empty (all lengths are 0)\n",
        "        if not torch.any(valid_sents_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sentence features and corresponding lengths\n",
        "        sents_features_packed_data = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed_data = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack padded sequence (length tensor needs to be moved to CPU for packing)\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed_data, sents_word_lengths_packed_data.cpu(),\n",
        "                                                batch_first=True, enforce_sorted=False)\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers*num_directions, B*S_valid, sentence_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get the actual output dimension of sentence LSTM (considering bidirectional)\n",
        "        sent_hidden_dim_actual = self.sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "        # Get the hidden state of the last time step (for unidirectional LSTM, take the last layer; for bidirectional, concatenate the last time steps of the last two layers)\n",
        "        # Output shape: (B*S_valid, sentence_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "        # Apply dropout to sentence embeddings\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Put valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Create a zero tensor with shape (B*S, actual_sentence_hidden_dim)\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        # Fill valid sentence embeddings into corresponding positions\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent), reshape to sample LSTM input format\n",
        "        sample_features_for_sample_lstm = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack padded sequence (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0 (i.e., samples with actual sentence count of 0)\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        # If all samples are empty (actual sentence counts are all 0)\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sample features and corresponding lengths\n",
        "        sample_features_packed_input_data = sample_features_for_sample_lstm[valid_sample_indices]\n",
        "        sample_lengths_packed_data = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input_data, sample_lengths_packed_data.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers*num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get the hidden state of the last time step\n",
        "        # Output shape: (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "        # Apply dropout to sample embeddings\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Put valid sample embeddings back to their original positions, use zero vectors for empty samples\n",
        "        # Create a zero tensor with shape (B, final_output_dim_sample_lstm)\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "        # Fill valid sample embeddings into corresponding positions\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "        # Return final sample embeddings\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "# --- GLU Linear Layer ---\n",
        "class GLULinear(nn.Module):\n",
        "    # Initialization function, input dimension and output dimension\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GLULinear, self).__init__()\n",
        "        # The first linear layer is followed by a GELU activation function\n",
        "        self.layer1 = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU())\n",
        "        # The second linear layer\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    # Forward propagation function\n",
        "    def forward(self, x):\n",
        "        # Element-wise multiplication of the outputs of the two linear layers\n",
        "        return self.layer1(x) * self.layer2(x)\n",
        "\n",
        "# --- Advanced Cross-Attention/Self-Attention Module ---\n",
        "class MultiHeadAttentionModule(nn.Module):\n",
        "    # Initialization function\n",
        "    # dim: feature dimension, num_heads: number of attention heads\n",
        "    def __init__(self, dim, num_heads=1):\n",
        "        super(MultiHeadAttentionModule, self).__init__()\n",
        "        # Feature dimension\n",
        "        self.dim = dim\n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.head_dim = dim // num_heads\n",
        "        # Ensure dimension is divisible by the number of heads\n",
        "        if self.head_dim * num_heads != self.dim:\n",
        "            raise ValueError(\"dim must be divisible by num_heads\")\n",
        "\n",
        "        # Linear layer to generate Key\n",
        "        self.K_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Value\n",
        "        self.V_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Query\n",
        "        self.Q_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Softmax layer, used to calculate attention weights\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        # Fully connected layer before output\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "    # Forward propagation function\n",
        "    # feat1_query is Query, feat2_key_value is Key and Value\n",
        "    # mask: optional attention mask\n",
        "    def forward(self, feat1_query, feat2_key_value, mask=None):\n",
        "        # Query shape: (batch_size, Query_sequence_length, Query_dimension)\n",
        "        B_q, N_q, C_q = feat1_query.shape\n",
        "        # Key/Value shape: (batch_size, Key/Value_sequence_length, Key/Value_dimension)\n",
        "        B_kv, N_kv, C_kv = feat2_key_value.shape\n",
        "\n",
        "        # Check if batch sizes of Query and Key/Value match\n",
        "        if B_q != B_kv: raise ValueError(f\"Batch sizes do not match: Query is {B_q}, Key/Value is {B_kv}\")\n",
        "\n",
        "        # Generate Q, K, V and adjust shape for multi-head: (batch, num_heads, sequence_length, head_dimension)\n",
        "        # Q: (B, N_q, C_q) -> (B, N_q, num_heads, head_dim) -> (B, num_heads, N_q, head_dim)\n",
        "        Q = self.Q_layer(feat1_query).reshape(B_q, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        K = self.K_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        V = self.V_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate dot product of Q and K_transpose and scale ( scaled_dot_product = (Q @ K.T) / sqrt(head_dim) )\n",
        "        # dots shape: (B, num_heads, N_q, N_kv)\n",
        "        dots = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # If a mask is provided (usually a padding mask for K,V sequence, shape B, N_kv)\n",
        "        if mask is not None:\n",
        "            # unsqueeze expands the mask to (B, 1, 1, N_kv) to match the shape of dots (B, nH, N_q, N_kv) for broadcasting\n",
        "            # Fill positions in dots where mask is 0 (i.e., padding positions) with a very small value, so their weight approaches 0 after softmax\n",
        "            dots = dots.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "\n",
        "        # Calculate attention weights (attn_weights shape: B, num_heads, N_q, N_kv)\n",
        "        attn_weights = self.attend(dots)\n",
        "        # Attention weights weighted V (out shape: B, num_heads, N_q, head_dim)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # Permute and merge multi-head results: (B, num_heads, N_q, head_dim) -> (B, N_q, num_heads, head_dim) -> (B, N_q, dim)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B_q, N_q, self.dim)\n",
        "        # Pass through output fully connected layer\n",
        "        out = self.fc_out(out)\n",
        "        # Return final output\n",
        "        return out\n",
        "\n",
        "# --- Adapted Single Stream Processor ---\n",
        "class AdaptedSingleStreamProcessor(nn.Module):\n",
        "    # Initialization function\n",
        "    # audio_video_input_dim: Input dimension after audio/video projection\n",
        "    # bert_hidden_size: BERT's hidden layer size\n",
        "    # max_bert_len_for_lstm: Maximum input sequence length expected by the internal text LSTM\n",
        "    # lstm_hidden_size: Hidden size of the internal text LSTM\n",
        "    # attention_token_dim: Dimension of attention tokens\n",
        "    # num_attention_tokens_per_modal: Number of tokens output after processing each modality\n",
        "    # active_modalities: Tuple of active modalities, e.g., ('audio', 'video', 'text')\n",
        "    # num_ca_sa_heads: Number of heads for cross-attention and self-attention modules\n",
        "    # dropout_rate: Dropout rate for the text FC part\n",
        "    def __init__(self, audio_video_input_dim, bert_hidden_size, max_bert_len_for_lstm,\n",
        "                 lstm_hidden_size, attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 active_modalities=('audio', 'video', 'text'), num_ca_sa_heads=1, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        # Number of tokens output after processing each modality\n",
        "        self.n_tokens_per_modal = num_attention_tokens_per_modal\n",
        "        # Dimension of attention tokens\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        # Maximum input sequence length expected by the internal text LSTM\n",
        "        self.max_bert_len_for_lstm_input = max_bert_len_for_lstm\n",
        "        # Active modalities\n",
        "        self.active_modalities = active_modalities\n",
        "        # Flattened feature dimension output by each modality processor (N * C)\n",
        "        self.expected_feature_dim_after_mod_proc = self.n_tokens_per_modal * self.attention_token_dim\n",
        "\n",
        "        # Audio feature processor: receives projected features, maps to NxC token representation\n",
        "        self.audio_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc) # Layer normalization\n",
        "        )\n",
        "        # Video feature processor: logic same as audio\n",
        "        self.vision_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "        # Text processing: BERT hidden state -> LSTM -> Fully connected layer -> NxC token representation\n",
        "        # Text LSTM processor\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        # Text FC processor, maps LSTM output to token representation\n",
        "        self.text_fc_processor_to_tokens = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), # Dropout layer\n",
        "            # LSTM output is (B, S_lstm, H_lstm), after reshape it's (B, S_lstm * H_lstm)\n",
        "            GLULinear(lstm_hidden_size * self.max_bert_len_for_lstm_input, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "\n",
        "        # Attention module instantiation\n",
        "        # ZA: Audio cross-attention (query is concatenation of all modalities, key/value are audio tokens)\n",
        "        self.ZA = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZV: Video cross-attention\n",
        "        self.ZV = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZT: Text cross-attention\n",
        "        self.ZT = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # SA_stream: Intra-stream self-attention\n",
        "        self.SA_stream = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # Final output dimension of this stream processor (after averaging SA output, or, dimension of a single token)\n",
        "        self.output_final_dim = attention_token_dim\n",
        "\n",
        "    # Forward propagation function\n",
        "    # audio_input_proj, vision_input_proj from Hierarchical LSTM + Projector layer (B, D_projector)\n",
        "    # text_sequence_input_bert is BERT's hidden state (B, S_bert, D_bert)\n",
        "    def forward(self, audio_input_proj, vision_input_proj, text_sequence_input_bert):\n",
        "        # Dynamically determine batch size\n",
        "        b = 0\n",
        "        if audio_input_proj is not None and audio_input_proj.nelement() > 0: b = audio_input_proj.shape[0]\n",
        "        elif vision_input_proj is not None and vision_input_proj.nelement() > 0: b = vision_input_proj.shape[0]\n",
        "        elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: b = text_sequence_input_bert.shape[0]\n",
        "\n",
        "        # Handle empty batch (all inputs are empty or None)\n",
        "        if b == 0:\n",
        "            dev = torch.device(\"cpu\") # Default device\n",
        "            # Try to get device from valid input\n",
        "            if audio_input_proj is not None and audio_input_proj.nelement() > 0: dev = audio_input_proj.device\n",
        "            elif vision_input_proj is not None and vision_input_proj.nelement() > 0: dev = vision_input_proj.device\n",
        "            elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: dev = text_sequence_input_bert.device\n",
        "\n",
        "            # Create empty flat features and stream output\n",
        "            empty_flat = torch.zeros(0, self.expected_feature_dim_after_mod_proc, device=dev)\n",
        "            empty_stream_out = torch.zeros(0, 1, self.output_final_dim, device=dev)\n",
        "            # Return empty flat features for contrastive loss and empty stream output\n",
        "            return empty_flat, empty_flat, empty_flat, empty_stream_out\n",
        "\n",
        "        # Get current device (ensure at least one valid input to determine device)\n",
        "        device = audio_input_proj.device if audio_input_proj is not None and audio_input_proj.nelement() > 0 else \\\n",
        "                 (vision_input_proj.device if vision_input_proj is not None and vision_input_proj.nelement() > 0 else \\\n",
        "                  text_sequence_input_bert.device)\n",
        "\n",
        "        # Initialize flat features for contrastive loss (audio_f_flat) and token features for attention (audio_f_tokens)\n",
        "        # Audio processing\n",
        "        audio_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        audio_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        # If audio modality is active, input is not empty, and input is not all zeros (indicates actual content)\n",
        "        if 'audio' in self.active_modalities and audio_input_proj is not None and audio_input_proj.nelement() > 0 and audio_input_proj.abs().sum() > 1e-9 :\n",
        "            audio_f_flat = self.audio_feat_processor_to_tokens(audio_input_proj) # (B, N*C)\n",
        "            audio_f_tokens = audio_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Video processing (logic same as audio)\n",
        "        vis_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        vis_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'video' in self.active_modalities and vision_input_proj is not None and vision_input_proj.nelement() > 0 and vision_input_proj.abs().sum() > 1e-9:\n",
        "            vis_f_flat = self.vision_feat_processor_to_tokens(vision_input_proj)\n",
        "            vis_f_tokens = vis_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim)\n",
        "\n",
        "        # Text processing\n",
        "        text_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        text_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0 and text_sequence_input_bert.abs().sum() > 1e-9:\n",
        "            # Get current BERT output sequence length\n",
        "            current_bert_seq_len = text_sequence_input_bert.shape[1]\n",
        "            text_sequence_input_bert_adjusted = text_sequence_input_bert\n",
        "            # Adjust BERT output sequence length to match LSTM expected input\n",
        "            if current_bert_seq_len != self.max_bert_len_for_lstm_input:\n",
        "                if current_bert_seq_len > self.max_bert_len_for_lstm_input: # Truncate if too long\n",
        "                    text_sequence_input_bert_adjusted = text_sequence_input_bert[:, :self.max_bert_len_for_lstm_input, :]\n",
        "                else: # Pad with zeros if too short\n",
        "                    padding_needed = self.max_bert_len_for_lstm_input - current_bert_seq_len\n",
        "                    # Create padding tensor (B, padding_needed, D_bert)\n",
        "                    padding_tensor = torch.zeros(b, padding_needed, text_sequence_input_bert.shape[2], device=device)\n",
        "                    # Concatenate original BERT output and padding tensor\n",
        "                    text_sequence_input_bert_adjusted = torch.cat([text_sequence_input_bert, padding_tensor], dim=1)\n",
        "\n",
        "            # Pass through text LSTM\n",
        "            lstm_output, _ = self.text_lstm_processor(text_sequence_input_bert_adjusted) # (B, S_lstm, H_lstm)\n",
        "            # Flatten LSTM output: (B, S_lstm * H_lstm)\n",
        "            text_f_flat_from_lstm = lstm_output.reshape(b, -1)\n",
        "            # Process flattened LSTM output through FC layer\n",
        "            text_f_flat = self.text_fc_processor_to_tokens(text_f_flat_from_lstm) # (B, N*C)\n",
        "            # Reshape to token form\n",
        "            text_f_tokens = text_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Collect tokens from active modalities with content\n",
        "        active_mod_token_lists = []\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(audio_f_tokens)\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(vis_f_tokens)\n",
        "        if 'text'  in self.active_modalities and text_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(text_f_tokens)\n",
        "\n",
        "        # If there are no active modalities with content\n",
        "        if not active_mod_token_lists:\n",
        "            # Return flat features and zero stream output (because there's no content for attention calculation)\n",
        "            return audio_f_flat, vis_f_flat, text_f_flat, torch.zeros(b, 1, self.output_final_dim, device=device)\n",
        "\n",
        "        # Concatenate tokens of active modalities as Query for cross-attention\n",
        "        # query_for_modality_ca shape: (B, num_active_modalities * N, C_token)\n",
        "        query_for_modality_ca = torch.cat(active_mod_token_lists, dim=1)\n",
        "\n",
        "        # Perform inter-modality cross-attention\n",
        "        # Initialize result tensor\n",
        "        res_za, res_zv, res_zt = torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca)\n",
        "        # If audio is active and has content\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9:\n",
        "            # query_for_modality_ca as Query, audio_f_tokens as Key and Value\n",
        "            res_za = self.ZA(query_for_modality_ca, audio_f_tokens)\n",
        "        # If video is active and has content\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zv = self.ZV(query_for_modality_ca, vis_f_tokens)\n",
        "        # If text is active and has content\n",
        "        if 'text' in self.active_modalities and text_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zt = self.ZT(query_for_modality_ca, text_f_tokens)\n",
        "\n",
        "        # Merge cross-attention results (element-wise addition)\n",
        "        feat_after_mod_ca = res_za + res_zv + res_zt\n",
        "        # Intra-stream self-attention, with residual connection\n",
        "        # feat_after_mod_ca as Query, Key, and Value\n",
        "        feat_after_sa_stream = self.SA_stream(feat_after_mod_ca, feat_after_mod_ca) + feat_after_mod_ca\n",
        "        # Average the features after self-attention along the sequence dimension to get the final stream representation\n",
        "        stream_output_representation = torch.mean(feat_after_sa_stream, dim=1) # (B, C_token)\n",
        "\n",
        "        # Return flat features for contrastive loss, and the final stream output representation (add a dimension to match the expected (B, 1, C_token) shape)\n",
        "        return audio_f_flat, vis_f_flat, text_f_flat, stream_output_representation.unsqueeze(1)\n",
        "\n",
        "\n",
        "# --- Main Model: ContextPunchlineHumorModelNew ---\n",
        "class ContextPunchlineHumorModelNew(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self,\n",
        "                 bert_model_name_or_path,\n",
        "                 audio_word_dim, video_word_dim,\n",
        "                 sentence_lstm_hidden_dim, sample_lstm_hidden_dim, hier_lstm_dropout,\n",
        "                 projector_output_dim,\n",
        "                 bert_hidden_size_actual, max_bert_len_for_lstm,\n",
        "                 text_lstm_hidden_size_in_stream,\n",
        "                 attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 stream_ca_sa_heads, stream_dropout_rate,\n",
        "                 final_cross_attention_heads, # MODIFIED: This will now be used for the new final fusion heads\n",
        "                 mlp_hidden_dim, num_classes,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(bert_model_name_or_path)\n",
        "\n",
        "        self.ctx_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.ctx_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.context_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        self.pl_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.pl_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.punchline_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Final Fusion ---\n",
        "        self.final_fusion_input_dim = attention_token_dim\n",
        "\n",
        "        self.final_ca_query_streams_on_ctx_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_ca_query_streams_on_pl_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_fusion_sa_after_ca_sum = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        # REMOVED: self.cross_attention_final\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.final_fusion_input_dim, mlp_hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(mlp_hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self,\n",
        "                ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl,\n",
        "                ctx_padded_video, ctx_video_sl, ctx_video_ssl,\n",
        "                ctx_input_ids, ctx_attention_mask,\n",
        "                pl_padded_audio, pl_audio_sl, pl_audio_ssl,\n",
        "                pl_padded_video, pl_video_sl, pl_video_ssl,\n",
        "                pl_input_ids, pl_attention_mask,\n",
        "                current_modality_config=None, tokenizer_for_padding=None\n",
        "                ):\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_a = self.ctx_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_a_vec = torch.zeros(ctx_padded_audio.shape[0], actual_hier_lstm_output_dim_ctx_a, device=ctx_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(ctx_audio_sl > 0):\n",
        "                ctx_a_vec = self.ctx_audio_hier_lstm(ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_v = self.ctx_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_v_vec = torch.zeros(ctx_padded_video.shape[0], actual_hier_lstm_output_dim_ctx_v, device=ctx_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(ctx_video_sl > 0):\n",
        "                ctx_v_vec = self.ctx_video_hier_lstm(ctx_padded_video, ctx_video_sl, ctx_video_ssl)\n",
        "\n",
        "        ctx_a_proj = self.ctx_audio_projector(ctx_a_vec)\n",
        "        ctx_v_proj = self.ctx_video_projector(ctx_v_vec)\n",
        "\n",
        "        ctx_bert_hs = torch.zeros(ctx_input_ids.shape[0], ctx_input_ids.shape[1], self.bert_model.config.hidden_size, device=ctx_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(ctx_attention_mask.sum(dim=1) > 0):\n",
        "                ctx_bert_outputs = self.bert_model(input_ids=ctx_input_ids, attention_mask=ctx_attention_mask)\n",
        "                ctx_bert_hs = ctx_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        ctx_audio_f_flat, ctx_vis_f_flat, ctx_text_f_flat, ctx_stream_repr = self.context_processor(ctx_a_proj, ctx_v_proj, ctx_bert_hs)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_a = self.pl_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_a_vec = torch.zeros(pl_padded_audio.shape[0], actual_hier_lstm_output_dim_pl_a, device=pl_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(pl_audio_sl > 0):\n",
        "                pl_a_vec = self.pl_audio_hier_lstm(pl_padded_audio, pl_audio_sl, pl_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_v = self.pl_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_v_vec = torch.zeros(pl_padded_video.shape[0], actual_hier_lstm_output_dim_pl_v, device=pl_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(pl_video_sl > 0):\n",
        "                pl_v_vec = self.pl_video_hier_lstm(pl_padded_video, pl_video_sl, pl_video_ssl)\n",
        "\n",
        "        pl_a_proj = self.pl_audio_projector(pl_a_vec)\n",
        "        pl_v_proj = self.pl_video_projector(pl_v_vec)\n",
        "\n",
        "        pl_bert_hs = torch.zeros(pl_input_ids.shape[0], pl_input_ids.shape[1], self.bert_model.config.hidden_size, device=pl_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(pl_attention_mask.sum(dim=1) > 0):\n",
        "                pl_bert_outputs = self.bert_model(input_ids=pl_input_ids, attention_mask=pl_attention_mask)\n",
        "                pl_bert_hs = pl_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        pl_audio_f_flat, pl_vis_f_flat, pl_text_f_flat, pl_stream_repr = self.punchline_processor(pl_a_proj, pl_v_proj, pl_bert_hs)\n",
        "\n",
        "        # --- MODIFIED: New final fusion logic ---\n",
        "        streams_query = torch.cat((ctx_stream_repr, pl_stream_repr), dim=1)\n",
        "        res_ca_ctx = self.final_ca_query_streams_on_ctx_kv(streams_query, ctx_stream_repr)\n",
        "        res_ca_pl = self.final_ca_query_streams_on_pl_kv(streams_query, pl_stream_repr)\n",
        "        fused_after_ca = res_ca_ctx + res_ca_pl\n",
        "        fused_after_sa = self.final_fusion_sa_after_ca_sum(fused_after_ca, fused_after_ca)\n",
        "        fused_after_sa = fused_after_sa + fused_after_ca # Residual connection for the self-attention on fused representations\n",
        "        fused_representation = torch.mean(fused_after_sa, dim=1)\n",
        "\n",
        "        mlp_out = self.mlp(fused_representation)\n",
        "        logits = self.classifier(mlp_out)\n",
        "\n",
        "        contrastive_features = {\n",
        "            'ctx_audio': ctx_audio_f_flat, 'ctx_video': ctx_vis_f_flat, 'ctx_text': ctx_text_f_flat,\n",
        "            'pl_audio': pl_audio_f_flat, 'pl_video': pl_vis_f_flat, 'pl_text': pl_text_f_flat\n",
        "        }\n",
        "        return logits, contrastive_features\n",
        "\n",
        "\n",
        "# --- Contrastive Loss Function ---\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    # Initialization function, temperature coefficient\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        # Use CrossEntropyLoss to calculate loss (SimCLR style)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward propagation function\n",
        "    # emb_i, emb_j are embeddings from different modalities or views (B, D)\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        # Get batch size\n",
        "        batch_size = emb_i.shape[0]\n",
        "        # Contrastive loss requires at least 2 samples to compute, otherwise return 0 loss\n",
        "        if batch_size <= 1:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # Check if embeddings are all zeros, if so, loss is 0 (to avoid NaN)\n",
        "        # If the sum of absolute values of all elements in either embedding tensor is less than a very small value, it is considered empty or all zeros\n",
        "        if emb_i.abs().sum() < 1e-9 or emb_j.abs().sum() < 1e-9:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # L2 normalize embedding vectors\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        # Concatenate the two groups of normalized embeddings along the batch dimension: (2*B, D)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Calculate similarity matrix (cosine similarity between all sample pairs, then divide by temperature)\n",
        "        # (2*B, D) @ (D, 2*B) -> (2*B, 2*B)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "\n",
        "        # Create labels: for each sample in z_i, its positive sample is the corresponding sample in z_j\n",
        "        # For example, row similarity_matrix[0] is the similarity of z_i[0] with all representations\n",
        "        # Its positive sample z_j[0] has index batch_size + 0 in representations\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        # For each sample in z_j, its positive sample is the corresponding sample in z_i\n",
        "        # For example, row similarity_matrix[batch_size+0] is the similarity of z_j[0] with all representations\n",
        "        # Its positive sample z_i[0] has index 0 in representations\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Calculate loss, separately for z_i querying z_j and z_j querying z_i\n",
        "        # loss_i: z_i as anchor, corresponding sample in z_j as positive\n",
        "        # similarity_matrix[:batch_size] is the similarity of z_i with all representations (B, 2*B)\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        # loss_j: z_j as anchor, corresponding sample in z_i as positive\n",
        "        # similarity_matrix[batch_size:] is the similarity of z_j with all representations (B, 2*B)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        # Return average loss\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "\n",
        "# --- Modified Training Function (Only contrastive loss calculation method is changed) ---\n",
        "def train_new_model(model, data_loader, optimizer, scheduler,\n",
        "                    bce_criterion, contrastive_loss_fn, device, epoch, num_epochs,\n",
        "                    contrastive_loss_weight, current_modality_config, tokenizer_for_padding):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # The BERT part of the model is already globally frozen externally, no explicit model.bert_model.eval() needed here\n",
        "\n",
        "    # Initialize total BCE loss, total contrastive loss, total loss\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0 # Used to accumulate final_simclr_loss_for_batch for each batch\n",
        "    total_loss = 0\n",
        "    # Create tqdm progress bar to display training progress\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    # Iterate through each batch in the data loader\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack data from collate_fn\n",
        "        (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "         pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "         labels) = batch\n",
        "\n",
        "        # Get current batch size\n",
        "        current_batch_size = ctx_a_feat.shape[0]\n",
        "        # If batch is empty, skip\n",
        "        if current_batch_size == 0: continue\n",
        "\n",
        "        # Move data to the specified device (excluding the last label)\n",
        "        batch_data_on_device = []\n",
        "        for tensor_item in batch[:-1]:\n",
        "            batch_data_on_device.append(tensor_item.to(device))\n",
        "        # Move labels to device and convert to long type\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        # Clear optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model forward pass, get classification logits and contrastive features\n",
        "        logits, contrastive_feats = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "        # Calculate BCE classification loss\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # --- Contrastive Loss Calculation (modified to averaging method) ---\n",
        "        final_simclr_loss_for_batch = torch.tensor(0.0, device=device) # Initialize contrastive loss for this batch\n",
        "        if current_batch_size > 1 and contrastive_loss_weight > 0:\n",
        "            accumulated_contrastive_loss_components = []\n",
        "\n",
        "            # Contrastive loss for the context stream\n",
        "            ctx_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_video'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_video'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_video'], contrastive_feats['ctx_text']))\n",
        "\n",
        "            if ctx_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(ctx_individual_losses)))\n",
        "\n",
        "            # Contrastive loss for the punchline stream\n",
        "            pl_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_video'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_video'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_video'], contrastive_feats['pl_text']))\n",
        "\n",
        "            if pl_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(pl_individual_losses)))\n",
        "\n",
        "            # Calculate the final contrastive loss (if multiple components exist, take their average)\n",
        "            if accumulated_contrastive_loss_components:\n",
        "                final_simclr_loss_for_batch = torch.mean(torch.stack(accumulated_contrastive_loss_components))\n",
        "            # else: final_simclr_loss_for_batch remains its initial value of 0.0\n",
        "\n",
        "        # Total loss = BCE loss + contrastive_loss_weight * calculated batch contrastive loss\n",
        "        current_loss = bce_loss + contrastive_loss_weight * final_simclr_loss_for_batch\n",
        "\n",
        "        # Backpropagate to calculate gradients\n",
        "        current_loss.backward()\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        # If a learning rate scheduler is used\n",
        "        if scheduler is not None:\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss values (item() gets scalar value)\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += final_simclr_loss_for_batch.item() # Accumulate the calculated batch contrastive loss\n",
        "        total_loss += current_loss.item()\n",
        "        # Update progress bar display information\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{final_simclr_loss_for_batch.item():.4f}\")\n",
        "\n",
        "    # If data loader is not empty\n",
        "    if len(data_loader) > 0:\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        # Print average training loss for the current epoch\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "\n",
        "\n",
        "# --- Validation/Test Function (Added F1 Score) ---\n",
        "def validate_or_test_new_model(model, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                               current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize total BCE loss\n",
        "    total_bce_loss = 0\n",
        "    # List to store all prediction results\n",
        "    all_preds = []\n",
        "    # List to store all true labels\n",
        "    all_labels = []\n",
        "\n",
        "    # Set progress bar description (corrected logic)\n",
        "    if mode == \"Test\" and epoch is None:\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # and epoch is not None (implicitly for this branch after the first)\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Val\": # epoch should not be None for validation\n",
        "        desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    else: # Fallback, though ideally all cases are covered\n",
        "        desc = f\"Processing [{mode} {current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    # Do not calculate gradients within this block to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through each batch in the data loader\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            # Unpack data\n",
        "            (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "             pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "             labels) = batch\n",
        "\n",
        "            # Get current batch size\n",
        "            current_batch_size = ctx_a_feat.shape[0]\n",
        "            # If batch is empty, skip\n",
        "            if current_batch_size == 0: continue\n",
        "\n",
        "            # Move data to device\n",
        "            batch_data_on_device = [t.to(device) for t in batch[:-1]]\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            # Model forward pass, ignore contrastive features (not needed during validation/testing)\n",
        "            logits, _ = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "            # Calculate BCE loss\n",
        "            bce_loss = bce_criterion(logits, labels)\n",
        "            # Accumulate BCE loss\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            # Get predicted class (index of the max value in logits)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Store prediction results (convert to numpy array)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            # Store true labels (convert to numpy array)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # If data loader is empty or no labels were collected\n",
        "    if len(data_loader) == 0 or len(all_labels) == 0 :\n",
        "        print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty.\")\n",
        "        if mode == \"Val\": return 0.0, 0.0 # Validation mode returns 0.0 accuracy, 0.0 F1\n",
        "        return 0.0, 0.0, 0.0 # Test mode returns 0.0 loss, 0.0 accuracy, 0.0 F1\n",
        "\n",
        "    # Calculate average BCE loss\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    # Calculate accuracy (if label list is not empty)\n",
        "    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
        "    # Calculate F1 score (if label list is not empty), use 'binary' because it's binary classification, zero_division handles boundary cases\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0) if all_labels else 0.0\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # If validation mode, return accuracy and F1\n",
        "    if mode == \"Val\": return accuracy, f1\n",
        "    # If test mode, return average loss, accuracy, and F1\n",
        "    return avg_bce_loss, accuracy, f1\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameter Configuration ---\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # SENTENCE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    # SAMPLE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    HIER_LSTM_DROPOUT = 0.3\n",
        "\n",
        "    PROJECTOR_OUTPUT_DIM = 1024\n",
        "\n",
        "    MAX_BERT_LEN_FOR_PART_DATASET = 512\n",
        "    TEXT_LSTM_HIDDEN_SIZE_IN_STREAM = 256   # This version of the code still uses this parameter\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    STREAM_CA_SA_HEADS = 1\n",
        "    STREAM_DROPOUT_RATE = 0.3\n",
        "\n",
        "    FINAL_CROSS_ATTENTION_HEADS = 1         # Used for all attention modules in the new final fusion structure\n",
        "    MLP_HIDDEN_DIM = 256\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    BATCH_SIZE = 16 # Warning: The new final fusion structure is more complex, may need to reduce this value\n",
        "    LEARNING_RATE = 8e-5\n",
        "    NUM_EPOCHS = 8 # It is recommended to increase epochs for actual use\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"BERT model used: {BERT_MODEL_NAME_FOR_MAIN}\")\n",
        "    print(f\"Hierarchical LSTM: Sentence-level hidden dim {SENTENCE_LSTM_HIDDEN_DIM_CONFIG}, Sample-level hidden dim {SAMPLE_LSTM_HIDDEN_DIM_CONFIG}, Dropout {HIER_LSTM_DROPOUT}\")\n",
        "    print(f\"Projector output dimension (Stream processor audio/video input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"Max BERT length for context/punchline part: {MAX_BERT_LEN_FOR_PART_DATASET}\")\n",
        "    print(f\"Stream processor internal text LSTM hidden size: {TEXT_LSTM_HIDDEN_SIZE_IN_STREAM}\") # Keep printing as this param is still in model def\n",
        "    print(f\"Attention token dimension: {ATTENTION_TOKEN_DIM}, Tokens per modality: {NUM_ATTENTION_TOKENS_PER_MODAL}\")\n",
        "    print(f\"Stream processor attention heads: {STREAM_CA_SA_HEADS}, Stream processor text FC Dropout rate: {STREAM_DROPOUT_RATE}\")\n",
        "    print(f\"Final fusion stage attention heads: {FINAL_CROSS_ATTENTION_HEADS}, MLP hidden dimension: {MLP_HIDDEN_DIM}\")\n",
        "    print(f\"Training parameters: Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}, Epochs {NUM_EPOCHS}\")\n",
        "    print(f\"Contrastive loss: Temperature {TEMPERATURE_CONTRASTIVE}, Weight {CONTRASTIVE_LOSS_WEIGHT}\")\n",
        "    print(\"\\n !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \\n\")\n",
        "\n",
        "    # --- Load Raw Data ---\n",
        "    print(\"Loading raw data pickle files...\")\n",
        "    # Ensure paths are correct\n",
        "    # Example: data_folds_path = \"path_to_your_gdrive/Project_CCS2-main/sdk_features/data_folds.pkl\"\n",
        "    # Replace with your actual paths\n",
        "    # To run locally, you might need to download these files or adjust paths\n",
        "    # For demonstration, we'll assume files might not exist and add checks or placeholders.\n",
        "    try:\n",
        "        data_folds = load_pickle(data_folds_path)\n",
        "        language_sdk = load_pickle(language_file)\n",
        "        covarep_sdk = load_pickle(covarep_file)\n",
        "        openface_sdk = load_pickle(openface_file)\n",
        "        humor_label_sdk = load_pickle(humor_label_file)\n",
        "        print(\"Raw data loading complete.\")\n",
        "\n",
        "        train_ids = data_folds['train']\n",
        "        dev_ids = data_folds['dev']\n",
        "        test_ids = data_folds['test']\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: One or more data files not found. Please check paths and ensure files exist.\")\n",
        "        print(\"Using placeholder data for demonstration.\")\n",
        "        # Placeholder data for demonstration if files are missing\n",
        "        train_ids, dev_ids, test_ids = ['h1','h2'], ['h3'], ['h4']\n",
        "        language_sdk = {\n",
        "            f'h{i}': {'punchline_sentence': f'Punchline {i}', 'context_sentences': [f'Context sent {i}.1', f'Context sent {i}.2']} for i in range(1, 5)\n",
        "        }\n",
        "        covarep_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _AUDIO_WORD_DIM_CONST).astype(np.float32) if i % 2 == 0 else [], # Some empty\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _AUDIO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        openface_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _VIDEO_WORD_DIM_CONST).astype(np.float32),\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _VIDEO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        humor_label_sdk = {f'h{i}': float(i % 2) for i in range(1,5)}\n",
        "\n",
        "\n",
        "    print(\"Extracting features and labels...\")\n",
        "    (train_ps, train_cs, train_cvp_p, train_cvp_c, train_of_p, train_of_c, train_labels) = \\\n",
        "        extract_features_and_labels(train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (dev_ps, dev_cs, dev_cvp_p, dev_cvp_c, dev_of_p, dev_of_c, dev_labels) = \\\n",
        "        extract_features_and_labels(dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (test_ps, test_cs, test_cvp_p, test_cvp_c, test_of_p, test_of_c, test_labels) = \\\n",
        "        extract_features_and_labels(test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    print(\"Feature and label extraction complete.\")\n",
        "\n",
        "    print(\"Structuring data for new dataset format (context/punchline split)...\")\n",
        "    train_sample_data_dicts = concatenate_multimodal_data_for_dataset(train_cvp_c, train_of_c, train_cs, train_cvp_p, train_of_p, train_ps)\n",
        "    dev_sample_data_dicts = concatenate_multimodal_data_for_dataset(dev_cvp_c, dev_of_c, dev_cs, dev_cvp_p, dev_of_p, dev_ps)\n",
        "    test_sample_data_dicts = concatenate_multimodal_data_for_dataset(test_cvp_c, test_of_c, test_cs, test_cvp_p, test_of_p, test_ps)\n",
        "    print(\"Data structuring complete.\")\n",
        "\n",
        "    print(\"Initializing BERT tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    _bert_temp_model = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = _bert_temp_model.config.hidden_size\n",
        "    del _bert_temp_model\n",
        "    print(f\"Actual BERT hidden size: {BERT_HIDDEN_SIZE_ACTUAL}\")\n",
        "\n",
        "    print(\"Creating CustomFeatureDatasetContextPunchline instances...\")\n",
        "    train_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        train_sample_data_dicts, train_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        dev_sample_data_dicts, dev_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    test_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        test_sample_data_dicts, test_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=custom_collate_fn_context_punchline, drop_last=True if BATCH_SIZE > 1 and len(train_dataset) > BATCH_SIZE else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    print(f\"Dataloaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AVT_CtxPl_Contra_FinalFusionMimicASP_NoFinalOrigSA', 'audio': True, 'video': True, 'text': True},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    for config_idx, model_config_iter in enumerate(modality_configurations):\n",
        "        config_name = model_config_iter['name']\n",
        "        print(f\"\\n--- Starting processing for model config: {config_name} ---\")\n",
        "\n",
        "        model = ContextPunchlineHumorModelNew(\n",
        "            bert_model_name_or_path=BERT_MODEL_NAME_FOR_MAIN,\n",
        "            audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=SAMPLE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            hier_lstm_dropout=HIER_LSTM_DROPOUT,\n",
        "            projector_output_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size_actual=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len_for_lstm=MAX_BERT_LEN_FOR_PART_DATASET, # This is max_bert_len_for_part\n",
        "            text_lstm_hidden_size_in_stream=TEXT_LSTM_HIDDEN_SIZE_IN_STREAM,\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            stream_ca_sa_heads=STREAM_CA_SA_HEADS,\n",
        "            stream_dropout_rate=STREAM_DROPOUT_RATE,\n",
        "            final_cross_attention_heads=FINAL_CROSS_ATTENTION_HEADS,\n",
        "            mlp_hidden_dim=MLP_HIDDEN_DIM,\n",
        "            num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        print(\"Freezing BERT parameters in the main model...\")\n",
        "        for param in model.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen.\")\n",
        "\n",
        "        bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "        optimizer_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = AdamW(optimizer_params, lr=LEARNING_RATE)\n",
        "        scheduler = None\n",
        "        if len(train_loader) > 0 and NUM_EPOCHS > 0:\n",
        "            num_training_steps_per_epoch = len(train_loader)\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs.\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_val_f1_at_best_acc = 0.0 # Store F1 at the point of best accuracy\n",
        "        best_model_state_path = f\"best_model_{config_name}.pth\"\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader for {config_name} is empty. Skipping training.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_new_model(model, train_loader, optimizer, scheduler, bce_criterion,\n",
        "                                contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS,\n",
        "                                CONTRASTIVE_LOSS_WEIGHT, model_config_iter, bert_tokenizer_global)\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy, val_f1 = validate_or_test_new_model(model, val_loader, bce_criterion, DEVICE,\n",
        "                                                                    epoch, NUM_EPOCHS, model_config_iter,\n",
        "                                                                    bert_tokenizer_global, mode=\"Val\")\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        best_val_f1_at_best_acc = val_f1 # Save F1 at this best accuracy point\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f} (F1: {best_val_f1_at_best_acc:.4f}). Saving model...\")\n",
        "                        torch.save(model.state_dict(), best_model_state_path)\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (corresponding F1: {best_val_f1_at_best_acc:.4f})\")\n",
        "\n",
        "        print(f\"\\nStarting test phase for {config_name}...\")\n",
        "        test_accuracy, test_f1, test_loss = 0.0, 0.0, 0.0 # Initialize\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader for {config_name} is empty. Skipping test.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                                               'test_acc': 0.0, 'test_f1':0.0, 'test_loss': 0.0}\n",
        "        else:\n",
        "            if os.path.exists(best_model_state_path) and best_val_accuracy_for_config > 0: # Check if model was saved\n",
        "                print(f\"Loading best model state from {best_model_state_path} for testing.\")\n",
        "                model.load_state_dict(torch.load(best_model_state_path, map_location=DEVICE))\n",
        "            elif best_val_accuracy_for_config == 0 and len(train_loader) > 0 : # Was trained, but no improvement or no val\n",
        "                print(f\"No best validation model saved (or validation accuracy was 0), using model from last training epoch for testing.\")\n",
        "            elif len(train_loader) == 0: # Not trained\n",
        "                print(f\"No training was performed for {config_name}. Testing with initialized model (results might be poor).\")\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test_new_model(\n",
        "                model, test_loader, bce_criterion, DEVICE, epoch=None, num_epochs=NUM_EPOCHS, # epoch=None for final test\n",
        "                current_modality_config=model_config_iter, tokenizer_for_padding=bert_tokenizer_global, mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                'test_acc': test_accuracy, 'test_f1': test_f1, 'test_loss': test_loss,\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy: {results.get('val_acc', 0.0):.4f} (Corresponding Val F1: {results.get('val_f1', 0.0):.4f})\")\n",
        "        print(f\"  Test Set Accuracy: {results.get('test_acc', 0.0):.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results.get('test_f1', 0.0):.4f}\")\n",
        "        print(f\"  Test Set Loss: {results.get('test_loss', 0.0):.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "    print(\"All operations complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-head attention, new loss function, remove last self-attention layer\n",
        "# Import pickle module, used for serializing and deserializing Python object structures\n",
        "import pickle\n",
        "# Import numpy library, used for scientific computing, especially array operations\n",
        "import numpy as np\n",
        "# Import PyTorch library, an open-source machine learning framework\n",
        "import torch\n",
        "# Import PyTorch's neural network module\n",
        "import torch.nn as nn\n",
        "# Import PyTorch's neural network functional library\n",
        "import torch.nn.functional as F\n",
        "# Import Dataset and DataLoader classes from PyTorch, used for data loading\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Import auto tokenizer, auto model, and learning rate scheduler from transformers library\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "# Import AdamW optimizer from PyTorch\n",
        "from torch.optim import AdamW\n",
        "# Import accuracy and F1 score calculation functions from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Import copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "import copy\n",
        "# Import tqdm library, used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# Import os module, used for file path operations, etc.\n",
        "import os\n",
        "# Import functions for handling variable-length sequences from PyTorch's RNN utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "# Google Drive mount path (example)\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\"\n",
        "# Base path where feature files are located (example)\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\")\n",
        "\n",
        "# Path to the dataset split file\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "# Path to the OpenFace feature file\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "# Path to the COVAREP feature file\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "# Path to the language feature file\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "# Path to the humor label file\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "\n",
        "# Audio word-level feature dimension constant\n",
        "_AUDIO_WORD_DIM_CONST = 81\n",
        "# Video word-level feature dimension constant\n",
        "_VIDEO_WORD_DIM_CONST = 371\n",
        "# Hidden dimension of sentence-level LSTM in Hierarchical LSTM (Modified to align with Script_B's configuration idea)\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "# Hidden dimension of sample-level LSTM in Hierarchical LSTM (also its output dimension, projector layer input dimension) (Modified to align with Script_B)\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "# Helper function to load pickle files\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        # Open file in binary read mode\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            # Load pickle data\n",
        "            return pickle.load(f)\n",
        "    # Handle possible UnicodeDecodeError\n",
        "    except UnicodeDecodeError:\n",
        "        # If UnicodeDecodeError occurs, try opening with latin1 encoding\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    # Handle other possible exceptions\n",
        "    except Exception as e:\n",
        "        print(f'Cannot load data {pickle_file}: {e}')\n",
        "        # Raise exception\n",
        "        raise\n",
        "\n",
        "# Helper function to safely prepare feature data for np.array()\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    # If input data is None, return an empty list\n",
        "    if feature_data is None: return []\n",
        "    # If input data is a numpy array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's an empty numpy array, return an empty list\n",
        "        if feature_data.size == 0: return []\n",
        "        # Return non-empty numpy array\n",
        "        return feature_data\n",
        "    # If input data is a list\n",
        "    if isinstance(feature_data, list):\n",
        "        # If it's an empty list, return an empty list\n",
        "        if not feature_data: return []\n",
        "        # Return non-empty list\n",
        "        return feature_data\n",
        "    # Other unexpected types, return an empty list (can add a warning)\n",
        "    return []\n",
        "\n",
        "# Function to extract features and labels\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    # Initialize lists to store various features and labels\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "    # Iterate through the ID list\n",
        "    for hid in id_list:\n",
        "        # Add punchline text\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        # Add context text list\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP (audio) feature processing\n",
        "        # Prepare COVAREP features for the punchline\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline audio features to a float32 numpy array and add\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp, dtype=np.float32))\n",
        "        # Process context COVAREP features (one feature array per sentence)\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp, dtype=np.float32))\n",
        "        # Add the list of processed context audio features\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace (video) feature processing\n",
        "        # Prepare OpenFace features for the punchline\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline video features to a float32 numpy array and add\n",
        "        of_p_list.append(np.array(prepared_punchline_of, dtype=np.float32))\n",
        "        # Process context OpenFace features\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of, dtype=np.float32))\n",
        "        # Add the list of processed context video features\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        # Add labels\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    # Return all extracted features and labels, specifying the dtype for numpy arrays\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object), np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object), np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object), np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "# Prepare data for the new dataset structure: output a list of samples, each sample is a dictionary containing all sentence features/texts\n",
        "# Among them, the features/text of the punchline will be the last item in the corresponding modality list\n",
        "def concatenate_multimodal_data_for_dataset(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    # Get the number of samples (based on the number of context sentences)\n",
        "    num_samples = len(cs)\n",
        "    # List to store all sample data\n",
        "    all_samples_data = []\n",
        "    # Iterate through each sample\n",
        "    for i in range(num_samples):\n",
        "        # Data dictionary for a single sample, containing 'audio', 'video', 'text' keys\n",
        "        sample_data = {'audio': [], 'video': [], 'text': []}\n",
        "\n",
        "        # Audio data processing\n",
        "        # Extract context audio features, ensuring they are valid numpy arrays (word count > 0, correct dimension)\n",
        "        current_sample_audio = [s for s in list(cvp_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _AUDIO_WORD_DIM_CONST]\n",
        "        # Get punchline audio features\n",
        "        punchline_audio = cvp_p[i]\n",
        "        # Append punchline audio features to the end of the list, if valid\n",
        "        if isinstance(punchline_audio, np.ndarray) and punchline_audio.ndim == 2 and punchline_audio.shape[0] > 0 and punchline_audio.shape[1] == _AUDIO_WORD_DIM_CONST:\n",
        "            current_sample_audio.append(punchline_audio)\n",
        "        # If the current audio list is empty (both context and punchline are invalid or missing), add a placeholder for the punchline (single sample, correct dimension)\n",
        "        elif not current_sample_audio:\n",
        "            current_sample_audio.append(np.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        # Store the processed audio feature list into the sample data dictionary\n",
        "        sample_data['audio'] = current_sample_audio\n",
        "\n",
        "        # Video data processing (logic same as audio)\n",
        "        current_sample_video = [s for s in list(of_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _VIDEO_WORD_DIM_CONST]\n",
        "        punchline_video = of_p[i]\n",
        "        if isinstance(punchline_video, np.ndarray) and punchline_video.ndim == 2 and punchline_video.shape[0] > 0 and punchline_video.shape[1] == _VIDEO_WORD_DIM_CONST:\n",
        "            current_sample_video.append(punchline_video)\n",
        "        elif not current_sample_video:\n",
        "            current_sample_video.append(np.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        sample_data['video'] = current_sample_video\n",
        "\n",
        "        # Text data processing\n",
        "        # Extract context sentence text list\n",
        "        current_sample_text = [s for s in list(cs[i]) if isinstance(s, str)]\n",
        "        # Get punchline text\n",
        "        punchline_text_str = ps[i]\n",
        "        # If the punchline text is a string, append it\n",
        "        if isinstance(punchline_text_str, str):\n",
        "            current_sample_text.append(punchline_text_str)\n",
        "        # If the current text list is empty (both context and punchline are invalid or missing), add an empty string as a punchline placeholder\n",
        "        elif not current_sample_text:\n",
        "            current_sample_text.append(\"\")\n",
        "        sample_data['text'] = current_sample_text\n",
        "\n",
        "        # Add the current sample's data dictionary to the total list\n",
        "        all_samples_data.append(sample_data)\n",
        "    # Return the list containing all sample data\n",
        "    return all_samples_data\n",
        "\n",
        "\n",
        "# --- Dataset Class: Modified for Context/Punchline Splitting ---\n",
        "class CustomFeatureDatasetContextPunchline(Dataset):\n",
        "    # Initialization function\n",
        "    def __init__(self, list_of_sample_data_dicts, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len_for_part=512,\n",
        "                 audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST):\n",
        "\n",
        "        # List of sample data dictionaries (each element is a sample, containing 'audio', 'video', 'text' keys)\n",
        "        self.list_of_sample_data_dicts = list_of_sample_data_dicts\n",
        "        # List of labels, converted to torch.long type\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long)\n",
        "        # BERT tokenizer\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        # Maximum BERT length for each part (context/punchline)\n",
        "        self.max_bert_len_for_part = max_bert_len_for_part\n",
        "        # Audio word feature dimension\n",
        "        self.audio_word_dim = audio_word_dim\n",
        "        # Video word feature dimension\n",
        "        self.video_word_dim = video_word_dim\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    # Helper function to tokenize the text part\n",
        "    def _tokenize_text_part(self, text_sentences_list):\n",
        "        # If the text list is empty\n",
        "        if not text_sentences_list:\n",
        "            # If the tokenizer has a pad token, use it, otherwise an empty string might be tokenized into special tokens\n",
        "            processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "        else:\n",
        "            # Join all sentences in the sentence list with spaces\n",
        "            processed_text = \" \".join(text_sentences_list)\n",
        "            # If it's only whitespace or empty after joining\n",
        "            if not processed_text.strip():\n",
        "                processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "\n",
        "        # Call the tokenizer to tokenize\n",
        "        bert_inputs = self.tokenizer(\n",
        "            processed_text, add_special_tokens=True, return_attention_mask=True, # Add special tokens, return attention_mask\n",
        "            max_length=self.max_bert_len_for_part, padding='max_length', truncation=True, # Max length, pad to max length, truncate\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        )\n",
        "        # Return input_ids and attention_mask, and remove the batch dimension (because this is single sample processing)\n",
        "        return bert_inputs[\"input_ids\"].squeeze(0), bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    # Helper function to process audio/video parts\n",
        "    # all_sentences_features_for_sample: List of all sentence features for the entire sample (list of numpy arrays)\n",
        "    # part_sentences_indices: Indices in the total sentence list that the current part (context or punchline) should contain\n",
        "    # word_dim: Word feature dimension for audio or video\n",
        "    def _process_av_part(self, all_sentences_features_for_sample, part_sentences_indices, word_dim):\n",
        "        # List to store feature tensors of all sentences in this part\n",
        "        part_features_list = []\n",
        "        # If the sample itself does not have any sentence features (e.g., the entire sample is empty)\n",
        "        if not all_sentences_features_for_sample:\n",
        "            # Add a placeholder tensor (1 word, specified dimension)\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "            return part_features_list\n",
        "\n",
        "        # Iterate through the sentence indices of the specified part\n",
        "        for sent_idx in part_sentences_indices:\n",
        "            # Ensure the index is within the valid range\n",
        "            if 0 <= sent_idx < len(all_sentences_features_for_sample):\n",
        "                # Get features of a single sentence (numpy array)\n",
        "                sent_feat = all_sentences_features_for_sample[sent_idx]\n",
        "                # Validate feature validity: is a numpy array, 2D, word count > 0, correct dimension\n",
        "                if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0 and sent_feat.shape[1] == word_dim:\n",
        "                    # Convert to PyTorch tensor and add to the list\n",
        "                    part_features_list.append(torch.as_tensor(sent_feat, dtype=torch.float32))\n",
        "\n",
        "        # If this part is empty after processing (e.g., all sentences are invalid or indices are out of range, or the specified index list is empty)\n",
        "        if not part_features_list:\n",
        "            # Add a placeholder tensor for this part\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "        return part_features_list\n",
        "\n",
        "\n",
        "    # Method to get single sample data\n",
        "    def __getitem__(self, index):\n",
        "        # Get the sample data dictionary for the current index\n",
        "        sample_data = self.list_of_sample_data_dicts[index]\n",
        "        # Audio: list of numpy arrays (sentence features)\n",
        "        audio_all_sents_raw = sample_data['audio']\n",
        "        # Video: list of numpy arrays (sentence features)\n",
        "        video_all_sents_raw = sample_data['video']\n",
        "        # Text: list of sentence strings\n",
        "        text_all_sents_str = sample_data['text']\n",
        "        # Get label\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        # Determine the total number of sentences based on the number of text sentences\n",
        "        n_total_sents = len(text_all_sents_str)\n",
        "\n",
        "        # Prepare placeholder input_ids and attention_mask for empty text parts\n",
        "        empty_ids, empty_mask = self._tokenize_text_part([])\n",
        "\n",
        "        # Case 1: If the sample has no sentences at all (n_total_sents == 0)\n",
        "        if n_total_sents == 0:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is empty/placeholder\n",
        "            pl_audio_part = self._process_av_part([], [], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = empty_ids, empty_mask\n",
        "\n",
        "        # Case 2: If there is only one sentence, treat it as only punchline, context is empty\n",
        "        elif n_total_sents == 1:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty index list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is this one sentence (index 0)\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, [0], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, [0], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[0]])\n",
        "\n",
        "        # Case 3: If there are multiple sentences, split into context and punchline\n",
        "        else:\n",
        "            # Context sentence indices: from 0 to the second to last\n",
        "            ctx_indices = list(range(n_total_sents - 1))\n",
        "            # Punchline sentence index: only the last one\n",
        "            pl_indices = [n_total_sents - 1]\n",
        "\n",
        "            # Process context part\n",
        "            ctx_audio_part = self._process_av_part(audio_all_sents_raw, ctx_indices, self.audio_word_dim)\n",
        "            ctx_video_part = self._process_av_part(video_all_sents_raw, ctx_indices, self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in ctx_indices])\n",
        "\n",
        "            # Process punchline part\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, pl_indices, self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, pl_indices, self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in pl_indices])\n",
        "\n",
        "        # Return a tuple of context data, punchline data, and label\n",
        "        return (ctx_audio_part, ctx_video_part, ctx_input_ids, ctx_attention_mask,\n",
        "                pl_audio_part, pl_video_part, pl_input_ids, pl_attention_mask,\n",
        "                label)\n",
        "\n",
        "# --- Custom Collate Function for Context/Punchline Data ---\n",
        "def custom_collate_fn_context_punchline(batch):\n",
        "    # batch is a list where each element is the tuple returned by __getitem__\n",
        "    # Unpack batch data into respective lists\n",
        "    (ctx_audio_list, ctx_video_list, ctx_ids_list, ctx_mask_list,\n",
        "     pl_audio_list, pl_video_list, pl_ids_list, pl_mask_list,\n",
        "     labels_list) = zip(*batch)\n",
        "\n",
        "    # Directly stack text IDs, masks, and labels (they are already fixed-size tensors)\n",
        "    batched_ctx_ids = torch.stack(ctx_ids_list)\n",
        "    batched_ctx_masks = torch.stack(ctx_mask_list)\n",
        "    batched_pl_ids = torch.stack(pl_ids_list)\n",
        "    batched_pl_masks = torch.stack(pl_mask_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Helper function to process a list of audio/video data for a part (e.g., context audio)\n",
        "    # part_data_list: A list of samples, where each sample is a list of sentence tensors\n",
        "    # word_dim_const: Word feature dimension of this modality\n",
        "    def _collate_av_part(part_data_list, word_dim_const):\n",
        "        # Get the number of sentences in each sample\n",
        "        sample_lengths = [len(sample) for sample in part_data_list]\n",
        "        # Maximum number of sentences in the batch, 0 if empty\n",
        "        max_sents = max(sample_lengths) if sample_lengths else 0\n",
        "\n",
        "        # Get the word count of each sentence and find the maximum word count\n",
        "        sentence_word_counts_flat = []\n",
        "        for sample in part_data_list: # Iterate through each sample\n",
        "            for sentence_tensor in sample: # Iterate through each sentence tensor in the sample\n",
        "                sentence_word_counts_flat.append(sentence_tensor.shape[0]) # Add the word count of this sentence\n",
        "        # Maximum number of words in the batch, 0 if empty\n",
        "        max_words = max(sentence_word_counts_flat) if sentence_word_counts_flat else 0\n",
        "\n",
        "        # Ensure max_words and max_sents are at least 1 to avoid zero dimensions in tensors\n",
        "        max_words = max(1, max_words)\n",
        "        max_sents = max(1, max_sents)\n",
        "\n",
        "        # Create padded feature tensor and length tensor\n",
        "        # padded_features: (batch_size, max_sentences, max_words, feature_dimension)\n",
        "        # sentence_lengths_tensor: (batch_size, max_sentences) - records the actual word count of each sentence\n",
        "        padded_features = torch.zeros(len(part_data_list), max_sents, max_words, word_dim_const)\n",
        "        sentence_lengths_tensor = torch.zeros(len(part_data_list), max_sents, dtype=torch.long)\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, sample in enumerate(part_data_list):\n",
        "            # Iterate through each sentence tensor in the sample\n",
        "            for j, sentence_tensor in enumerate(sample):\n",
        "                # Word count of the current sentence\n",
        "                num_words = sentence_tensor.shape[0]\n",
        "                # Pad only if there are words\n",
        "                if num_words > 0:\n",
        "                    # Pad features into the padded_features tensor\n",
        "                    padded_features[i, j, :num_words, :] = sentence_tensor\n",
        "                    # Record the actual word count into the sentence_lengths_tensor tensor\n",
        "                    sentence_lengths_tensor[i, j] = num_words\n",
        "        # Return padded features, list of sentence counts per sample (as tensor), and word counts per sentence tensor\n",
        "        return padded_features, torch.tensor(sample_lengths, dtype=torch.long), sentence_lengths_tensor\n",
        "\n",
        "    # Process audio and video data for context and punchline separately\n",
        "    # ctx_padded_audio: (B, S_ctx_max, W_ctx_max, D_audio)\n",
        "    # ctx_audio_sl: (B,) - Actual number of sentences per sample for context\n",
        "    # ctx_audio_ssl: (B, S_ctx_max) - Actual word count of each sentence per sample for context\n",
        "    ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl = _collate_av_part(ctx_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    ctx_padded_video, ctx_video_sl, ctx_video_ssl = _collate_av_part(ctx_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "    pl_padded_audio, pl_audio_sl, pl_audio_ssl = _collate_av_part(pl_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    pl_padded_video, pl_video_sl, pl_video_ssl = _collate_av_part(pl_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "\n",
        "    # Return all processed batch data\n",
        "    return (ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl, # Context audio (features, sample sentence count, words per sentence)\n",
        "            ctx_padded_video, ctx_video_sl, ctx_video_ssl, # Context video\n",
        "            batched_ctx_ids, batched_ctx_masks,             # Context text\n",
        "            pl_padded_audio, pl_audio_sl, pl_audio_ssl,     # Punchline audio\n",
        "            pl_padded_video, pl_video_sl, pl_video_ssl,     # Punchline video\n",
        "            batched_pl_ids, batched_pl_masks,               # Punchline text\n",
        "            batched_labels)                                 # Labels\n",
        "\n",
        "\n",
        "# --- Hierarchical LSTM Aggregator ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Hidden dimension of sentence-level LSTM\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        # Hidden dimension of sample-level LSTM\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        # Sentence-level LSTM: input word embeddings, output sentence representation\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed, output dimension will become 2*hidden_dim\n",
        "\n",
        "        # If sentence LSTM is bidirectional, the input dimension of sample LSTM needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        # Sample-level LSTM: input sentence representations, output sample representation\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sentences, max_words, word_dimension)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sentences) - actual word count per sentence\n",
        "\n",
        "        # Get the shape of the feature tensor\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "        # Final output dimension of sample LSTM (considering bidirectional case)\n",
        "        final_output_dim_sample = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "\n",
        "        # Handle the extreme case where all inputs in the batch are empty\n",
        "        # If max_sentences or max_words is 0, or batch_size is 0, or all sample_lengths are 0\n",
        "        if max_sents == 0 or max_words == 0 or batch_size == 0 or torch.all(sample_lengths == 0):\n",
        "            # Return a zero tensor with shape (batch_size, final_output_dim_sample)\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sentence dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        # If all sentences are empty (all lengths are 0)\n",
        "        if not torch.any(valid_sents_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sentence features and corresponding lengths\n",
        "        sents_features_packed_data = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed_data = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack padded sequence (length tensor needs to be moved to CPU for packing)\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed_data, sents_word_lengths_packed_data.cpu(),\n",
        "                                                batch_first=True, enforce_sorted=False)\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers*num_directions, B*S_valid, sentence_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get the actual output dimension of sentence LSTM (considering bidirectional)\n",
        "        sent_hidden_dim_actual = self.sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "        # Get the hidden state of the last time step (for unidirectional LSTM, take the last layer; for bidirectional, concatenate the last time steps of the last two layers)\n",
        "        # Output shape: (B*S_valid, sentence_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "        # Apply dropout to sentence embeddings\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Put valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Create a zero tensor with shape (B*S, actual_sentence_hidden_dim)\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        # Fill valid sentence embeddings into corresponding positions\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent), reshape to sample LSTM input format\n",
        "        sample_features_for_sample_lstm = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack padded sequence (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0 (i.e., samples with actual sentence count of 0)\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        # If all samples are empty (actual sentence counts are all 0)\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sample features and corresponding lengths\n",
        "        sample_features_packed_input_data = sample_features_for_sample_lstm[valid_sample_indices]\n",
        "        sample_lengths_packed_data = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input_data, sample_lengths_packed_data.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers*num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get the hidden state of the last time step\n",
        "        # Output shape: (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "        # Apply dropout to sample embeddings\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Put valid sample embeddings back to their original positions, use zero vectors for empty samples\n",
        "        # Create a zero tensor with shape (B, final_output_dim_sample_lstm)\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "        # Fill valid sample embeddings into corresponding positions\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "        # Return final sample embeddings\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "# --- GLU Linear Layer ---\n",
        "class GLULinear(nn.Module):\n",
        "    # Initialization function, input dimension and output dimension\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GLULinear, self).__init__()\n",
        "        # The first linear layer is followed by a GELU activation function\n",
        "        self.layer1 = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU())\n",
        "        # The second linear layer\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    # Forward propagation function\n",
        "    def forward(self, x):\n",
        "        # Element-wise multiplication of the outputs of the two linear layers\n",
        "        return self.layer1(x) * self.layer2(x)\n",
        "\n",
        "# --- Advanced Cross-Attention/Self-Attention Module ---\n",
        "class MultiHeadAttentionModule(nn.Module):\n",
        "    # Initialization function\n",
        "    # dim: feature dimension, num_heads: number of attention heads\n",
        "    def __init__(self, dim, num_heads=1):\n",
        "        super(MultiHeadAttentionModule, self).__init__()\n",
        "        # Feature dimension\n",
        "        self.dim = dim\n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.head_dim = dim // num_heads\n",
        "        # Ensure dimension is divisible by the number of heads\n",
        "        if self.head_dim * num_heads != self.dim:\n",
        "            raise ValueError(\"dim must be divisible by num_heads\")\n",
        "\n",
        "        # Linear layer to generate Key\n",
        "        self.K_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Value\n",
        "        self.V_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Query\n",
        "        self.Q_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Softmax layer, used to calculate attention weights\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        # Fully connected layer before output\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "    # Forward propagation function\n",
        "    # feat1_query is Query, feat2_key_value is Key and Value\n",
        "    # mask: optional attention mask\n",
        "    def forward(self, feat1_query, feat2_key_value, mask=None):\n",
        "        # Query shape: (batch_size, Query_sequence_length, Query_dimension)\n",
        "        B_q, N_q, C_q = feat1_query.shape\n",
        "        # Key/Value shape: (batch_size, Key/Value_sequence_length, Key/Value_dimension)\n",
        "        B_kv, N_kv, C_kv = feat2_key_value.shape\n",
        "\n",
        "        # Check if batch sizes of Query and Key/Value match\n",
        "        if B_q != B_kv: raise ValueError(f\"Batch sizes do not match: Query is {B_q}, Key/Value is {B_kv}\")\n",
        "\n",
        "        # Generate Q, K, V and adjust shape for multi-head: (batch, num_heads, sequence_length, head_dimension)\n",
        "        # Q: (B, N_q, C_q) -> (B, N_q, num_heads, head_dim) -> (B, num_heads, N_q, head_dim)\n",
        "        Q = self.Q_layer(feat1_query).reshape(B_q, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        K = self.K_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        V = self.V_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate dot product of Q and K_transpose and scale ( scaled_dot_product = (Q @ K.T) / sqrt(head_dim) )\n",
        "        # dots shape: (B, num_heads, N_q, N_kv)\n",
        "        dots = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # If a mask is provided (usually a padding mask for K,V sequence, shape B, N_kv)\n",
        "        if mask is not None:\n",
        "            # unsqueeze expands the mask to (B, 1, 1, N_kv) to match the shape of dots (B, nH, N_q, N_kv) for broadcasting\n",
        "            # Fill positions in dots where mask is 0 (i.e., padding positions) with a very small value, so their weight approaches 0 after softmax\n",
        "            dots = dots.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "\n",
        "        # Calculate attention weights (attn_weights shape: B, num_heads, N_q, N_kv)\n",
        "        attn_weights = self.attend(dots)\n",
        "        # Attention weights weighted V (out shape: B, num_heads, N_q, head_dim)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # Permute and merge multi-head results: (B, num_heads, N_q, head_dim) -> (B, N_q, num_heads, head_dim) -> (B, N_q, dim)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B_q, N_q, self.dim)\n",
        "        # Pass through output fully connected layer\n",
        "        out = self.fc_out(out)\n",
        "        # Return final output\n",
        "        return out\n",
        "\n",
        "# --- Adapted Single Stream Processor ---\n",
        "class AdaptedSingleStreamProcessor(nn.Module):\n",
        "    # Initialization function\n",
        "    # audio_video_input_dim: Input dimension after audio/video projection\n",
        "    # bert_hidden_size: BERT's hidden layer size\n",
        "    # max_bert_len_for_lstm: Maximum input sequence length expected by the internal text LSTM\n",
        "    # lstm_hidden_size: Hidden size of the internal text LSTM\n",
        "    # attention_token_dim: Dimension of attention tokens\n",
        "    # num_attention_tokens_per_modal: Number of tokens output after processing each modality\n",
        "    # active_modalities: Tuple of active modalities, e.g., ('audio', 'video', 'text')\n",
        "    # num_ca_sa_heads: Number of heads for cross-attention and self-attention modules\n",
        "    # dropout_rate: Dropout rate for the text FC part\n",
        "    def __init__(self, audio_video_input_dim, bert_hidden_size, max_bert_len_for_lstm,\n",
        "                 lstm_hidden_size, attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 active_modalities=('audio', 'video', 'text'), num_ca_sa_heads=1, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        # Number of tokens output after processing each modality\n",
        "        self.n_tokens_per_modal = num_attention_tokens_per_modal\n",
        "        # Dimension of attention tokens\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        # Maximum input sequence length expected by the internal text LSTM\n",
        "        self.max_bert_len_for_lstm_input = max_bert_len_for_lstm\n",
        "        # Active modalities\n",
        "        self.active_modalities = active_modalities\n",
        "        # Flattened feature dimension output by each modality processor (N * C)\n",
        "        self.expected_feature_dim_after_mod_proc = self.n_tokens_per_modal * self.attention_token_dim\n",
        "\n",
        "        # Audio feature processor: receives projected features, maps to NxC token representation\n",
        "        self.audio_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc) # Layer normalization\n",
        "        )\n",
        "        # Video feature processor: logic same as audio\n",
        "        self.vision_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "        # Text processing: BERT hidden state -> LSTM -> Fully connected layer -> NxC token representation\n",
        "        # Text LSTM processor\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        # Text FC processor, maps LSTM output to token representation\n",
        "        self.text_fc_processor_to_tokens = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), # Dropout layer\n",
        "            # LSTM output is (B, S_lstm, H_lstm), after reshape it's (B, S_lstm * H_lstm)\n",
        "            GLULinear(lstm_hidden_size * self.max_bert_len_for_lstm_input, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "\n",
        "        # Attention module instantiation\n",
        "        # ZA: Audio cross-attention (query is concatenation of all modalities, key/value are audio tokens)\n",
        "        self.ZA = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZV: Video cross-attention\n",
        "        self.ZV = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZT: Text cross-attention\n",
        "        self.ZT = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # SA_stream: Intra-stream self-attention\n",
        "        self.SA_stream = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # Final output dimension of this stream processor (after averaging SA output, or, dimension of a single token)\n",
        "        self.output_final_dim = attention_token_dim\n",
        "\n",
        "    # Forward propagation function\n",
        "    # audio_input_proj, vision_input_proj from Hierarchical LSTM + Projector layer (B, D_projector)\n",
        "    # text_sequence_input_bert is BERT's hidden state (B, S_bert, D_bert)\n",
        "    def forward(self, audio_input_proj, vision_input_proj, text_sequence_input_bert):\n",
        "        # Dynamically determine batch size\n",
        "        b = 0\n",
        "        if audio_input_proj is not None and audio_input_proj.nelement() > 0: b = audio_input_proj.shape[0]\n",
        "        elif vision_input_proj is not None and vision_input_proj.nelement() > 0: b = vision_input_proj.shape[0]\n",
        "        elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: b = text_sequence_input_bert.shape[0]\n",
        "\n",
        "        # Handle empty batch (all inputs are empty or None)\n",
        "        if b == 0:\n",
        "            dev = torch.device(\"cpu\") # Default device\n",
        "            # Try to get device from valid input\n",
        "            if audio_input_proj is not None and audio_input_proj.nelement() > 0: dev = audio_input_proj.device\n",
        "            elif vision_input_proj is not None and vision_input_proj.nelement() > 0: dev = vision_input_proj.device\n",
        "            elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: dev = text_sequence_input_bert.device\n",
        "\n",
        "            # Create empty flat features and stream output\n",
        "            empty_flat = torch.zeros(0, self.expected_feature_dim_after_mod_proc, device=dev)\n",
        "            empty_stream_out = torch.zeros(0, 1, self.output_final_dim, device=dev)\n",
        "            # Return empty flat features for contrastive loss and empty stream output\n",
        "            return empty_flat, empty_flat, empty_flat, empty_stream_out\n",
        "\n",
        "        # Get current device (ensure at least one valid input to determine device)\n",
        "        device = audio_input_proj.device if audio_input_proj is not None and audio_input_proj.nelement() > 0 else \\\n",
        "                 (vision_input_proj.device if vision_input_proj is not None and vision_input_proj.nelement() > 0 else \\\n",
        "                  text_sequence_input_bert.device)\n",
        "\n",
        "        # Initialize flat features for contrastive loss (audio_f_flat) and token features for attention (audio_f_tokens)\n",
        "        # Audio processing\n",
        "        audio_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        audio_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        # If audio modality is active, input is not empty, and input is not all zeros (indicates actual content)\n",
        "        if 'audio' in self.active_modalities and audio_input_proj is not None and audio_input_proj.nelement() > 0 and audio_input_proj.abs().sum() > 1e-9 :\n",
        "            audio_f_flat = self.audio_feat_processor_to_tokens(audio_input_proj) # (B, N*C)\n",
        "            audio_f_tokens = audio_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Video processing (logic same as audio)\n",
        "        vis_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        vis_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'video' in self.active_modalities and vision_input_proj is not None and vision_input_proj.nelement() > 0 and vision_input_proj.abs().sum() > 1e-9:\n",
        "            vis_f_flat = self.vision_feat_processor_to_tokens(vision_input_proj)\n",
        "            vis_f_tokens = vis_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim)\n",
        "\n",
        "        # Text processing\n",
        "        text_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        text_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0 and text_sequence_input_bert.abs().sum() > 1e-9:\n",
        "            # Get current BERT output sequence length\n",
        "            current_bert_seq_len = text_sequence_input_bert.shape[1]\n",
        "            text_sequence_input_bert_adjusted = text_sequence_input_bert\n",
        "            # Adjust BERT output sequence length to match LSTM expected input\n",
        "            if current_bert_seq_len != self.max_bert_len_for_lstm_input:\n",
        "                if current_bert_seq_len > self.max_bert_len_for_lstm_input: # Truncate if too long\n",
        "                    text_sequence_input_bert_adjusted = text_sequence_input_bert[:, :self.max_bert_len_for_lstm_input, :]\n",
        "                else: # Pad with zeros if too short\n",
        "                    padding_needed = self.max_bert_len_for_lstm_input - current_bert_seq_len\n",
        "                    # Create padding tensor (B, padding_needed, D_bert)\n",
        "                    padding_tensor = torch.zeros(b, padding_needed, text_sequence_input_bert.shape[2], device=device)\n",
        "                    # Concatenate original BERT output and padding tensor\n",
        "                    text_sequence_input_bert_adjusted = torch.cat([text_sequence_input_bert, padding_tensor], dim=1)\n",
        "\n",
        "            # Pass through text LSTM\n",
        "            lstm_output, _ = self.text_lstm_processor(text_sequence_input_bert_adjusted) # (B, S_lstm, H_lstm)\n",
        "            # Flatten LSTM output: (B, S_lstm * H_lstm)\n",
        "            text_f_flat_from_lstm = lstm_output.reshape(b, -1)\n",
        "            # Process flattened LSTM output through FC layer\n",
        "            text_f_flat = self.text_fc_processor_to_tokens(text_f_flat_from_lstm) # (B, N*C)\n",
        "            # Reshape to token form\n",
        "            text_f_tokens = text_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Collect tokens from active modalities with content\n",
        "        active_mod_token_lists = []\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(audio_f_tokens)\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(vis_f_tokens)\n",
        "        if 'text'  in self.active_modalities and text_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(text_f_tokens)\n",
        "\n",
        "        # If there are no active modalities with content\n",
        "        if not active_mod_token_lists:\n",
        "            # Return flat features and zero stream output (because there's no content for attention calculation)\n",
        "            return audio_f_flat, vis_f_flat, text_f_flat, torch.zeros(b, 1, self.output_final_dim, device=device)\n",
        "\n",
        "        # Concatenate tokens of active modalities as Query for cross-attention\n",
        "        # query_for_modality_ca shape: (B, num_active_modalities * N, C_token)\n",
        "        query_for_modality_ca = torch.cat(active_mod_token_lists, dim=1)\n",
        "\n",
        "        # Perform inter-modality cross-attention\n",
        "        # Initialize result tensor\n",
        "        res_za, res_zv, res_zt = torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca)\n",
        "        # If audio is active and has content\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9:\n",
        "            # query_for_modality_ca as Query, audio_f_tokens as Key and Value\n",
        "            res_za = self.ZA(query_for_modality_ca, audio_f_tokens)\n",
        "        # If video is active and has content\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zv = self.ZV(query_for_modality_ca, vis_f_tokens)\n",
        "        # If text is active and has content\n",
        "        if 'text' in self.active_modalities and text_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zt = self.ZT(query_for_modality_ca, text_f_tokens)\n",
        "\n",
        "        # Merge cross-attention results (element-wise addition)\n",
        "        feat_after_mod_ca = res_za + res_zv + res_zt\n",
        "        # Intra-stream self-attention, with residual connection\n",
        "        # feat_after_mod_ca as Query, Key, and Value\n",
        "        feat_after_sa_stream = self.SA_stream(feat_after_mod_ca, feat_after_mod_ca) + feat_after_mod_ca\n",
        "        # Average the features after self-attention along the sequence dimension to get the final stream representation\n",
        "        stream_output_representation = torch.mean(feat_after_sa_stream, dim=1) # (B, C_token)\n",
        "\n",
        "        # Return flat features for contrastive loss, and the final stream output representation (add a dimension to match the expected (B, 1, C_token) shape)\n",
        "        return audio_f_flat, vis_f_flat, text_f_flat, stream_output_representation.unsqueeze(1)\n",
        "\n",
        "\n",
        "# --- Main Model: ContextPunchlineHumorModelNew ---\n",
        "class ContextPunchlineHumorModelNew(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self,\n",
        "                 bert_model_name_or_path,\n",
        "                 audio_word_dim, video_word_dim,\n",
        "                 sentence_lstm_hidden_dim, sample_lstm_hidden_dim, hier_lstm_dropout,\n",
        "                 projector_output_dim,\n",
        "                 bert_hidden_size_actual, max_bert_len_for_lstm,\n",
        "                 text_lstm_hidden_size_in_stream,\n",
        "                 attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 stream_ca_sa_heads, stream_dropout_rate,\n",
        "                 final_cross_attention_heads, # MODIFIED: This will now be used for the new final fusion heads\n",
        "                 mlp_hidden_dim, num_classes,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(bert_model_name_or_path)\n",
        "\n",
        "        self.ctx_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.ctx_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.context_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        self.pl_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.pl_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.punchline_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Final Fusion ---\n",
        "        self.final_fusion_input_dim = attention_token_dim\n",
        "\n",
        "        self.final_ca_query_streams_on_ctx_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_ca_query_streams_on_pl_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_fusion_sa_after_ca_sum = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        # REMOVED: self.cross_attention_final\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.final_fusion_input_dim, mlp_hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(mlp_hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self,\n",
        "                ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl,\n",
        "                ctx_padded_video, ctx_video_sl, ctx_video_ssl,\n",
        "                ctx_input_ids, ctx_attention_mask,\n",
        "                pl_padded_audio, pl_audio_sl, pl_audio_ssl,\n",
        "                pl_padded_video, pl_video_sl, pl_video_ssl,\n",
        "                pl_input_ids, pl_attention_mask,\n",
        "                current_modality_config=None, tokenizer_for_padding=None\n",
        "                ):\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_a = self.ctx_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_a_vec = torch.zeros(ctx_padded_audio.shape[0], actual_hier_lstm_output_dim_ctx_a, device=ctx_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(ctx_audio_sl > 0):\n",
        "                ctx_a_vec = self.ctx_audio_hier_lstm(ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_v = self.ctx_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_v_vec = torch.zeros(ctx_padded_video.shape[0], actual_hier_lstm_output_dim_ctx_v, device=ctx_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(ctx_video_sl > 0):\n",
        "                ctx_v_vec = self.ctx_video_hier_lstm(ctx_padded_video, ctx_video_sl, ctx_video_ssl)\n",
        "\n",
        "        ctx_a_proj = self.ctx_audio_projector(ctx_a_vec)\n",
        "        ctx_v_proj = self.ctx_video_projector(ctx_v_vec)\n",
        "\n",
        "        ctx_bert_hs = torch.zeros(ctx_input_ids.shape[0], ctx_input_ids.shape[1], self.bert_model.config.hidden_size, device=ctx_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(ctx_attention_mask.sum(dim=1) > 0):\n",
        "                ctx_bert_outputs = self.bert_model(input_ids=ctx_input_ids, attention_mask=ctx_attention_mask)\n",
        "                ctx_bert_hs = ctx_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        ctx_audio_f_flat, ctx_vis_f_flat, ctx_text_f_flat, ctx_stream_repr = self.context_processor(ctx_a_proj, ctx_v_proj, ctx_bert_hs)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_a = self.pl_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_a_vec = torch.zeros(pl_padded_audio.shape[0], actual_hier_lstm_output_dim_pl_a, device=pl_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(pl_audio_sl > 0):\n",
        "                pl_a_vec = self.pl_audio_hier_lstm(pl_padded_audio, pl_audio_sl, pl_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_v = self.pl_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_v_vec = torch.zeros(pl_padded_video.shape[0], actual_hier_lstm_output_dim_pl_v, device=pl_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(pl_video_sl > 0):\n",
        "                pl_v_vec = self.pl_video_hier_lstm(pl_padded_video, pl_video_sl, pl_video_ssl)\n",
        "\n",
        "        pl_a_proj = self.pl_audio_projector(pl_a_vec)\n",
        "        pl_v_proj = self.pl_video_projector(pl_v_vec)\n",
        "\n",
        "        pl_bert_hs = torch.zeros(pl_input_ids.shape[0], pl_input_ids.shape[1], self.bert_model.config.hidden_size, device=pl_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(pl_attention_mask.sum(dim=1) > 0):\n",
        "                pl_bert_outputs = self.bert_model(input_ids=pl_input_ids, attention_mask=pl_attention_mask)\n",
        "                pl_bert_hs = pl_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        pl_audio_f_flat, pl_vis_f_flat, pl_text_f_flat, pl_stream_repr = self.punchline_processor(pl_a_proj, pl_v_proj, pl_bert_hs)\n",
        "\n",
        "        # --- MODIFIED: New final fusion logic ---\n",
        "        streams_query = torch.cat((ctx_stream_repr, pl_stream_repr), dim=1)\n",
        "        res_ca_ctx = self.final_ca_query_streams_on_ctx_kv(streams_query, ctx_stream_repr)\n",
        "        res_ca_pl = self.final_ca_query_streams_on_pl_kv(streams_query, pl_stream_repr)\n",
        "        fused_after_ca = res_ca_ctx + res_ca_pl\n",
        "        fused_after_sa = self.final_fusion_sa_after_ca_sum(fused_after_ca, fused_after_ca)\n",
        "        fused_after_sa = fused_after_sa + fused_after_ca # Residual connection for the self-attention on fused representations\n",
        "        fused_representation = torch.mean(fused_after_sa, dim=1)\n",
        "\n",
        "        mlp_out = self.mlp(fused_representation)\n",
        "        logits = self.classifier(mlp_out)\n",
        "\n",
        "        contrastive_features = {\n",
        "            'ctx_audio': ctx_audio_f_flat, 'ctx_video': ctx_vis_f_flat, 'ctx_text': ctx_text_f_flat,\n",
        "            'pl_audio': pl_audio_f_flat, 'pl_video': pl_vis_f_flat, 'pl_text': pl_text_f_flat\n",
        "        }\n",
        "        return logits, contrastive_features\n",
        "\n",
        "\n",
        "# --- Contrastive Loss Function ---\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    # Initialization function, temperature coefficient\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        # Use CrossEntropyLoss to calculate loss (SimCLR style)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward propagation function\n",
        "    # emb_i, emb_j are embeddings from different modalities or views (B, D)\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        # Get batch size\n",
        "        batch_size = emb_i.shape[0]\n",
        "        # Contrastive loss requires at least 2 samples to compute, otherwise return 0 loss\n",
        "        if batch_size <= 1:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # Check if embeddings are all zeros, if so, loss is 0 (to avoid NaN)\n",
        "        # If the sum of absolute values of all elements in either embedding tensor is less than a very small value, it is considered empty or all zeros\n",
        "        if emb_i.abs().sum() < 1e-9 or emb_j.abs().sum() < 1e-9:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # L2 normalize embedding vectors\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        # Concatenate the two groups of normalized embeddings along the batch dimension: (2*B, D)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Calculate similarity matrix (cosine similarity between all sample pairs, then divide by temperature)\n",
        "        # (2*B, D) @ (D, 2*B) -> (2*B, 2*B)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "\n",
        "        # Create labels: for each sample in z_i, its positive sample is the corresponding sample in z_j\n",
        "        # For example, row similarity_matrix[0] is the similarity of z_i[0] with all representations\n",
        "        # Its positive sample z_j[0] has index batch_size + 0 in representations\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        # For each sample in z_j, its positive sample is the corresponding sample in z_i\n",
        "        # For example, row similarity_matrix[batch_size+0] is the similarity of z_j[0] with all representations\n",
        "        # Its positive sample z_i[0] has index 0 in representations\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Calculate loss, separately for z_i querying z_j and z_j querying z_i\n",
        "        # loss_i: z_i as anchor, corresponding sample in z_j as positive\n",
        "        # similarity_matrix[:batch_size] is the similarity of z_i with all representations (B, 2*B)\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        # loss_j: z_j as anchor, corresponding sample in z_i as positive\n",
        "        # similarity_matrix[batch_size:] is the similarity of z_j with all representations (B, 2*B)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        # Return average loss\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "\n",
        "# --- Modified Training Function (Only contrastive loss calculation method is changed) ---\n",
        "def train_new_model(model, data_loader, optimizer, scheduler,\n",
        "                    bce_criterion, contrastive_loss_fn, device, epoch, num_epochs,\n",
        "                    contrastive_loss_weight, current_modality_config, tokenizer_for_padding):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # The BERT part of the model is already globally frozen externally, no explicit model.bert_model.eval() needed here\n",
        "\n",
        "    # Initialize total BCE loss, total contrastive loss, total loss\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0 # Used to accumulate final_simclr_loss_for_batch for each batch\n",
        "    total_loss = 0\n",
        "    # Create tqdm progress bar to display training progress\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    # Iterate through each batch in the data loader\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack data from collate_fn\n",
        "        (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "         pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "         labels) = batch\n",
        "\n",
        "        # Get current batch size\n",
        "        current_batch_size = ctx_a_feat.shape[0]\n",
        "        # If batch is empty, skip\n",
        "        if current_batch_size == 0: continue\n",
        "\n",
        "        # Move data to the specified device (excluding the last label)\n",
        "        batch_data_on_device = []\n",
        "        for tensor_item in batch[:-1]:\n",
        "            batch_data_on_device.append(tensor_item.to(device))\n",
        "        # Move labels to device and convert to long type\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        # Clear optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model forward pass, get classification logits and contrastive features\n",
        "        logits, contrastive_feats = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "        # Calculate BCE classification loss\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # --- Contrastive Loss Calculation (modified to averaging method) ---\n",
        "        final_simclr_loss_for_batch = torch.tensor(0.0, device=device) # Initialize contrastive loss for this batch\n",
        "        if current_batch_size > 1 and contrastive_loss_weight > 0:\n",
        "            accumulated_contrastive_loss_components = []\n",
        "\n",
        "            # Contrastive loss for the context stream\n",
        "            ctx_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_video'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_video'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_video'], contrastive_feats['ctx_text']))\n",
        "\n",
        "            if ctx_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(ctx_individual_losses)))\n",
        "\n",
        "            # Contrastive loss for the punchline stream\n",
        "            pl_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_video'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_video'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_video'], contrastive_feats['pl_text']))\n",
        "\n",
        "            if pl_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(pl_individual_losses)))\n",
        "\n",
        "            # Calculate the final contrastive loss (if multiple components exist, take their average)\n",
        "            if accumulated_contrastive_loss_components:\n",
        "                final_simclr_loss_for_batch = torch.mean(torch.stack(accumulated_contrastive_loss_components))\n",
        "            # else: final_simclr_loss_for_batch remains its initial value of 0.0\n",
        "\n",
        "        # Total loss = BCE loss + contrastive_loss_weight * calculated batch contrastive loss\n",
        "        current_loss = bce_loss + contrastive_loss_weight * final_simclr_loss_for_batch\n",
        "\n",
        "        # Backpropagate to calculate gradients\n",
        "        current_loss.backward()\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        # If a learning rate scheduler is used\n",
        "        if scheduler is not None:\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss values (item() gets scalar value)\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += final_simclr_loss_for_batch.item() # Accumulate the calculated batch contrastive loss\n",
        "        total_loss += current_loss.item()\n",
        "        # Update progress bar display information\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{final_simclr_loss_for_batch.item():.4f}\")\n",
        "\n",
        "    # If data loader is not empty\n",
        "    if len(data_loader) > 0:\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        # Print average training loss for the current epoch\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "\n",
        "\n",
        "# --- Validation/Test Function (Added F1 Score) ---\n",
        "def validate_or_test_new_model(model, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                               current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize total BCE loss\n",
        "    total_bce_loss = 0\n",
        "    # List to store all prediction results\n",
        "    all_preds = []\n",
        "    # List to store all true labels\n",
        "    all_labels = []\n",
        "\n",
        "    # Set progress bar description (corrected logic)\n",
        "    if mode == \"Test\" and epoch is None:\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # and epoch is not None (implicitly for this branch after the first)\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Val\": # epoch should not be None for validation\n",
        "        desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    else: # Fallback, though ideally all cases are covered\n",
        "        desc = f\"Processing [{mode} {current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    # Do not calculate gradients within this block to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through each batch in the data loader\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            # Unpack data\n",
        "            (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "             pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "             labels) = batch\n",
        "\n",
        "            # Get current batch size\n",
        "            current_batch_size = ctx_a_feat.shape[0]\n",
        "            # If batch is empty, skip\n",
        "            if current_batch_size == 0: continue\n",
        "\n",
        "            # Move data to device\n",
        "            batch_data_on_device = [t.to(device) for t in batch[:-1]]\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            # Model forward pass, ignore contrastive features (not needed during validation/testing)\n",
        "            logits, _ = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "            # Calculate BCE loss\n",
        "            bce_loss = bce_criterion(logits, labels)\n",
        "            # Accumulate BCE loss\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            # Get predicted class (index of the max value in logits)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Store prediction results (convert to numpy array)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            # Store true labels (convert to numpy array)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # If data loader is empty or no labels were collected\n",
        "    if len(data_loader) == 0 or len(all_labels) == 0 :\n",
        "        print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty.\")\n",
        "        if mode == \"Val\": return 0.0, 0.0 # Validation mode returns 0.0 accuracy, 0.0 F1\n",
        "        return 0.0, 0.0, 0.0 # Test mode returns 0.0 loss, 0.0 accuracy, 0.0 F1\n",
        "\n",
        "    # Calculate average BCE loss\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    # Calculate accuracy (if label list is not empty)\n",
        "    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
        "    # Calculate F1 score (if label list is not empty), use 'binary' because it's binary classification, zero_division handles boundary cases\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0) if all_labels else 0.0\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # If validation mode, return accuracy and F1\n",
        "    if mode == \"Val\": return accuracy, f1\n",
        "    # If test mode, return average loss, accuracy, and F1\n",
        "    return avg_bce_loss, accuracy, f1\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameter Configuration ---\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # SENTENCE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    # SAMPLE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    HIER_LSTM_DROPOUT = 0.3\n",
        "\n",
        "    PROJECTOR_OUTPUT_DIM = 1024\n",
        "\n",
        "    MAX_BERT_LEN_FOR_PART_DATASET = 512\n",
        "    TEXT_LSTM_HIDDEN_SIZE_IN_STREAM = 256   # This version of the code still uses this parameter\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    STREAM_CA_SA_HEADS = 1\n",
        "    STREAM_DROPOUT_RATE = 0.3\n",
        "\n",
        "    FINAL_CROSS_ATTENTION_HEADS = 1         # Used for all attention modules in the new final fusion structure\n",
        "    MLP_HIDDEN_DIM = 256\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    BATCH_SIZE = 16 # Warning: The new final fusion structure is more complex, may need to reduce this value\n",
        "    LEARNING_RATE = 8e-5\n",
        "    NUM_EPOCHS = 4 # It is recommended to increase epochs for actual use\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"BERT model used: {BERT_MODEL_NAME_FOR_MAIN}\")\n",
        "    print(f\"Hierarchical LSTM: Sentence-level hidden dim {SENTENCE_LSTM_HIDDEN_DIM_CONFIG}, Sample-level hidden dim {SAMPLE_LSTM_HIDDEN_DIM_CONFIG}, Dropout {HIER_LSTM_DROPOUT}\")\n",
        "    print(f\"Projector output dimension (Stream processor audio/video input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"Max BERT length for context/punchline part: {MAX_BERT_LEN_FOR_PART_DATASET}\")\n",
        "    print(f\"Stream processor internal text LSTM hidden size: {TEXT_LSTM_HIDDEN_SIZE_IN_STREAM}\") # Keep printing as this param is still in model def\n",
        "    print(f\"Attention token dimension: {ATTENTION_TOKEN_DIM}, Tokens per modality: {NUM_ATTENTION_TOKENS_PER_MODAL}\")\n",
        "    print(f\"Stream processor attention heads: {STREAM_CA_SA_HEADS}, Stream processor text FC Dropout rate: {STREAM_DROPOUT_RATE}\")\n",
        "    print(f\"Final fusion stage attention heads: {FINAL_CROSS_ATTENTION_HEADS}, MLP hidden dimension: {MLP_HIDDEN_DIM}\")\n",
        "    print(f\"Training parameters: Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}, Epochs {NUM_EPOCHS}\")\n",
        "    print(f\"Contrastive loss: Temperature {TEMPERATURE_CONTRASTIVE}, Weight {CONTRASTIVE_LOSS_WEIGHT}\")\n",
        "    print(\"\\n !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \\n\")\n",
        "\n",
        "    # --- Load Raw Data ---\n",
        "    print(\"Loading raw data pickle files...\")\n",
        "    # Ensure paths are correct\n",
        "    # Example: data_folds_path = \"path_to_your_gdrive/Project_CCS2-main/sdk_features/data_folds.pkl\"\n",
        "    # Replace with your actual paths\n",
        "    # To run locally, you might need to download these files or adjust paths\n",
        "    # For demonstration, we'll assume files might not exist and add checks or placeholders.\n",
        "    try:\n",
        "        data_folds = load_pickle(data_folds_path)\n",
        "        language_sdk = load_pickle(language_file)\n",
        "        covarep_sdk = load_pickle(covarep_file)\n",
        "        openface_sdk = load_pickle(openface_file)\n",
        "        humor_label_sdk = load_pickle(humor_label_file)\n",
        "        print(\"Raw data loading complete.\")\n",
        "\n",
        "        train_ids = data_folds['train']\n",
        "        dev_ids = data_folds['dev']\n",
        "        test_ids = data_folds['test']\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: One or more data files not found. Please check paths and ensure files exist.\")\n",
        "        print(\"Using placeholder data for demonstration.\")\n",
        "        # Placeholder data for demonstration if files are missing\n",
        "        train_ids, dev_ids, test_ids = ['h1','h2'], ['h3'], ['h4']\n",
        "        language_sdk = {\n",
        "            f'h{i}': {'punchline_sentence': f'Punchline {i}', 'context_sentences': [f'Context sent {i}.1', f'Context sent {i}.2']} for i in range(1, 5)\n",
        "        }\n",
        "        covarep_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _AUDIO_WORD_DIM_CONST).astype(np.float32) if i % 2 == 0 else [], # Some empty\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _AUDIO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        openface_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _VIDEO_WORD_DIM_CONST).astype(np.float32),\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _VIDEO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        humor_label_sdk = {f'h{i}': float(i % 2) for i in range(1,5)}\n",
        "\n",
        "\n",
        "    print(\"Extracting features and labels...\")\n",
        "    (train_ps, train_cs, train_cvp_p, train_cvp_c, train_of_p, train_of_c, train_labels) = \\\n",
        "        extract_features_and_labels(train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (dev_ps, dev_cs, dev_cvp_p, dev_cvp_c, dev_of_p, dev_of_c, dev_labels) = \\\n",
        "        extract_features_and_labels(dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (test_ps, test_cs, test_cvp_p, test_cvp_c, test_of_p, test_of_c, test_labels) = \\\n",
        "        extract_features_and_labels(test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    print(\"Feature and label extraction complete.\")\n",
        "\n",
        "    print(\"Structuring data for new dataset format (context/punchline split)...\")\n",
        "    train_sample_data_dicts = concatenate_multimodal_data_for_dataset(train_cvp_c, train_of_c, train_cs, train_cvp_p, train_of_p, train_ps)\n",
        "    dev_sample_data_dicts = concatenate_multimodal_data_for_dataset(dev_cvp_c, dev_of_c, dev_cs, dev_cvp_p, dev_of_p, dev_ps)\n",
        "    test_sample_data_dicts = concatenate_multimodal_data_for_dataset(test_cvp_c, test_of_c, test_cs, test_cvp_p, test_of_p, test_ps)\n",
        "    print(\"Data structuring complete.\")\n",
        "\n",
        "    print(\"Initializing BERT tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    _bert_temp_model = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = _bert_temp_model.config.hidden_size\n",
        "    del _bert_temp_model\n",
        "    print(f\"Actual BERT hidden size: {BERT_HIDDEN_SIZE_ACTUAL}\")\n",
        "\n",
        "    print(\"Creating CustomFeatureDatasetContextPunchline instances...\")\n",
        "    train_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        train_sample_data_dicts, train_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        dev_sample_data_dicts, dev_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    test_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        test_sample_data_dicts, test_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=custom_collate_fn_context_punchline, drop_last=True if BATCH_SIZE > 1 and len(train_dataset) > BATCH_SIZE else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    print(f\"Dataloaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        {'name': 'AT',  'audio': True,  'video': False,  'text': True},\n",
        "        {'name': 'VT',  'audio': False,  'video': True,  'text': True},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    for config_idx, model_config_iter in enumerate(modality_configurations):\n",
        "        config_name = model_config_iter['name']\n",
        "        print(f\"\\n--- Starting processing for model config: {config_name} ---\")\n",
        "\n",
        "        model = ContextPunchlineHumorModelNew(\n",
        "            bert_model_name_or_path=BERT_MODEL_NAME_FOR_MAIN,\n",
        "            audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=SAMPLE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            hier_lstm_dropout=HIER_LSTM_DROPOUT,\n",
        "            projector_output_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size_actual=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len_for_lstm=MAX_BERT_LEN_FOR_PART_DATASET, # This is max_bert_len_for_part\n",
        "            text_lstm_hidden_size_in_stream=TEXT_LSTM_HIDDEN_SIZE_IN_STREAM,\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            stream_ca_sa_heads=STREAM_CA_SA_HEADS,\n",
        "            stream_dropout_rate=STREAM_DROPOUT_RATE,\n",
        "            final_cross_attention_heads=FINAL_CROSS_ATTENTION_HEADS,\n",
        "            mlp_hidden_dim=MLP_HIDDEN_DIM,\n",
        "            num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        print(\"Freezing BERT parameters in the main model...\")\n",
        "        for param in model.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen.\")\n",
        "\n",
        "        bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "        optimizer_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = AdamW(optimizer_params, lr=LEARNING_RATE)\n",
        "        scheduler = None\n",
        "        if len(train_loader) > 0 and NUM_EPOCHS > 0:\n",
        "            num_training_steps_per_epoch = len(train_loader)\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs.\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_val_f1_at_best_acc = 0.0 # Store F1 at the point of best accuracy\n",
        "        best_model_state_path = f\"best_model_{config_name}.pth\"\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader for {config_name} is empty. Skipping training.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_new_model(model, train_loader, optimizer, scheduler, bce_criterion,\n",
        "                                contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS,\n",
        "                                CONTRASTIVE_LOSS_WEIGHT, model_config_iter, bert_tokenizer_global)\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy, val_f1 = validate_or_test_new_model(model, val_loader, bce_criterion, DEVICE,\n",
        "                                                                    epoch, NUM_EPOCHS, model_config_iter,\n",
        "                                                                    bert_tokenizer_global, mode=\"Val\")\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        best_val_f1_at_best_acc = val_f1 # Save F1 at this best accuracy point\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f} (F1: {best_val_f1_at_best_acc:.4f}). Saving model...\")\n",
        "                        torch.save(model.state_dict(), best_model_state_path)\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (corresponding F1: {best_val_f1_at_best_acc:.4f})\")\n",
        "\n",
        "        print(f\"\\nStarting test phase for {config_name}...\")\n",
        "        test_accuracy, test_f1, test_loss = 0.0, 0.0, 0.0 # Initialize\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader for {config_name} is empty. Skipping test.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                                               'test_acc': 0.0, 'test_f1':0.0, 'test_loss': 0.0}\n",
        "        else:\n",
        "            if os.path.exists(best_model_state_path) and best_val_accuracy_for_config > 0: # Check if model was saved\n",
        "                print(f\"Loading best model state from {best_model_state_path} for testing.\")\n",
        "                model.load_state_dict(torch.load(best_model_state_path, map_location=DEVICE))\n",
        "            elif best_val_accuracy_for_config == 0 and len(train_loader) > 0 : # Was trained, but no improvement or no val\n",
        "                print(f\"No best validation model saved (or validation accuracy was 0), using model from last training epoch for testing.\")\n",
        "            elif len(train_loader) == 0: # Not trained\n",
        "                print(f\"No training was performed for {config_name}. Testing with initialized model (results might be poor).\")\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test_new_model(\n",
        "                model, test_loader, bce_criterion, DEVICE, epoch=None, num_epochs=NUM_EPOCHS, # epoch=None for final test\n",
        "                current_modality_config=model_config_iter, tokenizer_for_padding=bert_tokenizer_global, mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                'test_acc': test_accuracy, 'test_f1': test_f1, 'test_loss': test_loss,\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy: {results.get('val_acc', 0.0):.4f} (Corresponding Val F1: {results.get('val_f1', 0.0):.4f})\")\n",
        "        print(f\"  Test Set Accuracy: {results.get('test_acc', 0.0):.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results.get('test_f1', 0.0):.4f}\")\n",
        "        print(f\"  Test Set Loss: {results.get('test_loss', 0.0):.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "id": "JlukGDhSoj1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dffca95-8379-4af0-ec21-9b003eeb0a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT model used: bert-base-uncased\n",
            "Hierarchical LSTM: Sentence-level hidden dim 256, Sample-level hidden dim 512, Dropout 0.3\n",
            "Projector output dimension (Stream processor audio/video input): 1024\n",
            "Max BERT length for context/punchline part: 512\n",
            "Stream processor internal text LSTM hidden size: 256\n",
            "Attention token dimension: 32, Tokens per modality: 16\n",
            "Stream processor attention heads: 1, Stream processor text FC Dropout rate: 0.3\n",
            "Final fusion stage attention heads: 1, MLP hidden dimension: 256\n",
            "Training parameters: Batch size 16, Learning rate 8e-05, Epochs 4\n",
            "Contrastive loss: Temperature 0.5, Weight 0.03\n",
            "\n",
            " !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \n",
            "\n",
            "Loading raw data pickle files...\n",
            "Raw data loading complete.\n",
            "Extracting features and labels...\n",
            "Feature and label extraction complete.\n",
            "Structuring data for new dataset format (context/punchline split)...\n",
            "Data structuring complete.\n",
            "Initializing BERT tokenizer...\n",
            "Actual BERT hidden size: 768\n",
            "Creating CustomFeatureDatasetContextPunchline instances...\n",
            "Dataloaders created. Train batches: 475, Val batches: 62, Test batches: 63\n",
            "\n",
            "--- Starting processing for model config: AV ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for AV... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Train Avg Loss: 0.7929, BCE: 0.6913, SimCLR: 3.3839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Val Avg BCE: 0.6920, Accuracy: 0.5316, F1: 0.3200\n",
            "Epoch 1 (AV): New best validation accuracy: 0.5316 (F1: 0.3200). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Train Avg Loss: 0.7652, BCE: 0.6772, SimCLR: 2.9339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Val Avg BCE: 0.6793, Accuracy: 0.5592, F1: 0.5057\n",
            "Epoch 2 (AV): New best validation accuracy: 0.5592 (F1: 0.5057). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Train Avg Loss: 0.7600, BCE: 0.6715, SimCLR: 2.9523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Val Avg BCE: 0.6842, Accuracy: 0.5602, F1: 0.5130\n",
            "Epoch 3 (AV): New best validation accuracy: 0.5602 (F1: 0.5130). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Train Avg Loss: 0.7519, BCE: 0.6650, SimCLR: 2.8971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Val Avg BCE: 0.6811, Accuracy: 0.5673, F1: 0.5115\n",
            "Epoch 4 (AV): New best validation accuracy: 0.5673 (F1: 0.5115). Saving model...\n",
            "Training for AV complete. Best validation accuracy for this config: 0.5673 (corresponding F1: 0.5115)\n",
            "\n",
            "Starting test phase for AV...\n",
            "Loading best model state from best_model_AV.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (AV) Test Avg BCE: 0.6635, Accuracy: 0.6137, F1: 0.5789\n",
            "Final test results for AV -> Avg BCE Loss: 0.6635, Accuracy: 0.6137, F1: 0.5789\n",
            "\n",
            "--- Starting processing for model config: AT ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for AT... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Train Avg Loss: 0.7590, BCE: 0.6534, SimCLR: 3.5171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Val Avg BCE: 0.6158, Accuracy: 0.6622, F1: 0.7183\n",
            "Epoch 1 (AT): New best validation accuracy: 0.6622 (F1: 0.7183). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Train Avg Loss: 0.6879, BCE: 0.5909, SimCLR: 3.2332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Val Avg BCE: 0.6027, Accuracy: 0.6918, F1: 0.6962\n",
            "Epoch 2 (AT): New best validation accuracy: 0.6918 (F1: 0.6962). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Train Avg Loss: 0.6357, BCE: 0.5393, SimCLR: 3.2115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Val Avg BCE: 0.6088, Accuracy: 0.6888, F1: 0.6859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Train Avg Loss: 0.4387, BCE: 0.3406, SimCLR: 3.2705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Val Avg BCE: 0.8108, Accuracy: 0.6551, F1: 0.6302\n",
            "Training for AT complete. Best validation accuracy for this config: 0.6918 (corresponding F1: 0.6962)\n",
            "\n",
            "Starting test phase for AT...\n",
            "Loading best model state from best_model_AT.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (AT) Test Avg BCE: 0.5898, Accuracy: 0.6710, F1: 0.6785\n",
            "Final test results for AT -> Avg BCE Loss: 0.5898, Accuracy: 0.6710, F1: 0.6785\n",
            "\n",
            "--- Starting processing for model config: VT ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for VT... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Train Avg Loss: 0.7550, BCE: 0.6521, SimCLR: 3.4287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Val Avg BCE: 0.6135, Accuracy: 0.6857, F1: 0.6957\n",
            "Epoch 1 (VT): New best validation accuracy: 0.6857 (F1: 0.6957). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Train Avg Loss: 0.6829, BCE: 0.5896, SimCLR: 3.1106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Val Avg BCE: 0.5945, Accuracy: 0.6765, F1: 0.6742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Train Avg Loss: 0.6239, BCE: 0.5305, SimCLR: 3.1144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Val Avg BCE: 0.6210, Accuracy: 0.6714, F1: 0.6454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Train Avg Loss: 0.3752, BCE: 0.2790, SimCLR: 3.2091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Val Avg BCE: 0.8405, Accuracy: 0.6306, F1: 0.6082\n",
            "Training for VT complete. Best validation accuracy for this config: 0.6857 (corresponding F1: 0.6957)\n",
            "\n",
            "Starting test phase for VT...\n",
            "Loading best model state from best_model_VT.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (VT) Test Avg BCE: 0.5959, Accuracy: 0.6751, F1: 0.6891\n",
            "Final test results for VT -> Avg BCE Loss: 0.5959, Accuracy: 0.6751, F1: 0.6891\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AV\n",
            "  Best Validation Accuracy: 0.5673 (Corresponding Val F1: 0.5115)\n",
            "  Test Set Accuracy: 0.6137\n",
            "  Test Set F1 Score: 0.5789\n",
            "  Test Set Loss: 0.6635\n",
            "------------------------------\n",
            "Configuration: AT\n",
            "  Best Validation Accuracy: 0.6918 (Corresponding Val F1: 0.6962)\n",
            "  Test Set Accuracy: 0.6710\n",
            "  Test Set F1 Score: 0.6785\n",
            "  Test Set Loss: 0.5898\n",
            "------------------------------\n",
            "Configuration: VT\n",
            "  Best Validation Accuracy: 0.6857 (Corresponding Val F1: 0.6957)\n",
            "  Test Set Accuracy: 0.6751\n",
            "  Test Set F1 Score: 0.6891\n",
            "  Test Set Loss: 0.5959\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-head attention, new loss function, remove last self-attention layer\n",
        "# Import pickle module, used for serializing and deserializing Python object structures\n",
        "import pickle\n",
        "# Import numpy library, used for scientific computing, especially array operations\n",
        "import numpy as np\n",
        "# Import PyTorch library, an open-source machine learning framework\n",
        "import torch\n",
        "# Import PyTorch's neural network module\n",
        "import torch.nn as nn\n",
        "# Import PyTorch's neural network functional library\n",
        "import torch.nn.functional as F\n",
        "# Import Dataset and DataLoader classes from PyTorch, used for data loading\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Import auto tokenizer, auto model, and learning rate scheduler from transformers library\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "# Import AdamW optimizer from PyTorch\n",
        "from torch.optim import AdamW\n",
        "# Import accuracy and F1 score calculation functions from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Import copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "import copy\n",
        "# Import tqdm library, used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# Import os module, used for file path operations, etc.\n",
        "import os\n",
        "# Import functions for handling variable-length sequences from PyTorch's RNN utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "# Google Drive mount path (example)\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\"\n",
        "# Base path where feature files are located (example)\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\")\n",
        "\n",
        "# Path to the dataset split file\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "# Path to the OpenFace feature file\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "# Path to the COVAREP feature file\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "# Path to the language feature file\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "# Path to the humor label file\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "\n",
        "# Audio word-level feature dimension constant\n",
        "_AUDIO_WORD_DIM_CONST = 81\n",
        "# Video word-level feature dimension constant\n",
        "_VIDEO_WORD_DIM_CONST = 371\n",
        "# Hidden dimension of sentence-level LSTM in Hierarchical LSTM (Modified to align with Script_B's configuration idea)\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "# Hidden dimension of sample-level LSTM in Hierarchical LSTM (also its output dimension, projector layer input dimension) (Modified to align with Script_B)\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "# Helper function to load pickle files\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        # Open file in binary read mode\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            # Load pickle data\n",
        "            return pickle.load(f)\n",
        "    # Handle possible UnicodeDecodeError\n",
        "    except UnicodeDecodeError:\n",
        "        # If UnicodeDecodeError occurs, try opening with latin1 encoding\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    # Handle other possible exceptions\n",
        "    except Exception as e:\n",
        "        print(f'Cannot load data {pickle_file}: {e}')\n",
        "        # Raise exception\n",
        "        raise\n",
        "\n",
        "# Helper function to safely prepare feature data for np.array()\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    # If input data is None, return an empty list\n",
        "    if feature_data is None: return []\n",
        "    # If input data is a numpy array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's an empty numpy array, return an empty list\n",
        "        if feature_data.size == 0: return []\n",
        "        # Return non-empty numpy array\n",
        "        return feature_data\n",
        "    # If input data is a list\n",
        "    if isinstance(feature_data, list):\n",
        "        # If it's an empty list, return an empty list\n",
        "        if not feature_data: return []\n",
        "        # Return non-empty list\n",
        "        return feature_data\n",
        "    # Other unexpected types, return an empty list (can add a warning)\n",
        "    return []\n",
        "\n",
        "# Function to extract features and labels\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    # Initialize lists to store various features and labels\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "    # Iterate through the ID list\n",
        "    for hid in id_list:\n",
        "        # Add punchline text\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        # Add context text list\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP (audio) feature processing\n",
        "        # Prepare COVAREP features for the punchline\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline audio features to a float32 numpy array and add\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp, dtype=np.float32))\n",
        "        # Process context COVAREP features (one feature array per sentence)\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp, dtype=np.float32))\n",
        "        # Add the list of processed context audio features\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace (video) feature processing\n",
        "        # Prepare OpenFace features for the punchline\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline video features to a float32 numpy array and add\n",
        "        of_p_list.append(np.array(prepared_punchline_of, dtype=np.float32))\n",
        "        # Process context OpenFace features\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of, dtype=np.float32))\n",
        "        # Add the list of processed context video features\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        # Add labels\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    # Return all extracted features and labels, specifying the dtype for numpy arrays\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object), np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object), np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object), np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "# Prepare data for the new dataset structure: output a list of samples, each sample is a dictionary containing all sentence features/texts\n",
        "# Among them, the features/text of the punchline will be the last item in the corresponding modality list\n",
        "def concatenate_multimodal_data_for_dataset(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    # Get the number of samples (based on the number of context sentences)\n",
        "    num_samples = len(cs)\n",
        "    # List to store all sample data\n",
        "    all_samples_data = []\n",
        "    # Iterate through each sample\n",
        "    for i in range(num_samples):\n",
        "        # Data dictionary for a single sample, containing 'audio', 'video', 'text' keys\n",
        "        sample_data = {'audio': [], 'video': [], 'text': []}\n",
        "\n",
        "        # Audio data processing\n",
        "        # Extract context audio features, ensuring they are valid numpy arrays (word count > 0, correct dimension)\n",
        "        current_sample_audio = [s for s in list(cvp_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _AUDIO_WORD_DIM_CONST]\n",
        "        # Get punchline audio features\n",
        "        punchline_audio = cvp_p[i]\n",
        "        # Append punchline audio features to the end of the list, if valid\n",
        "        if isinstance(punchline_audio, np.ndarray) and punchline_audio.ndim == 2 and punchline_audio.shape[0] > 0 and punchline_audio.shape[1] == _AUDIO_WORD_DIM_CONST:\n",
        "            current_sample_audio.append(punchline_audio)\n",
        "        # If the current audio list is empty (both context and punchline are invalid or missing), add a placeholder for the punchline (single sample, correct dimension)\n",
        "        elif not current_sample_audio:\n",
        "            current_sample_audio.append(np.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        # Store the processed audio feature list into the sample data dictionary\n",
        "        sample_data['audio'] = current_sample_audio\n",
        "\n",
        "        # Video data processing (logic same as audio)\n",
        "        current_sample_video = [s for s in list(of_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _VIDEO_WORD_DIM_CONST]\n",
        "        punchline_video = of_p[i]\n",
        "        if isinstance(punchline_video, np.ndarray) and punchline_video.ndim == 2 and punchline_video.shape[0] > 0 and punchline_video.shape[1] == _VIDEO_WORD_DIM_CONST:\n",
        "            current_sample_video.append(punchline_video)\n",
        "        elif not current_sample_video:\n",
        "            current_sample_video.append(np.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        sample_data['video'] = current_sample_video\n",
        "\n",
        "        # Text data processing\n",
        "        # Extract context sentence text list\n",
        "        current_sample_text = [s for s in list(cs[i]) if isinstance(s, str)]\n",
        "        # Get punchline text\n",
        "        punchline_text_str = ps[i]\n",
        "        # If the punchline text is a string, append it\n",
        "        if isinstance(punchline_text_str, str):\n",
        "            current_sample_text.append(punchline_text_str)\n",
        "        # If the current text list is empty (both context and punchline are invalid or missing), add an empty string as a punchline placeholder\n",
        "        elif not current_sample_text:\n",
        "            current_sample_text.append(\"\")\n",
        "        sample_data['text'] = current_sample_text\n",
        "\n",
        "        # Add the current sample's data dictionary to the total list\n",
        "        all_samples_data.append(sample_data)\n",
        "    # Return the list containing all sample data\n",
        "    return all_samples_data\n",
        "\n",
        "\n",
        "# --- Dataset Class: Modified for Context/Punchline Splitting ---\n",
        "class CustomFeatureDatasetContextPunchline(Dataset):\n",
        "    # Initialization function\n",
        "    def __init__(self, list_of_sample_data_dicts, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len_for_part=512,\n",
        "                 audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST):\n",
        "\n",
        "        # List of sample data dictionaries (each element is a sample, containing 'audio', 'video', 'text' keys)\n",
        "        self.list_of_sample_data_dicts = list_of_sample_data_dicts\n",
        "        # List of labels, converted to torch.long type\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long)\n",
        "        # BERT tokenizer\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        # Maximum BERT length for each part (context/punchline)\n",
        "        self.max_bert_len_for_part = max_bert_len_for_part\n",
        "        # Audio word feature dimension\n",
        "        self.audio_word_dim = audio_word_dim\n",
        "        # Video word feature dimension\n",
        "        self.video_word_dim = video_word_dim\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    # Helper function to tokenize the text part\n",
        "    def _tokenize_text_part(self, text_sentences_list):\n",
        "        # If the text list is empty\n",
        "        if not text_sentences_list:\n",
        "            # If the tokenizer has a pad token, use it, otherwise an empty string might be tokenized into special tokens\n",
        "            processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "        else:\n",
        "            # Join all sentences in the sentence list with spaces\n",
        "            processed_text = \" \".join(text_sentences_list)\n",
        "            # If it's only whitespace or empty after joining\n",
        "            if not processed_text.strip():\n",
        "                processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "\n",
        "        # Call the tokenizer to tokenize\n",
        "        bert_inputs = self.tokenizer(\n",
        "            processed_text, add_special_tokens=True, return_attention_mask=True, # Add special tokens, return attention_mask\n",
        "            max_length=self.max_bert_len_for_part, padding='max_length', truncation=True, # Max length, pad to max length, truncate\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        )\n",
        "        # Return input_ids and attention_mask, and remove the batch dimension (because this is single sample processing)\n",
        "        return bert_inputs[\"input_ids\"].squeeze(0), bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    # Helper function to process audio/video parts\n",
        "    # all_sentences_features_for_sample: List of all sentence features for the entire sample (list of numpy arrays)\n",
        "    # part_sentences_indices: Indices in the total sentence list that the current part (context or punchline) should contain\n",
        "    # word_dim: Word feature dimension for audio or video\n",
        "    def _process_av_part(self, all_sentences_features_for_sample, part_sentences_indices, word_dim):\n",
        "        # List to store feature tensors of all sentences in this part\n",
        "        part_features_list = []\n",
        "        # If the sample itself does not have any sentence features (e.g., the entire sample is empty)\n",
        "        if not all_sentences_features_for_sample:\n",
        "            # Add a placeholder tensor (1 word, specified dimension)\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "            return part_features_list\n",
        "\n",
        "        # Iterate through the sentence indices of the specified part\n",
        "        for sent_idx in part_sentences_indices:\n",
        "            # Ensure the index is within the valid range\n",
        "            if 0 <= sent_idx < len(all_sentences_features_for_sample):\n",
        "                # Get features of a single sentence (numpy array)\n",
        "                sent_feat = all_sentences_features_for_sample[sent_idx]\n",
        "                # Validate feature validity: is a numpy array, 2D, word count > 0, correct dimension\n",
        "                if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0 and sent_feat.shape[1] == word_dim:\n",
        "                    # Convert to PyTorch tensor and add to the list\n",
        "                    part_features_list.append(torch.as_tensor(sent_feat, dtype=torch.float32))\n",
        "\n",
        "        # If this part is empty after processing (e.g., all sentences are invalid or indices are out of range, or the specified index list is empty)\n",
        "        if not part_features_list:\n",
        "            # Add a placeholder tensor for this part\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "        return part_features_list\n",
        "\n",
        "\n",
        "    # Method to get single sample data\n",
        "    def __getitem__(self, index):\n",
        "        # Get the sample data dictionary for the current index\n",
        "        sample_data = self.list_of_sample_data_dicts[index]\n",
        "        # Audio: list of numpy arrays (sentence features)\n",
        "        audio_all_sents_raw = sample_data['audio']\n",
        "        # Video: list of numpy arrays (sentence features)\n",
        "        video_all_sents_raw = sample_data['video']\n",
        "        # Text: list of sentence strings\n",
        "        text_all_sents_str = sample_data['text']\n",
        "        # Get label\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        # Determine the total number of sentences based on the number of text sentences\n",
        "        n_total_sents = len(text_all_sents_str)\n",
        "\n",
        "        # Prepare placeholder input_ids and attention_mask for empty text parts\n",
        "        empty_ids, empty_mask = self._tokenize_text_part([])\n",
        "\n",
        "        # Case 1: If the sample has no sentences at all (n_total_sents == 0)\n",
        "        if n_total_sents == 0:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is empty/placeholder\n",
        "            pl_audio_part = self._process_av_part([], [], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = empty_ids, empty_mask\n",
        "\n",
        "        # Case 2: If there is only one sentence, treat it as only punchline, context is empty\n",
        "        elif n_total_sents == 1:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty index list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is this one sentence (index 0)\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, [0], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, [0], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[0]])\n",
        "\n",
        "        # Case 3: If there are multiple sentences, split into context and punchline\n",
        "        else:\n",
        "            # Context sentence indices: from 0 to the second to last\n",
        "            ctx_indices = list(range(n_total_sents - 1))\n",
        "            # Punchline sentence index: only the last one\n",
        "            pl_indices = [n_total_sents - 1]\n",
        "\n",
        "            # Process context part\n",
        "            ctx_audio_part = self._process_av_part(audio_all_sents_raw, ctx_indices, self.audio_word_dim)\n",
        "            ctx_video_part = self._process_av_part(video_all_sents_raw, ctx_indices, self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in ctx_indices])\n",
        "\n",
        "            # Process punchline part\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, pl_indices, self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, pl_indices, self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in pl_indices])\n",
        "\n",
        "        # Return a tuple of context data, punchline data, and label\n",
        "        return (ctx_audio_part, ctx_video_part, ctx_input_ids, ctx_attention_mask,\n",
        "                pl_audio_part, pl_video_part, pl_input_ids, pl_attention_mask,\n",
        "                label)\n",
        "\n",
        "# --- Custom Collate Function for Context/Punchline Data ---\n",
        "def custom_collate_fn_context_punchline(batch):\n",
        "    # batch is a list where each element is the tuple returned by __getitem__\n",
        "    # Unpack batch data into respective lists\n",
        "    (ctx_audio_list, ctx_video_list, ctx_ids_list, ctx_mask_list,\n",
        "     pl_audio_list, pl_video_list, pl_ids_list, pl_mask_list,\n",
        "     labels_list) = zip(*batch)\n",
        "\n",
        "    # Directly stack text IDs, masks, and labels (they are already fixed-size tensors)\n",
        "    batched_ctx_ids = torch.stack(ctx_ids_list)\n",
        "    batched_ctx_masks = torch.stack(ctx_mask_list)\n",
        "    batched_pl_ids = torch.stack(pl_ids_list)\n",
        "    batched_pl_masks = torch.stack(pl_mask_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Helper function to process a list of audio/video data for a part (e.g., context audio)\n",
        "    # part_data_list: A list of samples, where each sample is a list of sentence tensors\n",
        "    # word_dim_const: Word feature dimension of this modality\n",
        "    def _collate_av_part(part_data_list, word_dim_const):\n",
        "        # Get the number of sentences in each sample\n",
        "        sample_lengths = [len(sample) for sample in part_data_list]\n",
        "        # Maximum number of sentences in the batch, 0 if empty\n",
        "        max_sents = max(sample_lengths) if sample_lengths else 0\n",
        "\n",
        "        # Get the word count of each sentence and find the maximum word count\n",
        "        sentence_word_counts_flat = []\n",
        "        for sample in part_data_list: # Iterate through each sample\n",
        "            for sentence_tensor in sample: # Iterate through each sentence tensor in the sample\n",
        "                sentence_word_counts_flat.append(sentence_tensor.shape[0]) # Add the word count of this sentence\n",
        "        # Maximum number of words in the batch, 0 if empty\n",
        "        max_words = max(sentence_word_counts_flat) if sentence_word_counts_flat else 0\n",
        "\n",
        "        # Ensure max_words and max_sents are at least 1 to avoid zero dimensions in tensors\n",
        "        max_words = max(1, max_words)\n",
        "        max_sents = max(1, max_sents)\n",
        "\n",
        "        # Create padded feature tensor and length tensor\n",
        "        # padded_features: (batch_size, max_sentences, max_words, feature_dimension)\n",
        "        # sentence_lengths_tensor: (batch_size, max_sentences) - records the actual word count of each sentence\n",
        "        padded_features = torch.zeros(len(part_data_list), max_sents, max_words, word_dim_const)\n",
        "        sentence_lengths_tensor = torch.zeros(len(part_data_list), max_sents, dtype=torch.long)\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, sample in enumerate(part_data_list):\n",
        "            # Iterate through each sentence tensor in the sample\n",
        "            for j, sentence_tensor in enumerate(sample):\n",
        "                # Word count of the current sentence\n",
        "                num_words = sentence_tensor.shape[0]\n",
        "                # Pad only if there are words\n",
        "                if num_words > 0:\n",
        "                    # Pad features into the padded_features tensor\n",
        "                    padded_features[i, j, :num_words, :] = sentence_tensor\n",
        "                    # Record the actual word count into the sentence_lengths_tensor tensor\n",
        "                    sentence_lengths_tensor[i, j] = num_words\n",
        "        # Return padded features, list of sentence counts per sample (as tensor), and word counts per sentence tensor\n",
        "        return padded_features, torch.tensor(sample_lengths, dtype=torch.long), sentence_lengths_tensor\n",
        "\n",
        "    # Process audio and video data for context and punchline separately\n",
        "    # ctx_padded_audio: (B, S_ctx_max, W_ctx_max, D_audio)\n",
        "    # ctx_audio_sl: (B,) - Actual number of sentences per sample for context\n",
        "    # ctx_audio_ssl: (B, S_ctx_max) - Actual word count of each sentence per sample for context\n",
        "    ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl = _collate_av_part(ctx_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    ctx_padded_video, ctx_video_sl, ctx_video_ssl = _collate_av_part(ctx_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "    pl_padded_audio, pl_audio_sl, pl_audio_ssl = _collate_av_part(pl_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    pl_padded_video, pl_video_sl, pl_video_ssl = _collate_av_part(pl_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "\n",
        "    # Return all processed batch data\n",
        "    return (ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl, # Context audio (features, sample sentence count, words per sentence)\n",
        "            ctx_padded_video, ctx_video_sl, ctx_video_ssl, # Context video\n",
        "            batched_ctx_ids, batched_ctx_masks,             # Context text\n",
        "            pl_padded_audio, pl_audio_sl, pl_audio_ssl,     # Punchline audio\n",
        "            pl_padded_video, pl_video_sl, pl_video_ssl,     # Punchline video\n",
        "            batched_pl_ids, batched_pl_masks,               # Punchline text\n",
        "            batched_labels)                                 # Labels\n",
        "\n",
        "\n",
        "# --- Hierarchical LSTM Aggregator ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Hidden dimension of sentence-level LSTM\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        # Hidden dimension of sample-level LSTM\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        # Sentence-level LSTM: input word embeddings, output sentence representation\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed, output dimension will become 2*hidden_dim\n",
        "\n",
        "        # If sentence LSTM is bidirectional, the input dimension of sample LSTM needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        # Sample-level LSTM: input sentence representations, output sample representation\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sentences, max_words, word_dimension)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sentences) - actual word count per sentence\n",
        "\n",
        "        # Get the shape of the feature tensor\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "        # Final output dimension of sample LSTM (considering bidirectional case)\n",
        "        final_output_dim_sample = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "\n",
        "        # Handle the extreme case where all inputs in the batch are empty\n",
        "        # If max_sentences or max_words is 0, or batch_size is 0, or all sample_lengths are 0\n",
        "        if max_sents == 0 or max_words == 0 or batch_size == 0 or torch.all(sample_lengths == 0):\n",
        "            # Return a zero tensor with shape (batch_size, final_output_dim_sample)\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sentence dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        # If all sentences are empty (all lengths are 0)\n",
        "        if not torch.any(valid_sents_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sentence features and corresponding lengths\n",
        "        sents_features_packed_data = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed_data = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack padded sequence (length tensor needs to be moved to CPU for packing)\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed_data, sents_word_lengths_packed_data.cpu(),\n",
        "                                                batch_first=True, enforce_sorted=False)\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers*num_directions, B*S_valid, sentence_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get the actual output dimension of sentence LSTM (considering bidirectional)\n",
        "        sent_hidden_dim_actual = self.sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "        # Get the hidden state of the last time step (for unidirectional LSTM, take the last layer; for bidirectional, concatenate the last time steps of the last two layers)\n",
        "        # Output shape: (B*S_valid, sentence_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "        # Apply dropout to sentence embeddings\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Put valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Create a zero tensor with shape (B*S, actual_sentence_hidden_dim)\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        # Fill valid sentence embeddings into corresponding positions\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent), reshape to sample LSTM input format\n",
        "        sample_features_for_sample_lstm = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack padded sequence (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0 (i.e., samples with actual sentence count of 0)\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        # If all samples are empty (actual sentence counts are all 0)\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sample features and corresponding lengths\n",
        "        sample_features_packed_input_data = sample_features_for_sample_lstm[valid_sample_indices]\n",
        "        sample_lengths_packed_data = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input_data, sample_lengths_packed_data.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers*num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get the hidden state of the last time step\n",
        "        # Output shape: (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "        # Apply dropout to sample embeddings\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Put valid sample embeddings back to their original positions, use zero vectors for empty samples\n",
        "        # Create a zero tensor with shape (B, final_output_dim_sample_lstm)\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "        # Fill valid sample embeddings into corresponding positions\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "        # Return final sample embeddings\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "# --- GLU Linear Layer ---\n",
        "class GLULinear(nn.Module):\n",
        "    # Initialization function, input dimension and output dimension\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GLULinear, self).__init__()\n",
        "        # The first linear layer is followed by a GELU activation function\n",
        "        self.layer1 = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU())\n",
        "        # The second linear layer\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    # Forward propagation function\n",
        "    def forward(self, x):\n",
        "        # Element-wise multiplication of the outputs of the two linear layers\n",
        "        return self.layer1(x) * self.layer2(x)\n",
        "\n",
        "# --- Advanced Cross-Attention/Self-Attention Module ---\n",
        "class MultiHeadAttentionModule(nn.Module):\n",
        "    # Initialization function\n",
        "    # dim: feature dimension, num_heads: number of attention heads\n",
        "    def __init__(self, dim, num_heads=1):\n",
        "        super(MultiHeadAttentionModule, self).__init__()\n",
        "        # Feature dimension\n",
        "        self.dim = dim\n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.head_dim = dim // num_heads\n",
        "        # Ensure dimension is divisible by the number of heads\n",
        "        if self.head_dim * num_heads != self.dim:\n",
        "            raise ValueError(\"dim must be divisible by num_heads\")\n",
        "\n",
        "        # Linear layer to generate Key\n",
        "        self.K_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Value\n",
        "        self.V_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Query\n",
        "        self.Q_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Softmax layer, used to calculate attention weights\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        # Fully connected layer before output\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "    # Forward propagation function\n",
        "    # feat1_query is Query, feat2_key_value is Key and Value\n",
        "    # mask: optional attention mask\n",
        "    def forward(self, feat1_query, feat2_key_value, mask=None):\n",
        "        # Query shape: (batch_size, Query_sequence_length, Query_dimension)\n",
        "        B_q, N_q, C_q = feat1_query.shape\n",
        "        # Key/Value shape: (batch_size, Key/Value_sequence_length, Key/Value_dimension)\n",
        "        B_kv, N_kv, C_kv = feat2_key_value.shape\n",
        "\n",
        "        # Check if batch sizes of Query and Key/Value match\n",
        "        if B_q != B_kv: raise ValueError(f\"Batch sizes do not match: Query is {B_q}, Key/Value is {B_kv}\")\n",
        "\n",
        "        # Generate Q, K, V and adjust shape for multi-head: (batch, num_heads, sequence_length, head_dimension)\n",
        "        # Q: (B, N_q, C_q) -> (B, N_q, num_heads, head_dim) -> (B, num_heads, N_q, head_dim)\n",
        "        Q = self.Q_layer(feat1_query).reshape(B_q, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        K = self.K_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        V = self.V_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate dot product of Q and K_transpose and scale ( scaled_dot_product = (Q @ K.T) / sqrt(head_dim) )\n",
        "        # dots shape: (B, num_heads, N_q, N_kv)\n",
        "        dots = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # If a mask is provided (usually a padding mask for K,V sequence, shape B, N_kv)\n",
        "        if mask is not None:\n",
        "            # unsqueeze expands the mask to (B, 1, 1, N_kv) to match the shape of dots (B, nH, N_q, N_kv) for broadcasting\n",
        "            # Fill positions in dots where mask is 0 (i.e., padding positions) with a very small value, so their weight approaches 0 after softmax\n",
        "            dots = dots.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "\n",
        "        # Calculate attention weights (attn_weights shape: B, num_heads, N_q, N_kv)\n",
        "        attn_weights = self.attend(dots)\n",
        "        # Attention weights weighted V (out shape: B, num_heads, N_q, head_dim)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # Permute and merge multi-head results: (B, num_heads, N_q, head_dim) -> (B, N_q, num_heads, head_dim) -> (B, N_q, dim)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B_q, N_q, self.dim)\n",
        "        # Pass through output fully connected layer\n",
        "        out = self.fc_out(out)\n",
        "        # Return final output\n",
        "        return out\n",
        "\n",
        "# --- Adapted Single Stream Processor ---\n",
        "class AdaptedSingleStreamProcessor(nn.Module):\n",
        "    # Initialization function\n",
        "    # audio_video_input_dim: Input dimension after audio/video projection\n",
        "    # bert_hidden_size: BERT's hidden layer size\n",
        "    # max_bert_len_for_lstm: Maximum input sequence length expected by the internal text LSTM\n",
        "    # lstm_hidden_size: Hidden size of the internal text LSTM\n",
        "    # attention_token_dim: Dimension of attention tokens\n",
        "    # num_attention_tokens_per_modal: Number of tokens output after processing each modality\n",
        "    # active_modalities: Tuple of active modalities, e.g., ('audio', 'video', 'text')\n",
        "    # num_ca_sa_heads: Number of heads for cross-attention and self-attention modules\n",
        "    # dropout_rate: Dropout rate for the text FC part\n",
        "    def __init__(self, audio_video_input_dim, bert_hidden_size, max_bert_len_for_lstm,\n",
        "                 lstm_hidden_size, attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 active_modalities=('audio', 'video', 'text'), num_ca_sa_heads=1, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        # Number of tokens output after processing each modality\n",
        "        self.n_tokens_per_modal = num_attention_tokens_per_modal\n",
        "        # Dimension of attention tokens\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        # Maximum input sequence length expected by the internal text LSTM\n",
        "        self.max_bert_len_for_lstm_input = max_bert_len_for_lstm\n",
        "        # Active modalities\n",
        "        self.active_modalities = active_modalities\n",
        "        # Flattened feature dimension output by each modality processor (N * C)\n",
        "        self.expected_feature_dim_after_mod_proc = self.n_tokens_per_modal * self.attention_token_dim\n",
        "\n",
        "        # Audio feature processor: receives projected features, maps to NxC token representation\n",
        "        self.audio_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc) # Layer normalization\n",
        "        )\n",
        "        # Video feature processor: logic same as audio\n",
        "        self.vision_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "        # Text processing: BERT hidden state -> LSTM -> Fully connected layer -> NxC token representation\n",
        "        # Text LSTM processor\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        # Text FC processor, maps LSTM output to token representation\n",
        "        self.text_fc_processor_to_tokens = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), # Dropout layer\n",
        "            # LSTM output is (B, S_lstm, H_lstm), after reshape it's (B, S_lstm * H_lstm)\n",
        "            GLULinear(lstm_hidden_size * self.max_bert_len_for_lstm_input, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "\n",
        "        # Attention module instantiation\n",
        "        # ZA: Audio cross-attention (query is concatenation of all modalities, key/value are audio tokens)\n",
        "        self.ZA = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZV: Video cross-attention\n",
        "        self.ZV = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZT: Text cross-attention\n",
        "        self.ZT = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # SA_stream: Intra-stream self-attention\n",
        "        self.SA_stream = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # Final output dimension of this stream processor (after averaging SA output, or, dimension of a single token)\n",
        "        self.output_final_dim = attention_token_dim\n",
        "\n",
        "    # Forward propagation function\n",
        "    # audio_input_proj, vision_input_proj from Hierarchical LSTM + Projector layer (B, D_projector)\n",
        "    # text_sequence_input_bert is BERT's hidden state (B, S_bert, D_bert)\n",
        "    def forward(self, audio_input_proj, vision_input_proj, text_sequence_input_bert):\n",
        "        # Dynamically determine batch size\n",
        "        b = 0\n",
        "        if audio_input_proj is not None and audio_input_proj.nelement() > 0: b = audio_input_proj.shape[0]\n",
        "        elif vision_input_proj is not None and vision_input_proj.nelement() > 0: b = vision_input_proj.shape[0]\n",
        "        elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: b = text_sequence_input_bert.shape[0]\n",
        "\n",
        "        # Handle empty batch (all inputs are empty or None)\n",
        "        if b == 0:\n",
        "            dev = torch.device(\"cpu\") # Default device\n",
        "            # Try to get device from valid input\n",
        "            if audio_input_proj is not None and audio_input_proj.nelement() > 0: dev = audio_input_proj.device\n",
        "            elif vision_input_proj is not None and vision_input_proj.nelement() > 0: dev = vision_input_proj.device\n",
        "            elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: dev = text_sequence_input_bert.device\n",
        "\n",
        "            # Create empty flat features and stream output\n",
        "            empty_flat = torch.zeros(0, self.expected_feature_dim_after_mod_proc, device=dev)\n",
        "            empty_stream_out = torch.zeros(0, 1, self.output_final_dim, device=dev)\n",
        "            # Return empty flat features for contrastive loss and empty stream output\n",
        "            return empty_flat, empty_flat, empty_flat, empty_stream_out\n",
        "\n",
        "        # Get current device (ensure at least one valid input to determine device)\n",
        "        device = audio_input_proj.device if audio_input_proj is not None and audio_input_proj.nelement() > 0 else \\\n",
        "                 (vision_input_proj.device if vision_input_proj is not None and vision_input_proj.nelement() > 0 else \\\n",
        "                  text_sequence_input_bert.device)\n",
        "\n",
        "        # Initialize flat features for contrastive loss (audio_f_flat) and token features for attention (audio_f_tokens)\n",
        "        # Audio processing\n",
        "        audio_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        audio_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        # If audio modality is active, input is not empty, and input is not all zeros (indicates actual content)\n",
        "        if 'audio' in self.active_modalities and audio_input_proj is not None and audio_input_proj.nelement() > 0 and audio_input_proj.abs().sum() > 1e-9 :\n",
        "            audio_f_flat = self.audio_feat_processor_to_tokens(audio_input_proj) # (B, N*C)\n",
        "            audio_f_tokens = audio_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Video processing (logic same as audio)\n",
        "        vis_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        vis_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'video' in self.active_modalities and vision_input_proj is not None and vision_input_proj.nelement() > 0 and vision_input_proj.abs().sum() > 1e-9:\n",
        "            vis_f_flat = self.vision_feat_processor_to_tokens(vision_input_proj)\n",
        "            vis_f_tokens = vis_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim)\n",
        "\n",
        "        # Text processing\n",
        "        text_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        text_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0 and text_sequence_input_bert.abs().sum() > 1e-9:\n",
        "            # Get current BERT output sequence length\n",
        "            current_bert_seq_len = text_sequence_input_bert.shape[1]\n",
        "            text_sequence_input_bert_adjusted = text_sequence_input_bert\n",
        "            # Adjust BERT output sequence length to match LSTM expected input\n",
        "            if current_bert_seq_len != self.max_bert_len_for_lstm_input:\n",
        "                if current_bert_seq_len > self.max_bert_len_for_lstm_input: # Truncate if too long\n",
        "                    text_sequence_input_bert_adjusted = text_sequence_input_bert[:, :self.max_bert_len_for_lstm_input, :]\n",
        "                else: # Pad with zeros if too short\n",
        "                    padding_needed = self.max_bert_len_for_lstm_input - current_bert_seq_len\n",
        "                    # Create padding tensor (B, padding_needed, D_bert)\n",
        "                    padding_tensor = torch.zeros(b, padding_needed, text_sequence_input_bert.shape[2], device=device)\n",
        "                    # Concatenate original BERT output and padding tensor\n",
        "                    text_sequence_input_bert_adjusted = torch.cat([text_sequence_input_bert, padding_tensor], dim=1)\n",
        "\n",
        "            # Pass through text LSTM\n",
        "            lstm_output, _ = self.text_lstm_processor(text_sequence_input_bert_adjusted) # (B, S_lstm, H_lstm)\n",
        "            # Flatten LSTM output: (B, S_lstm * H_lstm)\n",
        "            text_f_flat_from_lstm = lstm_output.reshape(b, -1)\n",
        "            # Process flattened LSTM output through FC layer\n",
        "            text_f_flat = self.text_fc_processor_to_tokens(text_f_flat_from_lstm) # (B, N*C)\n",
        "            # Reshape to token form\n",
        "            text_f_tokens = text_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Collect tokens from active modalities with content\n",
        "        active_mod_token_lists = []\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(audio_f_tokens)\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(vis_f_tokens)\n",
        "        if 'text'  in self.active_modalities and text_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(text_f_tokens)\n",
        "\n",
        "        # If there are no active modalities with content\n",
        "        if not active_mod_token_lists:\n",
        "            # Return flat features and zero stream output (because there's no content for attention calculation)\n",
        "            return audio_f_flat, vis_f_flat, text_f_flat, torch.zeros(b, 1, self.output_final_dim, device=device)\n",
        "\n",
        "        # Concatenate tokens of active modalities as Query for cross-attention\n",
        "        # query_for_modality_ca shape: (B, num_active_modalities * N, C_token)\n",
        "        query_for_modality_ca = torch.cat(active_mod_token_lists, dim=1)\n",
        "\n",
        "        # Perform inter-modality cross-attention\n",
        "        # Initialize result tensor\n",
        "        res_za, res_zv, res_zt = torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca)\n",
        "        # If audio is active and has content\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9:\n",
        "            # query_for_modality_ca as Query, audio_f_tokens as Key and Value\n",
        "            res_za = self.ZA(query_for_modality_ca, audio_f_tokens)\n",
        "        # If video is active and has content\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zv = self.ZV(query_for_modality_ca, vis_f_tokens)\n",
        "        # If text is active and has content\n",
        "        if 'text' in self.active_modalities and text_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zt = self.ZT(query_for_modality_ca, text_f_tokens)\n",
        "\n",
        "        # Merge cross-attention results (element-wise addition)\n",
        "        feat_after_mod_ca = res_za + res_zv + res_zt\n",
        "        # Intra-stream self-attention, with residual connection\n",
        "        # feat_after_mod_ca as Query, Key, and Value\n",
        "        feat_after_sa_stream = self.SA_stream(feat_after_mod_ca, feat_after_mod_ca) + feat_after_mod_ca\n",
        "        # Average the features after self-attention along the sequence dimension to get the final stream representation\n",
        "        stream_output_representation = torch.mean(feat_after_sa_stream, dim=1) # (B, C_token)\n",
        "\n",
        "        # Return flat features for contrastive loss, and the final stream output representation (add a dimension to match the expected (B, 1, C_token) shape)\n",
        "        return audio_f_flat, vis_f_flat, text_f_flat, stream_output_representation.unsqueeze(1)\n",
        "\n",
        "\n",
        "# --- Main Model: ContextPunchlineHumorModelNew ---\n",
        "class ContextPunchlineHumorModelNew(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self,\n",
        "                 bert_model_name_or_path,\n",
        "                 audio_word_dim, video_word_dim,\n",
        "                 sentence_lstm_hidden_dim, sample_lstm_hidden_dim, hier_lstm_dropout,\n",
        "                 projector_output_dim,\n",
        "                 bert_hidden_size_actual, max_bert_len_for_lstm,\n",
        "                 text_lstm_hidden_size_in_stream,\n",
        "                 attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 stream_ca_sa_heads, stream_dropout_rate,\n",
        "                 final_cross_attention_heads, # MODIFIED: This will now be used for the new final fusion heads\n",
        "                 mlp_hidden_dim, num_classes,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(bert_model_name_or_path)\n",
        "\n",
        "        self.ctx_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.ctx_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.context_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        self.pl_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.pl_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.punchline_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Final Fusion ---\n",
        "        self.final_fusion_input_dim = attention_token_dim\n",
        "\n",
        "        self.final_ca_query_streams_on_ctx_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_ca_query_streams_on_pl_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_fusion_sa_after_ca_sum = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        # REMOVED: self.cross_attention_final\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.final_fusion_input_dim, mlp_hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(mlp_hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self,\n",
        "                ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl,\n",
        "                ctx_padded_video, ctx_video_sl, ctx_video_ssl,\n",
        "                ctx_input_ids, ctx_attention_mask,\n",
        "                pl_padded_audio, pl_audio_sl, pl_audio_ssl,\n",
        "                pl_padded_video, pl_video_sl, pl_video_ssl,\n",
        "                pl_input_ids, pl_attention_mask,\n",
        "                current_modality_config=None, tokenizer_for_padding=None\n",
        "                ):\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_a = self.ctx_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_a_vec = torch.zeros(ctx_padded_audio.shape[0], actual_hier_lstm_output_dim_ctx_a, device=ctx_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(ctx_audio_sl > 0):\n",
        "                ctx_a_vec = self.ctx_audio_hier_lstm(ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_v = self.ctx_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_v_vec = torch.zeros(ctx_padded_video.shape[0], actual_hier_lstm_output_dim_ctx_v, device=ctx_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(ctx_video_sl > 0):\n",
        "                ctx_v_vec = self.ctx_video_hier_lstm(ctx_padded_video, ctx_video_sl, ctx_video_ssl)\n",
        "\n",
        "        ctx_a_proj = self.ctx_audio_projector(ctx_a_vec)\n",
        "        ctx_v_proj = self.ctx_video_projector(ctx_v_vec)\n",
        "\n",
        "        ctx_bert_hs = torch.zeros(ctx_input_ids.shape[0], ctx_input_ids.shape[1], self.bert_model.config.hidden_size, device=ctx_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(ctx_attention_mask.sum(dim=1) > 0):\n",
        "                ctx_bert_outputs = self.bert_model(input_ids=ctx_input_ids, attention_mask=ctx_attention_mask)\n",
        "                ctx_bert_hs = ctx_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        ctx_audio_f_flat, ctx_vis_f_flat, ctx_text_f_flat, ctx_stream_repr = self.context_processor(ctx_a_proj, ctx_v_proj, ctx_bert_hs)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_a = self.pl_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_a_vec = torch.zeros(pl_padded_audio.shape[0], actual_hier_lstm_output_dim_pl_a, device=pl_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(pl_audio_sl > 0):\n",
        "                pl_a_vec = self.pl_audio_hier_lstm(pl_padded_audio, pl_audio_sl, pl_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_v = self.pl_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_v_vec = torch.zeros(pl_padded_video.shape[0], actual_hier_lstm_output_dim_pl_v, device=pl_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(pl_video_sl > 0):\n",
        "                pl_v_vec = self.pl_video_hier_lstm(pl_padded_video, pl_video_sl, pl_video_ssl)\n",
        "\n",
        "        pl_a_proj = self.pl_audio_projector(pl_a_vec)\n",
        "        pl_v_proj = self.pl_video_projector(pl_v_vec)\n",
        "\n",
        "        pl_bert_hs = torch.zeros(pl_input_ids.shape[0], pl_input_ids.shape[1], self.bert_model.config.hidden_size, device=pl_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(pl_attention_mask.sum(dim=1) > 0):\n",
        "                pl_bert_outputs = self.bert_model(input_ids=pl_input_ids, attention_mask=pl_attention_mask)\n",
        "                pl_bert_hs = pl_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        pl_audio_f_flat, pl_vis_f_flat, pl_text_f_flat, pl_stream_repr = self.punchline_processor(pl_a_proj, pl_v_proj, pl_bert_hs)\n",
        "\n",
        "        # --- MODIFIED: New final fusion logic ---\n",
        "        streams_query = torch.cat((ctx_stream_repr, pl_stream_repr), dim=1)\n",
        "        res_ca_ctx = self.final_ca_query_streams_on_ctx_kv(streams_query, ctx_stream_repr)\n",
        "        res_ca_pl = self.final_ca_query_streams_on_pl_kv(streams_query, pl_stream_repr)\n",
        "        fused_after_ca = res_ca_ctx + res_ca_pl\n",
        "        fused_after_sa = self.final_fusion_sa_after_ca_sum(fused_after_ca, fused_after_ca)\n",
        "        fused_after_sa = fused_after_sa + fused_after_ca # Residual connection for the self-attention on fused representations\n",
        "        fused_representation = torch.mean(fused_after_sa, dim=1)\n",
        "\n",
        "        mlp_out = self.mlp(fused_representation)\n",
        "        logits = self.classifier(mlp_out)\n",
        "\n",
        "        contrastive_features = {\n",
        "            'ctx_audio': ctx_audio_f_flat, 'ctx_video': ctx_vis_f_flat, 'ctx_text': ctx_text_f_flat,\n",
        "            'pl_audio': pl_audio_f_flat, 'pl_video': pl_vis_f_flat, 'pl_text': pl_text_f_flat\n",
        "        }\n",
        "        return logits, contrastive_features\n",
        "\n",
        "\n",
        "# --- Contrastive Loss Function ---\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    # Initialization function, temperature coefficient\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        # Use CrossEntropyLoss to calculate loss (SimCLR style)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward propagation function\n",
        "    # emb_i, emb_j are embeddings from different modalities or views (B, D)\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        # Get batch size\n",
        "        batch_size = emb_i.shape[0]\n",
        "        # Contrastive loss requires at least 2 samples to compute, otherwise return 0 loss\n",
        "        if batch_size <= 1:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # Check if embeddings are all zeros, if so, loss is 0 (to avoid NaN)\n",
        "        # If the sum of absolute values of all elements in either embedding tensor is less than a very small value, it is considered empty or all zeros\n",
        "        if emb_i.abs().sum() < 1e-9 or emb_j.abs().sum() < 1e-9:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # L2 normalize embedding vectors\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        # Concatenate the two groups of normalized embeddings along the batch dimension: (2*B, D)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Calculate similarity matrix (cosine similarity between all sample pairs, then divide by temperature)\n",
        "        # (2*B, D) @ (D, 2*B) -> (2*B, 2*B)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "\n",
        "        # Create labels: for each sample in z_i, its positive sample is the corresponding sample in z_j\n",
        "        # For example, row similarity_matrix[0] is the similarity of z_i[0] with all representations\n",
        "        # Its positive sample z_j[0] has index batch_size + 0 in representations\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        # For each sample in z_j, its positive sample is the corresponding sample in z_i\n",
        "        # For example, row similarity_matrix[batch_size+0] is the similarity of z_j[0] with all representations\n",
        "        # Its positive sample z_i[0] has index 0 in representations\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Calculate loss, separately for z_i querying z_j and z_j querying z_i\n",
        "        # loss_i: z_i as anchor, corresponding sample in z_j as positive\n",
        "        # similarity_matrix[:batch_size] is the similarity of z_i with all representations (B, 2*B)\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        # loss_j: z_j as anchor, corresponding sample in z_i as positive\n",
        "        # similarity_matrix[batch_size:] is the similarity of z_j with all representations (B, 2*B)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        # Return average loss\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "\n",
        "# --- Modified Training Function (Only contrastive loss calculation method is changed) ---\n",
        "def train_new_model(model, data_loader, optimizer, scheduler,\n",
        "                    bce_criterion, contrastive_loss_fn, device, epoch, num_epochs,\n",
        "                    contrastive_loss_weight, current_modality_config, tokenizer_for_padding):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # The BERT part of the model is already globally frozen externally, no explicit model.bert_model.eval() needed here\n",
        "\n",
        "    # Initialize total BCE loss, total contrastive loss, total loss\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0 # Used to accumulate final_simclr_loss_for_batch for each batch\n",
        "    total_loss = 0\n",
        "    # Create tqdm progress bar to display training progress\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    # Iterate through each batch in the data loader\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack data from collate_fn\n",
        "        (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "         pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "         labels) = batch\n",
        "\n",
        "        # Get current batch size\n",
        "        current_batch_size = ctx_a_feat.shape[0]\n",
        "        # If batch is empty, skip\n",
        "        if current_batch_size == 0: continue\n",
        "\n",
        "        # Move data to the specified device (excluding the last label)\n",
        "        batch_data_on_device = []\n",
        "        for tensor_item in batch[:-1]:\n",
        "            batch_data_on_device.append(tensor_item.to(device))\n",
        "        # Move labels to device and convert to long type\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        # Clear optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model forward pass, get classification logits and contrastive features\n",
        "        logits, contrastive_feats = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "        # Calculate BCE classification loss\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # --- Contrastive Loss Calculation (modified to averaging method) ---\n",
        "        final_simclr_loss_for_batch = torch.tensor(0.0, device=device) # Initialize contrastive loss for this batch\n",
        "        if current_batch_size > 1 and contrastive_loss_weight > 0:\n",
        "            accumulated_contrastive_loss_components = []\n",
        "\n",
        "            # Contrastive loss for the context stream\n",
        "            ctx_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_video'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_video'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_video'], contrastive_feats['ctx_text']))\n",
        "\n",
        "            if ctx_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(ctx_individual_losses)))\n",
        "\n",
        "            # Contrastive loss for the punchline stream\n",
        "            pl_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_video'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_video'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_video'], contrastive_feats['pl_text']))\n",
        "\n",
        "            if pl_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(pl_individual_losses)))\n",
        "\n",
        "            # Calculate the final contrastive loss (if multiple components exist, take their average)\n",
        "            if accumulated_contrastive_loss_components:\n",
        "                final_simclr_loss_for_batch = torch.mean(torch.stack(accumulated_contrastive_loss_components))\n",
        "            # else: final_simclr_loss_for_batch remains its initial value of 0.0\n",
        "\n",
        "        # Total loss = BCE loss + contrastive_loss_weight * calculated batch contrastive loss\n",
        "        current_loss = bce_loss + contrastive_loss_weight * final_simclr_loss_for_batch\n",
        "\n",
        "        # Backpropagate to calculate gradients\n",
        "        current_loss.backward()\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        # If a learning rate scheduler is used\n",
        "        if scheduler is not None:\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss values (item() gets scalar value)\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += final_simclr_loss_for_batch.item() # Accumulate the calculated batch contrastive loss\n",
        "        total_loss += current_loss.item()\n",
        "        # Update progress bar display information\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{final_simclr_loss_for_batch.item():.4f}\")\n",
        "\n",
        "    # If data loader is not empty\n",
        "    if len(data_loader) > 0:\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        # Print average training loss for the current epoch\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "\n",
        "\n",
        "# --- Validation/Test Function (Added F1 Score) ---\n",
        "def validate_or_test_new_model(model, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                               current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize total BCE loss\n",
        "    total_bce_loss = 0\n",
        "    # List to store all prediction results\n",
        "    all_preds = []\n",
        "    # List to store all true labels\n",
        "    all_labels = []\n",
        "\n",
        "    # Set progress bar description (corrected logic)\n",
        "    if mode == \"Test\" and epoch is None:\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # and epoch is not None (implicitly for this branch after the first)\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Val\": # epoch should not be None for validation\n",
        "        desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    else: # Fallback, though ideally all cases are covered\n",
        "        desc = f\"Processing [{mode} {current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    # Do not calculate gradients within this block to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through each batch in the data loader\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            # Unpack data\n",
        "            (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "             pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "             labels) = batch\n",
        "\n",
        "            # Get current batch size\n",
        "            current_batch_size = ctx_a_feat.shape[0]\n",
        "            # If batch is empty, skip\n",
        "            if current_batch_size == 0: continue\n",
        "\n",
        "            # Move data to device\n",
        "            batch_data_on_device = [t.to(device) for t in batch[:-1]]\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            # Model forward pass, ignore contrastive features (not needed during validation/testing)\n",
        "            logits, _ = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "            # Calculate BCE loss\n",
        "            bce_loss = bce_criterion(logits, labels)\n",
        "            # Accumulate BCE loss\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            # Get predicted class (index of the max value in logits)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Store prediction results (convert to numpy array)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            # Store true labels (convert to numpy array)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # If data loader is empty or no labels were collected\n",
        "    if len(data_loader) == 0 or len(all_labels) == 0 :\n",
        "        print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty.\")\n",
        "        if mode == \"Val\": return 0.0, 0.0 # Validation mode returns 0.0 accuracy, 0.0 F1\n",
        "        return 0.0, 0.0, 0.0 # Test mode returns 0.0 loss, 0.0 accuracy, 0.0 F1\n",
        "\n",
        "    # Calculate average BCE loss\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    # Calculate accuracy (if label list is not empty)\n",
        "    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
        "    # Calculate F1 score (if label list is not empty), use 'binary' because it's binary classification, zero_division handles boundary cases\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0) if all_labels else 0.0\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # If validation mode, return accuracy and F1\n",
        "    if mode == \"Val\": return accuracy, f1\n",
        "    # If test mode, return average loss, accuracy, and F1\n",
        "    return avg_bce_loss, accuracy, f1\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameter Configuration ---\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # SENTENCE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    # SAMPLE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    HIER_LSTM_DROPOUT = 0.3\n",
        "\n",
        "    PROJECTOR_OUTPUT_DIM = 1024\n",
        "\n",
        "    MAX_BERT_LEN_FOR_PART_DATASET = 512\n",
        "    TEXT_LSTM_HIDDEN_SIZE_IN_STREAM = 256   # This version of the code still uses this parameter\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    STREAM_CA_SA_HEADS = 1\n",
        "    STREAM_DROPOUT_RATE = 0.3\n",
        "\n",
        "    FINAL_CROSS_ATTENTION_HEADS = 1         # Used for all attention modules in the new final fusion structure\n",
        "    MLP_HIDDEN_DIM = 256\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    BATCH_SIZE = 16 # Warning: The new final fusion structure is more complex, may need to reduce this value\n",
        "    LEARNING_RATE = 8e-5\n",
        "    NUM_EPOCHS = 4 # It is recommended to increase epochs for actual use\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"BERT model used: {BERT_MODEL_NAME_FOR_MAIN}\")\n",
        "    print(f\"Hierarchical LSTM: Sentence-level hidden dim {SENTENCE_LSTM_HIDDEN_DIM_CONFIG}, Sample-level hidden dim {SAMPLE_LSTM_HIDDEN_DIM_CONFIG}, Dropout {HIER_LSTM_DROPOUT}\")\n",
        "    print(f\"Projector output dimension (Stream processor audio/video input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"Max BERT length for context/punchline part: {MAX_BERT_LEN_FOR_PART_DATASET}\")\n",
        "    print(f\"Stream processor internal text LSTM hidden size: {TEXT_LSTM_HIDDEN_SIZE_IN_STREAM}\") # Keep printing as this param is still in model def\n",
        "    print(f\"Attention token dimension: {ATTENTION_TOKEN_DIM}, Tokens per modality: {NUM_ATTENTION_TOKENS_PER_MODAL}\")\n",
        "    print(f\"Stream processor attention heads: {STREAM_CA_SA_HEADS}, Stream processor text FC Dropout rate: {STREAM_DROPOUT_RATE}\")\n",
        "    print(f\"Final fusion stage attention heads: {FINAL_CROSS_ATTENTION_HEADS}, MLP hidden dimension: {MLP_HIDDEN_DIM}\")\n",
        "    print(f\"Training parameters: Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}, Epochs {NUM_EPOCHS}\")\n",
        "    print(f\"Contrastive loss: Temperature {TEMPERATURE_CONTRASTIVE}, Weight {CONTRASTIVE_LOSS_WEIGHT}\")\n",
        "    print(\"\\n !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \\n\")\n",
        "\n",
        "    # --- Load Raw Data ---\n",
        "    print(\"Loading raw data pickle files...\")\n",
        "    # Ensure paths are correct\n",
        "    # Example: data_folds_path = \"path_to_your_gdrive/Project_CCS2-main/sdk_features/data_folds.pkl\"\n",
        "    # Replace with your actual paths\n",
        "    # To run locally, you might need to download these files or adjust paths\n",
        "    # For demonstration, we'll assume files might not exist and add checks or placeholders.\n",
        "    try:\n",
        "        data_folds = load_pickle(data_folds_path)\n",
        "        language_sdk = load_pickle(language_file)\n",
        "        covarep_sdk = load_pickle(covarep_file)\n",
        "        openface_sdk = load_pickle(openface_file)\n",
        "        humor_label_sdk = load_pickle(humor_label_file)\n",
        "        print(\"Raw data loading complete.\")\n",
        "\n",
        "        train_ids = data_folds['train']\n",
        "        dev_ids = data_folds['dev']\n",
        "        test_ids = data_folds['test']\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: One or more data files not found. Please check paths and ensure files exist.\")\n",
        "        print(\"Using placeholder data for demonstration.\")\n",
        "        # Placeholder data for demonstration if files are missing\n",
        "        train_ids, dev_ids, test_ids = ['h1','h2'], ['h3'], ['h4']\n",
        "        language_sdk = {\n",
        "            f'h{i}': {'punchline_sentence': f'Punchline {i}', 'context_sentences': [f'Context sent {i}.1', f'Context sent {i}.2']} for i in range(1, 5)\n",
        "        }\n",
        "        covarep_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _AUDIO_WORD_DIM_CONST).astype(np.float32) if i % 2 == 0 else [], # Some empty\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _AUDIO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        openface_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _VIDEO_WORD_DIM_CONST).astype(np.float32),\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _VIDEO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        humor_label_sdk = {f'h{i}': float(i % 2) for i in range(1,5)}\n",
        "\n",
        "\n",
        "    print(\"Extracting features and labels...\")\n",
        "    (train_ps, train_cs, train_cvp_p, train_cvp_c, train_of_p, train_of_c, train_labels) = \\\n",
        "        extract_features_and_labels(train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (dev_ps, dev_cs, dev_cvp_p, dev_cvp_c, dev_of_p, dev_of_c, dev_labels) = \\\n",
        "        extract_features_and_labels(dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (test_ps, test_cs, test_cvp_p, test_cvp_c, test_of_p, test_of_c, test_labels) = \\\n",
        "        extract_features_and_labels(test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    print(\"Feature and label extraction complete.\")\n",
        "\n",
        "    print(\"Structuring data for new dataset format (context/punchline split)...\")\n",
        "    train_sample_data_dicts = concatenate_multimodal_data_for_dataset(train_cvp_c, train_of_c, train_cs, train_cvp_p, train_of_p, train_ps)\n",
        "    dev_sample_data_dicts = concatenate_multimodal_data_for_dataset(dev_cvp_c, dev_of_c, dev_cs, dev_cvp_p, dev_of_p, dev_ps)\n",
        "    test_sample_data_dicts = concatenate_multimodal_data_for_dataset(test_cvp_c, test_of_c, test_cs, test_cvp_p, test_of_p, test_ps)\n",
        "    print(\"Data structuring complete.\")\n",
        "\n",
        "    print(\"Initializing BERT tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    _bert_temp_model = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = _bert_temp_model.config.hidden_size\n",
        "    del _bert_temp_model\n",
        "    print(f\"Actual BERT hidden size: {BERT_HIDDEN_SIZE_ACTUAL}\")\n",
        "\n",
        "    print(\"Creating CustomFeatureDatasetContextPunchline instances...\")\n",
        "    train_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        train_sample_data_dicts, train_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        dev_sample_data_dicts, dev_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    test_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        test_sample_data_dicts, test_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=custom_collate_fn_context_punchline, drop_last=True if BATCH_SIZE > 1 and len(train_dataset) > BATCH_SIZE else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    print(f\"Dataloaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "        {'name': 'V',   'audio': False, 'video': True, 'text': False},\n",
        "        {'name': 'A',   'audio': True, 'video': False, 'text': False},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    for config_idx, model_config_iter in enumerate(modality_configurations):\n",
        "        config_name = model_config_iter['name']\n",
        "        print(f\"\\n--- Starting processing for model config: {config_name} ---\")\n",
        "\n",
        "        model = ContextPunchlineHumorModelNew(\n",
        "            bert_model_name_or_path=BERT_MODEL_NAME_FOR_MAIN,\n",
        "            audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=SAMPLE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            hier_lstm_dropout=HIER_LSTM_DROPOUT,\n",
        "            projector_output_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size_actual=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len_for_lstm=MAX_BERT_LEN_FOR_PART_DATASET, # This is max_bert_len_for_part\n",
        "            text_lstm_hidden_size_in_stream=TEXT_LSTM_HIDDEN_SIZE_IN_STREAM,\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            stream_ca_sa_heads=STREAM_CA_SA_HEADS,\n",
        "            stream_dropout_rate=STREAM_DROPOUT_RATE,\n",
        "            final_cross_attention_heads=FINAL_CROSS_ATTENTION_HEADS,\n",
        "            mlp_hidden_dim=MLP_HIDDEN_DIM,\n",
        "            num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        print(\"Freezing BERT parameters in the main model...\")\n",
        "        for param in model.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen.\")\n",
        "\n",
        "        bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "        optimizer_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = AdamW(optimizer_params, lr=LEARNING_RATE)\n",
        "        scheduler = None\n",
        "        if len(train_loader) > 0 and NUM_EPOCHS > 0:\n",
        "            num_training_steps_per_epoch = len(train_loader)\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs.\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_val_f1_at_best_acc = 0.0 # Store F1 at the point of best accuracy\n",
        "        best_model_state_path = f\"best_model_{config_name}.pth\"\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader for {config_name} is empty. Skipping training.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_new_model(model, train_loader, optimizer, scheduler, bce_criterion,\n",
        "                                contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS,\n",
        "                                CONTRASTIVE_LOSS_WEIGHT, model_config_iter, bert_tokenizer_global)\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy, val_f1 = validate_or_test_new_model(model, val_loader, bce_criterion, DEVICE,\n",
        "                                                                    epoch, NUM_EPOCHS, model_config_iter,\n",
        "                                                                    bert_tokenizer_global, mode=\"Val\")\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        best_val_f1_at_best_acc = val_f1 # Save F1 at this best accuracy point\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f} (F1: {best_val_f1_at_best_acc:.4f}). Saving model...\")\n",
        "                        torch.save(model.state_dict(), best_model_state_path)\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (corresponding F1: {best_val_f1_at_best_acc:.4f})\")\n",
        "\n",
        "        print(f\"\\nStarting test phase for {config_name}...\")\n",
        "        test_accuracy, test_f1, test_loss = 0.0, 0.0, 0.0 # Initialize\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader for {config_name} is empty. Skipping test.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                                               'test_acc': 0.0, 'test_f1':0.0, 'test_loss': 0.0}\n",
        "        else:\n",
        "            if os.path.exists(best_model_state_path) and best_val_accuracy_for_config > 0: # Check if model was saved\n",
        "                print(f\"Loading best model state from {best_model_state_path} for testing.\")\n",
        "                model.load_state_dict(torch.load(best_model_state_path, map_location=DEVICE))\n",
        "            elif best_val_accuracy_for_config == 0 and len(train_loader) > 0 : # Was trained, but no improvement or no val\n",
        "                print(f\"No best validation model saved (or validation accuracy was 0), using model from last training epoch for testing.\")\n",
        "            elif len(train_loader) == 0: # Not trained\n",
        "                print(f\"No training was performed for {config_name}. Testing with initialized model (results might be poor).\")\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test_new_model(\n",
        "                model, test_loader, bce_criterion, DEVICE, epoch=None, num_epochs=NUM_EPOCHS, # epoch=None for final test\n",
        "                current_modality_config=model_config_iter, tokenizer_for_padding=bert_tokenizer_global, mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                'test_acc': test_accuracy, 'test_f1': test_f1, 'test_loss': test_loss,\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy: {results.get('val_acc', 0.0):.4f} (Corresponding Val F1: {results.get('val_f1', 0.0):.4f})\")\n",
        "        print(f\"  Test Set Accuracy: {results.get('test_acc', 0.0):.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results.get('test_f1', 0.0):.4f}\")\n",
        "        print(f\"  Test Set Loss: {results.get('test_loss', 0.0):.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q9hBntZmZfS",
        "outputId": "ee1e15cd-757e-447d-d291-2d5c6c7b2d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT model used: bert-base-uncased\n",
            "Hierarchical LSTM: Sentence-level hidden dim 256, Sample-level hidden dim 512, Dropout 0.3\n",
            "Projector output dimension (Stream processor audio/video input): 1024\n",
            "Max BERT length for context/punchline part: 512\n",
            "Stream processor internal text LSTM hidden size: 256\n",
            "Attention token dimension: 32, Tokens per modality: 16\n",
            "Stream processor attention heads: 1, Stream processor text FC Dropout rate: 0.3\n",
            "Final fusion stage attention heads: 1, MLP hidden dimension: 256\n",
            "Training parameters: Batch size 16, Learning rate 8e-05, Epochs 4\n",
            "Contrastive loss: Temperature 0.5, Weight 0.03\n",
            "\n",
            " !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \n",
            "\n",
            "Loading raw data pickle files...\n",
            "Raw data loading complete.\n",
            "Extracting features and labels...\n",
            "Feature and label extraction complete.\n",
            "Structuring data for new dataset format (context/punchline split)...\n",
            "Data structuring complete.\n",
            "Initializing BERT tokenizer...\n",
            "Actual BERT hidden size: 768\n",
            "Creating CustomFeatureDatasetContextPunchline instances...\n",
            "Dataloaders created. Train batches: 475, Val batches: 62, Test batches: 63\n",
            "\n",
            "--- Starting processing for model config: T ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for T... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Train Avg Loss: 0.6663, BCE: 0.6663, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Val Avg BCE: 0.6380, Accuracy: 0.6469, F1: 0.6388\n",
            "Epoch 1 (T): New best validation accuracy: 0.6469 (F1: 0.6388). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Train Avg Loss: 0.6067, BCE: 0.6067, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Val Avg BCE: 0.5953, Accuracy: 0.6847, F1: 0.6771\n",
            "Epoch 2 (T): New best validation accuracy: 0.6847 (F1: 0.6771). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Train Avg Loss: 0.5770, BCE: 0.5770, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Val Avg BCE: 0.5982, Accuracy: 0.6878, F1: 0.6695\n",
            "Epoch 3 (T): New best validation accuracy: 0.6878 (F1: 0.6695). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Train Avg Loss: 0.5473, BCE: 0.5473, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Val Avg BCE: 0.5921, Accuracy: 0.6776, F1: 0.6550\n",
            "Training for T complete. Best validation accuracy for this config: 0.6878 (corresponding F1: 0.6695)\n",
            "\n",
            "Starting test phase for T...\n",
            "Loading best model state from best_model_T.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (T) Test Avg BCE: 0.6000, Accuracy: 0.6811, F1: 0.6708\n",
            "Final test results for T -> Avg BCE Loss: 0.6000, Accuracy: 0.6811, F1: 0.6708\n",
            "\n",
            "--- Starting processing for model config: V ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for V... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Train Avg Loss: 0.6925, BCE: 0.6925, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Val Avg BCE: 0.6886, Accuracy: 0.5459, F1: 0.4914\n",
            "Epoch 1 (V): New best validation accuracy: 0.5459 (F1: 0.4914). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Train Avg Loss: 0.6818, BCE: 0.6818, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Val Avg BCE: 0.6867, Accuracy: 0.5490, F1: 0.5830\n",
            "Epoch 2 (V): New best validation accuracy: 0.5490 (F1: 0.5830). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Train Avg Loss: 0.6713, BCE: 0.6713, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Val Avg BCE: 0.6859, Accuracy: 0.5561, F1: 0.5672\n",
            "Epoch 3 (V): New best validation accuracy: 0.5561 (F1: 0.5672). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Train Avg Loss: 0.6674, BCE: 0.6674, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Val Avg BCE: 0.6863, Accuracy: 0.5643, F1: 0.5472\n",
            "Epoch 4 (V): New best validation accuracy: 0.5643 (F1: 0.5472). Saving model...\n",
            "Training for V complete. Best validation accuracy for this config: 0.5643 (corresponding F1: 0.5472)\n",
            "\n",
            "Starting test phase for V...\n",
            "Loading best model state from best_model_V.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (V) Test Avg BCE: 0.6678, Accuracy: 0.5966, F1: 0.5986\n",
            "Final test results for V -> Avg BCE Loss: 0.6678, Accuracy: 0.5966, F1: 0.5986\n",
            "\n",
            "--- Starting processing for model config: A ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for A... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Train Avg Loss: 0.6936, BCE: 0.6936, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Val Avg BCE: 0.6931, Accuracy: 0.5041, F1: 0.6703\n",
            "Epoch 1 (A): New best validation accuracy: 0.5041 (F1: 0.6703). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Train Avg Loss: 0.6936, BCE: 0.6936, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Val Avg BCE: 0.6933, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Train Avg Loss: 0.6936, BCE: 0.6936, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Val Avg BCE: 0.6931, Accuracy: 0.5041, F1: 0.6703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Train Avg Loss: 0.6935, BCE: 0.6935, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Val Avg BCE: 0.6931, Accuracy: 0.5041, F1: 0.6703\n",
            "Training for A complete. Best validation accuracy for this config: 0.5041 (corresponding F1: 0.6703)\n",
            "\n",
            "Starting test phase for A...\n",
            "Loading best model state from best_model_A.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (A) Test Avg BCE: 0.6933, Accuracy: 0.4930, F1: 0.6604\n",
            "Final test results for A -> Avg BCE Loss: 0.6933, Accuracy: 0.4930, F1: 0.6604\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: T\n",
            "  Best Validation Accuracy: 0.6878 (Corresponding Val F1: 0.6695)\n",
            "  Test Set Accuracy: 0.6811\n",
            "  Test Set F1 Score: 0.6708\n",
            "  Test Set Loss: 0.6000\n",
            "------------------------------\n",
            "Configuration: V\n",
            "  Best Validation Accuracy: 0.5643 (Corresponding Val F1: 0.5472)\n",
            "  Test Set Accuracy: 0.5966\n",
            "  Test Set F1 Score: 0.5986\n",
            "  Test Set Loss: 0.6678\n",
            "------------------------------\n",
            "Configuration: A\n",
            "  Best Validation Accuracy: 0.5041 (Corresponding Val F1: 0.6703)\n",
            "  Test Set Accuracy: 0.4930\n",
            "  Test Set F1 Score: 0.6604\n",
            "  Test Set Loss: 0.6933\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-head attention, new loss function, remove last self-attention layer\n",
        "# Import pickle module, used for serializing and deserializing Python object structures\n",
        "import pickle\n",
        "# Import numpy library, used for scientific computing, especially array operations\n",
        "import numpy as np\n",
        "# Import PyTorch library, an open-source machine learning framework\n",
        "import torch\n",
        "# Import PyTorch's neural network module\n",
        "import torch.nn as nn\n",
        "# Import PyTorch's neural network functional library\n",
        "import torch.nn.functional as F\n",
        "# Import Dataset and DataLoader classes from PyTorch, used for data loading\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Import auto tokenizer, auto model, and learning rate scheduler from transformers library\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "# Import AdamW optimizer from PyTorch\n",
        "from torch.optim import AdamW\n",
        "# Import accuracy and F1 score calculation functions from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Import copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "import copy\n",
        "# Import tqdm library, used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# Import os module, used for file path operations, etc.\n",
        "import os\n",
        "# Import functions for handling variable-length sequences from PyTorch's RNN utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "# Google Drive mount path (example)\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\"\n",
        "# Base path where feature files are located (example)\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\")\n",
        "\n",
        "# Path to the dataset split file\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "# Path to the OpenFace feature file\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "# Path to the COVAREP feature file\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "# Path to the language feature file\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "# Path to the humor label file\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "\n",
        "# Audio word-level feature dimension constant\n",
        "_AUDIO_WORD_DIM_CONST = 81\n",
        "# Video word-level feature dimension constant\n",
        "_VIDEO_WORD_DIM_CONST = 371\n",
        "# Hidden dimension of sentence-level LSTM in Hierarchical LSTM (Modified to align with Script_B's configuration idea)\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "# Hidden dimension of sample-level LSTM in Hierarchical LSTM (also its output dimension, projector layer input dimension) (Modified to align with Script_B)\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "# Helper function to load pickle files\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        # Open file in binary read mode\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            # Load pickle data\n",
        "            return pickle.load(f)\n",
        "    # Handle possible UnicodeDecodeError\n",
        "    except UnicodeDecodeError:\n",
        "        # If UnicodeDecodeError occurs, try opening with latin1 encoding\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    # Handle other possible exceptions\n",
        "    except Exception as e:\n",
        "        print(f'Cannot load data {pickle_file}: {e}')\n",
        "        # Raise exception\n",
        "        raise\n",
        "\n",
        "# Helper function to safely prepare feature data for np.array()\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    # If input data is None, return an empty list\n",
        "    if feature_data is None: return []\n",
        "    # If input data is a numpy array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's an empty numpy array, return an empty list\n",
        "        if feature_data.size == 0: return []\n",
        "        # Return non-empty numpy array\n",
        "        return feature_data\n",
        "    # If input data is a list\n",
        "    if isinstance(feature_data, list):\n",
        "        # If it's an empty list, return an empty list\n",
        "        if not feature_data: return []\n",
        "        # Return non-empty list\n",
        "        return feature_data\n",
        "    # Other unexpected types, return an empty list (can add a warning)\n",
        "    return []\n",
        "\n",
        "# Function to extract features and labels\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    # Initialize lists to store various features and labels\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "    # Iterate through the ID list\n",
        "    for hid in id_list:\n",
        "        # Add punchline text\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        # Add context text list\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP (audio) feature processing\n",
        "        # Prepare COVAREP features for the punchline\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline audio features to a float32 numpy array and add\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp, dtype=np.float32))\n",
        "        # Process context COVAREP features (one feature array per sentence)\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp, dtype=np.float32))\n",
        "        # Add the list of processed context audio features\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace (video) feature processing\n",
        "        # Prepare OpenFace features for the punchline\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline video features to a float32 numpy array and add\n",
        "        of_p_list.append(np.array(prepared_punchline_of, dtype=np.float32))\n",
        "        # Process context OpenFace features\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of, dtype=np.float32))\n",
        "        # Add the list of processed context video features\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        # Add labels\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    # Return all extracted features and labels, specifying the dtype for numpy arrays\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object), np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object), np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object), np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "# Prepare data for the new dataset structure: output a list of samples, each sample is a dictionary containing all sentence features/texts\n",
        "# Among them, the features/text of the punchline will be the last item in the corresponding modality list\n",
        "def concatenate_multimodal_data_for_dataset(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    # Get the number of samples (based on the number of context sentences)\n",
        "    num_samples = len(cs)\n",
        "    # List to store all sample data\n",
        "    all_samples_data = []\n",
        "    # Iterate through each sample\n",
        "    for i in range(num_samples):\n",
        "        # Data dictionary for a single sample, containing 'audio', 'video', 'text' keys\n",
        "        sample_data = {'audio': [], 'video': [], 'text': []}\n",
        "\n",
        "        # Audio data processing\n",
        "        # Extract context audio features, ensuring they are valid numpy arrays (word count > 0, correct dimension)\n",
        "        current_sample_audio = [s for s in list(cvp_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _AUDIO_WORD_DIM_CONST]\n",
        "        # Get punchline audio features\n",
        "        punchline_audio = cvp_p[i]\n",
        "        # Append punchline audio features to the end of the list, if valid\n",
        "        if isinstance(punchline_audio, np.ndarray) and punchline_audio.ndim == 2 and punchline_audio.shape[0] > 0 and punchline_audio.shape[1] == _AUDIO_WORD_DIM_CONST:\n",
        "            current_sample_audio.append(punchline_audio)\n",
        "        # If the current audio list is empty (both context and punchline are invalid or missing), add a placeholder for the punchline (single sample, correct dimension)\n",
        "        elif not current_sample_audio:\n",
        "            current_sample_audio.append(np.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        # Store the processed audio feature list into the sample data dictionary\n",
        "        sample_data['audio'] = current_sample_audio\n",
        "\n",
        "        # Video data processing (logic same as audio)\n",
        "        current_sample_video = [s for s in list(of_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _VIDEO_WORD_DIM_CONST]\n",
        "        punchline_video = of_p[i]\n",
        "        if isinstance(punchline_video, np.ndarray) and punchline_video.ndim == 2 and punchline_video.shape[0] > 0 and punchline_video.shape[1] == _VIDEO_WORD_DIM_CONST:\n",
        "            current_sample_video.append(punchline_video)\n",
        "        elif not current_sample_video:\n",
        "            current_sample_video.append(np.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        sample_data['video'] = current_sample_video\n",
        "\n",
        "        # Text data processing\n",
        "        # Extract context sentence text list\n",
        "        current_sample_text = [s for s in list(cs[i]) if isinstance(s, str)]\n",
        "        # Get punchline text\n",
        "        punchline_text_str = ps[i]\n",
        "        # If the punchline text is a string, append it\n",
        "        if isinstance(punchline_text_str, str):\n",
        "            current_sample_text.append(punchline_text_str)\n",
        "        # If the current text list is empty (both context and punchline are invalid or missing), add an empty string as a punchline placeholder\n",
        "        elif not current_sample_text:\n",
        "            current_sample_text.append(\"\")\n",
        "        sample_data['text'] = current_sample_text\n",
        "\n",
        "        # Add the current sample's data dictionary to the total list\n",
        "        all_samples_data.append(sample_data)\n",
        "    # Return the list containing all sample data\n",
        "    return all_samples_data\n",
        "\n",
        "\n",
        "# --- Dataset Class: Modified for Context/Punchline Splitting ---\n",
        "class CustomFeatureDatasetContextPunchline(Dataset):\n",
        "    # Initialization function\n",
        "    def __init__(self, list_of_sample_data_dicts, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len_for_part=512,\n",
        "                 audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST):\n",
        "\n",
        "        # List of sample data dictionaries (each element is a sample, containing 'audio', 'video', 'text' keys)\n",
        "        self.list_of_sample_data_dicts = list_of_sample_data_dicts\n",
        "        # List of labels, converted to torch.long type\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long)\n",
        "        # BERT tokenizer\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        # Maximum BERT length for each part (context/punchline)\n",
        "        self.max_bert_len_for_part = max_bert_len_for_part\n",
        "        # Audio word feature dimension\n",
        "        self.audio_word_dim = audio_word_dim\n",
        "        # Video word feature dimension\n",
        "        self.video_word_dim = video_word_dim\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    # Helper function to tokenize the text part\n",
        "    def _tokenize_text_part(self, text_sentences_list):\n",
        "        # If the text list is empty\n",
        "        if not text_sentences_list:\n",
        "            # If the tokenizer has a pad token, use it, otherwise an empty string might be tokenized into special tokens\n",
        "            processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "        else:\n",
        "            # Join all sentences in the sentence list with spaces\n",
        "            processed_text = \" \".join(text_sentences_list)\n",
        "            # If it's only whitespace or empty after joining\n",
        "            if not processed_text.strip():\n",
        "                processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "\n",
        "        # Call the tokenizer to tokenize\n",
        "        bert_inputs = self.tokenizer(\n",
        "            processed_text, add_special_tokens=True, return_attention_mask=True, # Add special tokens, return attention_mask\n",
        "            max_length=self.max_bert_len_for_part, padding='max_length', truncation=True, # Max length, pad to max length, truncate\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        )\n",
        "        # Return input_ids and attention_mask, and remove the batch dimension (because this is single sample processing)\n",
        "        return bert_inputs[\"input_ids\"].squeeze(0), bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    # Helper function to process audio/video parts\n",
        "    # all_sentences_features_for_sample: List of all sentence features for the entire sample (list of numpy arrays)\n",
        "    # part_sentences_indices: Indices in the total sentence list that the current part (context or punchline) should contain\n",
        "    # word_dim: Word feature dimension for audio or video\n",
        "    def _process_av_part(self, all_sentences_features_for_sample, part_sentences_indices, word_dim):\n",
        "        # List to store feature tensors of all sentences in this part\n",
        "        part_features_list = []\n",
        "        # If the sample itself does not have any sentence features (e.g., the entire sample is empty)\n",
        "        if not all_sentences_features_for_sample:\n",
        "            # Add a placeholder tensor (1 word, specified dimension)\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "            return part_features_list\n",
        "\n",
        "        # Iterate through the sentence indices of the specified part\n",
        "        for sent_idx in part_sentences_indices:\n",
        "            # Ensure the index is within the valid range\n",
        "            if 0 <= sent_idx < len(all_sentences_features_for_sample):\n",
        "                # Get features of a single sentence (numpy array)\n",
        "                sent_feat = all_sentences_features_for_sample[sent_idx]\n",
        "                # Validate feature validity: is a numpy array, 2D, word count > 0, correct dimension\n",
        "                if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0 and sent_feat.shape[1] == word_dim:\n",
        "                    # Convert to PyTorch tensor and add to the list\n",
        "                    part_features_list.append(torch.as_tensor(sent_feat, dtype=torch.float32))\n",
        "\n",
        "        # If this part is empty after processing (e.g., all sentences are invalid or indices are out of range, or the specified index list is empty)\n",
        "        if not part_features_list:\n",
        "            # Add a placeholder tensor for this part\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "        return part_features_list\n",
        "\n",
        "\n",
        "    # Method to get single sample data\n",
        "    def __getitem__(self, index):\n",
        "        # Get the sample data dictionary for the current index\n",
        "        sample_data = self.list_of_sample_data_dicts[index]\n",
        "        # Audio: list of numpy arrays (sentence features)\n",
        "        audio_all_sents_raw = sample_data['audio']\n",
        "        # Video: list of numpy arrays (sentence features)\n",
        "        video_all_sents_raw = sample_data['video']\n",
        "        # Text: list of sentence strings\n",
        "        text_all_sents_str = sample_data['text']\n",
        "        # Get label\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        # Determine the total number of sentences based on the number of text sentences\n",
        "        n_total_sents = len(text_all_sents_str)\n",
        "\n",
        "        # Prepare placeholder input_ids and attention_mask for empty text parts\n",
        "        empty_ids, empty_mask = self._tokenize_text_part([])\n",
        "\n",
        "        # Case 1: If the sample has no sentences at all (n_total_sents == 0)\n",
        "        if n_total_sents == 0:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is empty/placeholder\n",
        "            pl_audio_part = self._process_av_part([], [], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = empty_ids, empty_mask\n",
        "\n",
        "        # Case 2: If there is only one sentence, treat it as only punchline, context is empty\n",
        "        elif n_total_sents == 1:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty index list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is this one sentence (index 0)\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, [0], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, [0], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[0]])\n",
        "\n",
        "        # Case 3: If there are multiple sentences, split into context and punchline\n",
        "        else:\n",
        "            # Context sentence indices: from 0 to the second to last\n",
        "            ctx_indices = list(range(n_total_sents - 1))\n",
        "            # Punchline sentence index: only the last one\n",
        "            pl_indices = [n_total_sents - 1]\n",
        "\n",
        "            # Process context part\n",
        "            ctx_audio_part = self._process_av_part(audio_all_sents_raw, ctx_indices, self.audio_word_dim)\n",
        "            ctx_video_part = self._process_av_part(video_all_sents_raw, ctx_indices, self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in ctx_indices])\n",
        "\n",
        "            # Process punchline part\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, pl_indices, self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, pl_indices, self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in pl_indices])\n",
        "\n",
        "        # Return a tuple of context data, punchline data, and label\n",
        "        return (ctx_audio_part, ctx_video_part, ctx_input_ids, ctx_attention_mask,\n",
        "                pl_audio_part, pl_video_part, pl_input_ids, pl_attention_mask,\n",
        "                label)\n",
        "\n",
        "# --- Custom Collate Function for Context/Punchline Data ---\n",
        "def custom_collate_fn_context_punchline(batch):\n",
        "    # batch is a list where each element is the tuple returned by __getitem__\n",
        "    # Unpack batch data into respective lists\n",
        "    (ctx_audio_list, ctx_video_list, ctx_ids_list, ctx_mask_list,\n",
        "     pl_audio_list, pl_video_list, pl_ids_list, pl_mask_list,\n",
        "     labels_list) = zip(*batch)\n",
        "\n",
        "    # Directly stack text IDs, masks, and labels (they are already fixed-size tensors)\n",
        "    batched_ctx_ids = torch.stack(ctx_ids_list)\n",
        "    batched_ctx_masks = torch.stack(ctx_mask_list)\n",
        "    batched_pl_ids = torch.stack(pl_ids_list)\n",
        "    batched_pl_masks = torch.stack(pl_mask_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Helper function to process a list of audio/video data for a part (e.g., context audio)\n",
        "    # part_data_list: A list of samples, where each sample is a list of sentence tensors\n",
        "    # word_dim_const: Word feature dimension of this modality\n",
        "    def _collate_av_part(part_data_list, word_dim_const):\n",
        "        # Get the number of sentences in each sample\n",
        "        sample_lengths = [len(sample) for sample in part_data_list]\n",
        "        # Maximum number of sentences in the batch, 0 if empty\n",
        "        max_sents = max(sample_lengths) if sample_lengths else 0\n",
        "\n",
        "        # Get the word count of each sentence and find the maximum word count\n",
        "        sentence_word_counts_flat = []\n",
        "        for sample in part_data_list: # Iterate through each sample\n",
        "            for sentence_tensor in sample: # Iterate through each sentence tensor in the sample\n",
        "                sentence_word_counts_flat.append(sentence_tensor.shape[0]) # Add the word count of this sentence\n",
        "        # Maximum number of words in the batch, 0 if empty\n",
        "        max_words = max(sentence_word_counts_flat) if sentence_word_counts_flat else 0\n",
        "\n",
        "        # Ensure max_words and max_sents are at least 1 to avoid zero dimensions in tensors\n",
        "        max_words = max(1, max_words)\n",
        "        max_sents = max(1, max_sents)\n",
        "\n",
        "        # Create padded feature tensor and length tensor\n",
        "        # padded_features: (batch_size, max_sentences, max_words, feature_dimension)\n",
        "        # sentence_lengths_tensor: (batch_size, max_sentences) - records the actual word count of each sentence\n",
        "        padded_features = torch.zeros(len(part_data_list), max_sents, max_words, word_dim_const)\n",
        "        sentence_lengths_tensor = torch.zeros(len(part_data_list), max_sents, dtype=torch.long)\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, sample in enumerate(part_data_list):\n",
        "            # Iterate through each sentence tensor in the sample\n",
        "            for j, sentence_tensor in enumerate(sample):\n",
        "                # Word count of the current sentence\n",
        "                num_words = sentence_tensor.shape[0]\n",
        "                # Pad only if there are words\n",
        "                if num_words > 0:\n",
        "                    # Pad features into the padded_features tensor\n",
        "                    padded_features[i, j, :num_words, :] = sentence_tensor\n",
        "                    # Record the actual word count into the sentence_lengths_tensor tensor\n",
        "                    sentence_lengths_tensor[i, j] = num_words\n",
        "        # Return padded features, list of sentence counts per sample (as tensor), and word counts per sentence tensor\n",
        "        return padded_features, torch.tensor(sample_lengths, dtype=torch.long), sentence_lengths_tensor\n",
        "\n",
        "    # Process audio and video data for context and punchline separately\n",
        "    # ctx_padded_audio: (B, S_ctx_max, W_ctx_max, D_audio)\n",
        "    # ctx_audio_sl: (B,) - Actual number of sentences per sample for context\n",
        "    # ctx_audio_ssl: (B, S_ctx_max) - Actual word count of each sentence per sample for context\n",
        "    ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl = _collate_av_part(ctx_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    ctx_padded_video, ctx_video_sl, ctx_video_ssl = _collate_av_part(ctx_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "    pl_padded_audio, pl_audio_sl, pl_audio_ssl = _collate_av_part(pl_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    pl_padded_video, pl_video_sl, pl_video_ssl = _collate_av_part(pl_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "\n",
        "    # Return all processed batch data\n",
        "    return (ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl, # Context audio (features, sample sentence count, words per sentence)\n",
        "            ctx_padded_video, ctx_video_sl, ctx_video_ssl, # Context video\n",
        "            batched_ctx_ids, batched_ctx_masks,             # Context text\n",
        "            pl_padded_audio, pl_audio_sl, pl_audio_ssl,     # Punchline audio\n",
        "            pl_padded_video, pl_video_sl, pl_video_ssl,     # Punchline video\n",
        "            batched_pl_ids, batched_pl_masks,               # Punchline text\n",
        "            batched_labels)                                 # Labels\n",
        "\n",
        "\n",
        "# --- Hierarchical LSTM Aggregator ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Hidden dimension of sentence-level LSTM\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        # Hidden dimension of sample-level LSTM\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        # Sentence-level LSTM: input word embeddings, output sentence representation\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed, output dimension will become 2*hidden_dim\n",
        "\n",
        "        # If sentence LSTM is bidirectional, the input dimension of sample LSTM needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        # Sample-level LSTM: input sentence representations, output sample representation\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sentences, max_words, word_dimension)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sentences) - actual word count per sentence\n",
        "\n",
        "        # Get the shape of the feature tensor\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "        # Final output dimension of sample LSTM (considering bidirectional case)\n",
        "        final_output_dim_sample = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "\n",
        "        # Handle the extreme case where all inputs in the batch are empty\n",
        "        # If max_sentences or max_words is 0, or batch_size is 0, or all sample_lengths are 0\n",
        "        if max_sents == 0 or max_words == 0 or batch_size == 0 or torch.all(sample_lengths == 0):\n",
        "            # Return a zero tensor with shape (batch_size, final_output_dim_sample)\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sentence dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        # If all sentences are empty (all lengths are 0)\n",
        "        if not torch.any(valid_sents_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sentence features and corresponding lengths\n",
        "        sents_features_packed_data = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed_data = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack padded sequence (length tensor needs to be moved to CPU for packing)\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed_data, sents_word_lengths_packed_data.cpu(),\n",
        "                                                batch_first=True, enforce_sorted=False)\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers*num_directions, B*S_valid, sentence_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get the actual output dimension of sentence LSTM (considering bidirectional)\n",
        "        sent_hidden_dim_actual = self.sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "        # Get the hidden state of the last time step (for unidirectional LSTM, take the last layer; for bidirectional, concatenate the last time steps of the last two layers)\n",
        "        # Output shape: (B*S_valid, sentence_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "        # Apply dropout to sentence embeddings\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Put valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Create a zero tensor with shape (B*S, actual_sentence_hidden_dim)\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        # Fill valid sentence embeddings into corresponding positions\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent), reshape to sample LSTM input format\n",
        "        sample_features_for_sample_lstm = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack padded sequence (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0 (i.e., samples with actual sentence count of 0)\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        # If all samples are empty (actual sentence counts are all 0)\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sample features and corresponding lengths\n",
        "        sample_features_packed_input_data = sample_features_for_sample_lstm[valid_sample_indices]\n",
        "        sample_lengths_packed_data = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input_data, sample_lengths_packed_data.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers*num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get the hidden state of the last time step\n",
        "        # Output shape: (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "        # Apply dropout to sample embeddings\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Put valid sample embeddings back to their original positions, use zero vectors for empty samples\n",
        "        # Create a zero tensor with shape (B, final_output_dim_sample_lstm)\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "        # Fill valid sample embeddings into corresponding positions\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "        # Return final sample embeddings\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "# --- GLU Linear Layer ---\n",
        "class GLULinear(nn.Module):\n",
        "    # Initialization function, input dimension and output dimension\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GLULinear, self).__init__()\n",
        "        # The first linear layer is followed by a GELU activation function\n",
        "        self.layer1 = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU())\n",
        "        # The second linear layer\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    # Forward propagation function\n",
        "    def forward(self, x):\n",
        "        # Element-wise multiplication of the outputs of the two linear layers\n",
        "        return self.layer1(x) * self.layer2(x)\n",
        "\n",
        "# --- Advanced Cross-Attention/Self-Attention Module ---\n",
        "class MultiHeadAttentionModule(nn.Module):\n",
        "    # Initialization function\n",
        "    # dim: feature dimension, num_heads: number of attention heads\n",
        "    def __init__(self, dim, num_heads=1):\n",
        "        super(MultiHeadAttentionModule, self).__init__()\n",
        "        # Feature dimension\n",
        "        self.dim = dim\n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.head_dim = dim // num_heads\n",
        "        # Ensure dimension is divisible by the number of heads\n",
        "        if self.head_dim * num_heads != self.dim:\n",
        "            raise ValueError(\"dim must be divisible by num_heads\")\n",
        "\n",
        "        # Linear layer to generate Key\n",
        "        self.K_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Value\n",
        "        self.V_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Query\n",
        "        self.Q_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Softmax layer, used to calculate attention weights\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        # Fully connected layer before output\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "    # Forward propagation function\n",
        "    # feat1_query is Query, feat2_key_value is Key and Value\n",
        "    # mask: optional attention mask\n",
        "    def forward(self, feat1_query, feat2_key_value, mask=None):\n",
        "        # Query shape: (batch_size, Query_sequence_length, Query_dimension)\n",
        "        B_q, N_q, C_q = feat1_query.shape\n",
        "        # Key/Value shape: (batch_size, Key/Value_sequence_length, Key/Value_dimension)\n",
        "        B_kv, N_kv, C_kv = feat2_key_value.shape\n",
        "\n",
        "        # Check if batch sizes of Query and Key/Value match\n",
        "        if B_q != B_kv: raise ValueError(f\"Batch sizes do not match: Query is {B_q}, Key/Value is {B_kv}\")\n",
        "\n",
        "        # Generate Q, K, V and adjust shape for multi-head: (batch, num_heads, sequence_length, head_dimension)\n",
        "        # Q: (B, N_q, C_q) -> (B, N_q, num_heads, head_dim) -> (B, num_heads, N_q, head_dim)\n",
        "        Q = self.Q_layer(feat1_query).reshape(B_q, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        K = self.K_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        V = self.V_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate dot product of Q and K_transpose and scale ( scaled_dot_product = (Q @ K.T) / sqrt(head_dim) )\n",
        "        # dots shape: (B, num_heads, N_q, N_kv)\n",
        "        dots = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # If a mask is provided (usually a padding mask for K,V sequence, shape B, N_kv)\n",
        "        if mask is not None:\n",
        "            # unsqueeze expands the mask to (B, 1, 1, N_kv) to match the shape of dots (B, nH, N_q, N_kv) for broadcasting\n",
        "            # Fill positions in dots where mask is 0 (i.e., padding positions) with a very small value, so their weight approaches 0 after softmax\n",
        "            dots = dots.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "\n",
        "        # Calculate attention weights (attn_weights shape: B, num_heads, N_q, N_kv)\n",
        "        attn_weights = self.attend(dots)\n",
        "        # Attention weights weighted V (out shape: B, num_heads, N_q, head_dim)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # Permute and merge multi-head results: (B, num_heads, N_q, head_dim) -> (B, N_q, num_heads, head_dim) -> (B, N_q, dim)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B_q, N_q, self.dim)\n",
        "        # Pass through output fully connected layer\n",
        "        out = self.fc_out(out)\n",
        "        # Return final output\n",
        "        return out\n",
        "\n",
        "# --- Adapted Single Stream Processor ---\n",
        "class AdaptedSingleStreamProcessor(nn.Module):\n",
        "    # Initialization function\n",
        "    # audio_video_input_dim: Input dimension after audio/video projection\n",
        "    # bert_hidden_size: BERT's hidden layer size\n",
        "    # max_bert_len_for_lstm: Maximum input sequence length expected by the internal text LSTM\n",
        "    # lstm_hidden_size: Hidden size of the internal text LSTM\n",
        "    # attention_token_dim: Dimension of attention tokens\n",
        "    # num_attention_tokens_per_modal: Number of tokens output after processing each modality\n",
        "    # active_modalities: Tuple of active modalities, e.g., ('audio', 'video', 'text')\n",
        "    # num_ca_sa_heads: Number of heads for cross-attention and self-attention modules\n",
        "    # dropout_rate: Dropout rate for the text FC part\n",
        "    def __init__(self, audio_video_input_dim, bert_hidden_size, max_bert_len_for_lstm,\n",
        "                 lstm_hidden_size, attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 active_modalities=('audio', 'video', 'text'), num_ca_sa_heads=1, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        # Number of tokens output after processing each modality\n",
        "        self.n_tokens_per_modal = num_attention_tokens_per_modal\n",
        "        # Dimension of attention tokens\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        # Maximum input sequence length expected by the internal text LSTM\n",
        "        self.max_bert_len_for_lstm_input = max_bert_len_for_lstm\n",
        "        # Active modalities\n",
        "        self.active_modalities = active_modalities\n",
        "        # Flattened feature dimension output by each modality processor (N * C)\n",
        "        self.expected_feature_dim_after_mod_proc = self.n_tokens_per_modal * self.attention_token_dim\n",
        "\n",
        "        # Audio feature processor: receives projected features, maps to NxC token representation\n",
        "        self.audio_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc) # Layer normalization\n",
        "        )\n",
        "        # Video feature processor: logic same as audio\n",
        "        self.vision_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "        # Text processing: BERT hidden state -> LSTM -> Fully connected layer -> NxC token representation\n",
        "        # Text LSTM processor\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        # Text FC processor, maps LSTM output to token representation\n",
        "        self.text_fc_processor_to_tokens = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), # Dropout layer\n",
        "            # LSTM output is (B, S_lstm, H_lstm), after reshape it's (B, S_lstm * H_lstm)\n",
        "            GLULinear(lstm_hidden_size * self.max_bert_len_for_lstm_input, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "\n",
        "        # Attention module instantiation\n",
        "        # ZA: Audio cross-attention (query is concatenation of all modalities, key/value are audio tokens)\n",
        "        self.ZA = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZV: Video cross-attention\n",
        "        self.ZV = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZT: Text cross-attention\n",
        "        self.ZT = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # SA_stream: Intra-stream self-attention\n",
        "        self.SA_stream = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # Final output dimension of this stream processor (after averaging SA output, or, dimension of a single token)\n",
        "        self.output_final_dim = attention_token_dim\n",
        "\n",
        "    # Forward propagation function\n",
        "    # audio_input_proj, vision_input_proj from Hierarchical LSTM + Projector layer (B, D_projector)\n",
        "    # text_sequence_input_bert is BERT's hidden state (B, S_bert, D_bert)\n",
        "    def forward(self, audio_input_proj, vision_input_proj, text_sequence_input_bert):\n",
        "        # Dynamically determine batch size\n",
        "        b = 0\n",
        "        if audio_input_proj is not None and audio_input_proj.nelement() > 0: b = audio_input_proj.shape[0]\n",
        "        elif vision_input_proj is not None and vision_input_proj.nelement() > 0: b = vision_input_proj.shape[0]\n",
        "        elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: b = text_sequence_input_bert.shape[0]\n",
        "\n",
        "        # Handle empty batch (all inputs are empty or None)\n",
        "        if b == 0:\n",
        "            dev = torch.device(\"cpu\") # Default device\n",
        "            # Try to get device from valid input\n",
        "            if audio_input_proj is not None and audio_input_proj.nelement() > 0: dev = audio_input_proj.device\n",
        "            elif vision_input_proj is not None and vision_input_proj.nelement() > 0: dev = vision_input_proj.device\n",
        "            elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: dev = text_sequence_input_bert.device\n",
        "\n",
        "            # Create empty flat features and stream output\n",
        "            empty_flat = torch.zeros(0, self.expected_feature_dim_after_mod_proc, device=dev)\n",
        "            empty_stream_out = torch.zeros(0, 1, self.output_final_dim, device=dev)\n",
        "            # Return empty flat features for contrastive loss and empty stream output\n",
        "            return empty_flat, empty_flat, empty_flat, empty_stream_out\n",
        "\n",
        "        # Get current device (ensure at least one valid input to determine device)\n",
        "        device = audio_input_proj.device if audio_input_proj is not None and audio_input_proj.nelement() > 0 else \\\n",
        "                 (vision_input_proj.device if vision_input_proj is not None and vision_input_proj.nelement() > 0 else \\\n",
        "                  text_sequence_input_bert.device)\n",
        "\n",
        "        # Initialize flat features for contrastive loss (audio_f_flat) and token features for attention (audio_f_tokens)\n",
        "        # Audio processing\n",
        "        audio_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        audio_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        # If audio modality is active, input is not empty, and input is not all zeros (indicates actual content)\n",
        "        if 'audio' in self.active_modalities and audio_input_proj is not None and audio_input_proj.nelement() > 0 and audio_input_proj.abs().sum() > 1e-9 :\n",
        "            audio_f_flat = self.audio_feat_processor_to_tokens(audio_input_proj) # (B, N*C)\n",
        "            audio_f_tokens = audio_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Video processing (logic same as audio)\n",
        "        vis_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        vis_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'video' in self.active_modalities and vision_input_proj is not None and vision_input_proj.nelement() > 0 and vision_input_proj.abs().sum() > 1e-9:\n",
        "            vis_f_flat = self.vision_feat_processor_to_tokens(vision_input_proj)\n",
        "            vis_f_tokens = vis_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim)\n",
        "\n",
        "        # Text processing\n",
        "        text_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        text_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0 and text_sequence_input_bert.abs().sum() > 1e-9:\n",
        "            # Get current BERT output sequence length\n",
        "            current_bert_seq_len = text_sequence_input_bert.shape[1]\n",
        "            text_sequence_input_bert_adjusted = text_sequence_input_bert\n",
        "            # Adjust BERT output sequence length to match LSTM expected input\n",
        "            if current_bert_seq_len != self.max_bert_len_for_lstm_input:\n",
        "                if current_bert_seq_len > self.max_bert_len_for_lstm_input: # Truncate if too long\n",
        "                    text_sequence_input_bert_adjusted = text_sequence_input_bert[:, :self.max_bert_len_for_lstm_input, :]\n",
        "                else: # Pad with zeros if too short\n",
        "                    padding_needed = self.max_bert_len_for_lstm_input - current_bert_seq_len\n",
        "                    # Create padding tensor (B, padding_needed, D_bert)\n",
        "                    padding_tensor = torch.zeros(b, padding_needed, text_sequence_input_bert.shape[2], device=device)\n",
        "                    # Concatenate original BERT output and padding tensor\n",
        "                    text_sequence_input_bert_adjusted = torch.cat([text_sequence_input_bert, padding_tensor], dim=1)\n",
        "\n",
        "            # Pass through text LSTM\n",
        "            lstm_output, _ = self.text_lstm_processor(text_sequence_input_bert_adjusted) # (B, S_lstm, H_lstm)\n",
        "            # Flatten LSTM output: (B, S_lstm * H_lstm)\n",
        "            text_f_flat_from_lstm = lstm_output.reshape(b, -1)\n",
        "            # Process flattened LSTM output through FC layer\n",
        "            text_f_flat = self.text_fc_processor_to_tokens(text_f_flat_from_lstm) # (B, N*C)\n",
        "            # Reshape to token form\n",
        "            text_f_tokens = text_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Collect tokens from active modalities with content\n",
        "        active_mod_token_lists = []\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(audio_f_tokens)\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(vis_f_tokens)\n",
        "        if 'text'  in self.active_modalities and text_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(text_f_tokens)\n",
        "\n",
        "        # If there are no active modalities with content\n",
        "        if not active_mod_token_lists:\n",
        "            # Return flat features and zero stream output (because there's no content for attention calculation)\n",
        "            return audio_f_flat, vis_f_flat, text_f_flat, torch.zeros(b, 1, self.output_final_dim, device=device)\n",
        "\n",
        "        # Concatenate tokens of active modalities as Query for cross-attention\n",
        "        # query_for_modality_ca shape: (B, num_active_modalities * N, C_token)\n",
        "        query_for_modality_ca = torch.cat(active_mod_token_lists, dim=1)\n",
        "\n",
        "        # Perform inter-modality cross-attention\n",
        "        # Initialize result tensor\n",
        "        res_za, res_zv, res_zt = torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca)\n",
        "        # If audio is active and has content\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9:\n",
        "            # query_for_modality_ca as Query, audio_f_tokens as Key and Value\n",
        "            res_za = self.ZA(query_for_modality_ca, audio_f_tokens)\n",
        "        # If video is active and has content\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zv = self.ZV(query_for_modality_ca, vis_f_tokens)\n",
        "        # If text is active and has content\n",
        "        if 'text' in self.active_modalities and text_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zt = self.ZT(query_for_modality_ca, text_f_tokens)\n",
        "\n",
        "        # Merge cross-attention results (element-wise addition)\n",
        "        feat_after_mod_ca = res_za + res_zv + res_zt\n",
        "        # Intra-stream self-attention, with residual connection\n",
        "        # feat_after_mod_ca as Query, Key, and Value\n",
        "        feat_after_sa_stream = self.SA_stream(feat_after_mod_ca, feat_after_mod_ca) + feat_after_mod_ca\n",
        "        # Average the features after self-attention along the sequence dimension to get the final stream representation\n",
        "        stream_output_representation = torch.mean(feat_after_sa_stream, dim=1) # (B, C_token)\n",
        "\n",
        "        # Return flat features for contrastive loss, and the final stream output representation (add a dimension to match the expected (B, 1, C_token) shape)\n",
        "        return audio_f_flat, vis_f_flat, text_f_flat, stream_output_representation.unsqueeze(1)\n",
        "\n",
        "\n",
        "# --- Main Model: ContextPunchlineHumorModelNew ---\n",
        "class ContextPunchlineHumorModelNew(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self,\n",
        "                 bert_model_name_or_path,\n",
        "                 audio_word_dim, video_word_dim,\n",
        "                 sentence_lstm_hidden_dim, sample_lstm_hidden_dim, hier_lstm_dropout,\n",
        "                 projector_output_dim,\n",
        "                 bert_hidden_size_actual, max_bert_len_for_lstm,\n",
        "                 text_lstm_hidden_size_in_stream,\n",
        "                 attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 stream_ca_sa_heads, stream_dropout_rate,\n",
        "                 final_cross_attention_heads, # MODIFIED: This will now be used for the new final fusion heads\n",
        "                 mlp_hidden_dim, num_classes,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(bert_model_name_or_path)\n",
        "\n",
        "        self.ctx_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.ctx_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.context_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        self.pl_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.pl_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.punchline_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Final Fusion ---\n",
        "        self.final_fusion_input_dim = attention_token_dim\n",
        "\n",
        "        self.final_ca_query_streams_on_ctx_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_ca_query_streams_on_pl_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_fusion_sa_after_ca_sum = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        # REMOVED: self.cross_attention_final\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.final_fusion_input_dim, mlp_hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(mlp_hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self,\n",
        "                ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl,\n",
        "                ctx_padded_video, ctx_video_sl, ctx_video_ssl,\n",
        "                ctx_input_ids, ctx_attention_mask,\n",
        "                pl_padded_audio, pl_audio_sl, pl_audio_ssl,\n",
        "                pl_padded_video, pl_video_sl, pl_video_ssl,\n",
        "                pl_input_ids, pl_attention_mask,\n",
        "                current_modality_config=None, tokenizer_for_padding=None\n",
        "                ):\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_a = self.ctx_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_a_vec = torch.zeros(ctx_padded_audio.shape[0], actual_hier_lstm_output_dim_ctx_a, device=ctx_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(ctx_audio_sl > 0):\n",
        "                ctx_a_vec = self.ctx_audio_hier_lstm(ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_v = self.ctx_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_v_vec = torch.zeros(ctx_padded_video.shape[0], actual_hier_lstm_output_dim_ctx_v, device=ctx_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(ctx_video_sl > 0):\n",
        "                ctx_v_vec = self.ctx_video_hier_lstm(ctx_padded_video, ctx_video_sl, ctx_video_ssl)\n",
        "\n",
        "        ctx_a_proj = self.ctx_audio_projector(ctx_a_vec)\n",
        "        ctx_v_proj = self.ctx_video_projector(ctx_v_vec)\n",
        "\n",
        "        ctx_bert_hs = torch.zeros(ctx_input_ids.shape[0], ctx_input_ids.shape[1], self.bert_model.config.hidden_size, device=ctx_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(ctx_attention_mask.sum(dim=1) > 0):\n",
        "                ctx_bert_outputs = self.bert_model(input_ids=ctx_input_ids, attention_mask=ctx_attention_mask)\n",
        "                ctx_bert_hs = ctx_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        ctx_audio_f_flat, ctx_vis_f_flat, ctx_text_f_flat, ctx_stream_repr = self.context_processor(ctx_a_proj, ctx_v_proj, ctx_bert_hs)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_a = self.pl_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_a_vec = torch.zeros(pl_padded_audio.shape[0], actual_hier_lstm_output_dim_pl_a, device=pl_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(pl_audio_sl > 0):\n",
        "                pl_a_vec = self.pl_audio_hier_lstm(pl_padded_audio, pl_audio_sl, pl_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_v = self.pl_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_v_vec = torch.zeros(pl_padded_video.shape[0], actual_hier_lstm_output_dim_pl_v, device=pl_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(pl_video_sl > 0):\n",
        "                pl_v_vec = self.pl_video_hier_lstm(pl_padded_video, pl_video_sl, pl_video_ssl)\n",
        "\n",
        "        pl_a_proj = self.pl_audio_projector(pl_a_vec)\n",
        "        pl_v_proj = self.pl_video_projector(pl_v_vec)\n",
        "\n",
        "        pl_bert_hs = torch.zeros(pl_input_ids.shape[0], pl_input_ids.shape[1], self.bert_model.config.hidden_size, device=pl_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(pl_attention_mask.sum(dim=1) > 0):\n",
        "                pl_bert_outputs = self.bert_model(input_ids=pl_input_ids, attention_mask=pl_attention_mask)\n",
        "                pl_bert_hs = pl_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        pl_audio_f_flat, pl_vis_f_flat, pl_text_f_flat, pl_stream_repr = self.punchline_processor(pl_a_proj, pl_v_proj, pl_bert_hs)\n",
        "\n",
        "        # --- MODIFIED: New final fusion logic ---\n",
        "        streams_query = torch.cat((ctx_stream_repr, pl_stream_repr), dim=1)\n",
        "        res_ca_ctx = self.final_ca_query_streams_on_ctx_kv(streams_query, ctx_stream_repr)\n",
        "        res_ca_pl = self.final_ca_query_streams_on_pl_kv(streams_query, pl_stream_repr)\n",
        "        fused_after_ca = res_ca_ctx + res_ca_pl\n",
        "        fused_after_sa = self.final_fusion_sa_after_ca_sum(fused_after_ca, fused_after_ca)\n",
        "        fused_after_sa = fused_after_sa + fused_after_ca # Residual connection for the self-attention on fused representations\n",
        "        fused_representation = torch.mean(fused_after_sa, dim=1)\n",
        "\n",
        "        mlp_out = self.mlp(fused_representation)\n",
        "        logits = self.classifier(mlp_out)\n",
        "\n",
        "        contrastive_features = {\n",
        "            'ctx_audio': ctx_audio_f_flat, 'ctx_video': ctx_vis_f_flat, 'ctx_text': ctx_text_f_flat,\n",
        "            'pl_audio': pl_audio_f_flat, 'pl_video': pl_vis_f_flat, 'pl_text': pl_text_f_flat\n",
        "        }\n",
        "        return logits, contrastive_features\n",
        "\n",
        "\n",
        "# --- Contrastive Loss Function ---\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    # Initialization function, temperature coefficient\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        # Use CrossEntropyLoss to calculate loss (SimCLR style)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward propagation function\n",
        "    # emb_i, emb_j are embeddings from different modalities or views (B, D)\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        # Get batch size\n",
        "        batch_size = emb_i.shape[0]\n",
        "        # Contrastive loss requires at least 2 samples to compute, otherwise return 0 loss\n",
        "        if batch_size <= 1:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # Check if embeddings are all zeros, if so, loss is 0 (to avoid NaN)\n",
        "        # If the sum of absolute values of all elements in either embedding tensor is less than a very small value, it is considered empty or all zeros\n",
        "        if emb_i.abs().sum() < 1e-9 or emb_j.abs().sum() < 1e-9:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # L2 normalize embedding vectors\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        # Concatenate the two groups of normalized embeddings along the batch dimension: (2*B, D)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Calculate similarity matrix (cosine similarity between all sample pairs, then divide by temperature)\n",
        "        # (2*B, D) @ (D, 2*B) -> (2*B, 2*B)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "\n",
        "        # Create labels: for each sample in z_i, its positive sample is the corresponding sample in z_j\n",
        "        # For example, row similarity_matrix[0] is the similarity of z_i[0] with all representations\n",
        "        # Its positive sample z_j[0] has index batch_size + 0 in representations\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        # For each sample in z_j, its positive sample is the corresponding sample in z_i\n",
        "        # For example, row similarity_matrix[batch_size+0] is the similarity of z_j[0] with all representations\n",
        "        # Its positive sample z_i[0] has index 0 in representations\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Calculate loss, separately for z_i querying z_j and z_j querying z_i\n",
        "        # loss_i: z_i as anchor, corresponding sample in z_j as positive\n",
        "        # similarity_matrix[:batch_size] is the similarity of z_i with all representations (B, 2*B)\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        # loss_j: z_j as anchor, corresponding sample in z_i as positive\n",
        "        # similarity_matrix[batch_size:] is the similarity of z_j with all representations (B, 2*B)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        # Return average loss\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "\n",
        "# --- Modified Training Function (Only contrastive loss calculation method is changed) ---\n",
        "def train_new_model(model, data_loader, optimizer, scheduler,\n",
        "                    bce_criterion, contrastive_loss_fn, device, epoch, num_epochs,\n",
        "                    contrastive_loss_weight, current_modality_config, tokenizer_for_padding):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # The BERT part of the model is already globally frozen externally, no explicit model.bert_model.eval() needed here\n",
        "\n",
        "    # Initialize total BCE loss, total contrastive loss, total loss\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0 # Used to accumulate final_simclr_loss_for_batch for each batch\n",
        "    total_loss = 0\n",
        "    # Create tqdm progress bar to display training progress\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    # Iterate through each batch in the data loader\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack data from collate_fn\n",
        "        (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "         pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "         labels) = batch\n",
        "\n",
        "        # Get current batch size\n",
        "        current_batch_size = ctx_a_feat.shape[0]\n",
        "        # If batch is empty, skip\n",
        "        if current_batch_size == 0: continue\n",
        "\n",
        "        # Move data to the specified device (excluding the last label)\n",
        "        batch_data_on_device = []\n",
        "        for tensor_item in batch[:-1]:\n",
        "            batch_data_on_device.append(tensor_item.to(device))\n",
        "        # Move labels to device and convert to long type\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        # Clear optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model forward pass, get classification logits and contrastive features\n",
        "        logits, contrastive_feats = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "        # Calculate BCE classification loss\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # --- Contrastive Loss Calculation (modified to averaging method) ---\n",
        "        final_simclr_loss_for_batch = torch.tensor(0.0, device=device) # Initialize contrastive loss for this batch\n",
        "        if current_batch_size > 1 and contrastive_loss_weight > 0:\n",
        "            accumulated_contrastive_loss_components = []\n",
        "\n",
        "            # Contrastive loss for the context stream\n",
        "            ctx_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_video'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_video'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_video'], contrastive_feats['ctx_text']))\n",
        "\n",
        "            if ctx_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(ctx_individual_losses)))\n",
        "\n",
        "            # Contrastive loss for the punchline stream\n",
        "            pl_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_video'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_video'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_video'], contrastive_feats['pl_text']))\n",
        "\n",
        "            if pl_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(pl_individual_losses)))\n",
        "\n",
        "            # Calculate the final contrastive loss (if multiple components exist, take their average)\n",
        "            if accumulated_contrastive_loss_components:\n",
        "                final_simclr_loss_for_batch = torch.mean(torch.stack(accumulated_contrastive_loss_components))\n",
        "            # else: final_simclr_loss_for_batch remains its initial value of 0.0\n",
        "\n",
        "        # Total loss = BCE loss + contrastive_loss_weight * calculated batch contrastive loss\n",
        "        current_loss = bce_loss + contrastive_loss_weight * final_simclr_loss_for_batch\n",
        "\n",
        "        # Backpropagate to calculate gradients\n",
        "        current_loss.backward()\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        # If a learning rate scheduler is used\n",
        "        if scheduler is not None:\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss values (item() gets scalar value)\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += final_simclr_loss_for_batch.item() # Accumulate the calculated batch contrastive loss\n",
        "        total_loss += current_loss.item()\n",
        "        # Update progress bar display information\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{final_simclr_loss_for_batch.item():.4f}\")\n",
        "\n",
        "    # If data loader is not empty\n",
        "    if len(data_loader) > 0:\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        # Print average training loss for the current epoch\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "\n",
        "\n",
        "# --- Validation/Test Function (Added F1 Score) ---\n",
        "def validate_or_test_new_model(model, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                               current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize total BCE loss\n",
        "    total_bce_loss = 0\n",
        "    # List to store all prediction results\n",
        "    all_preds = []\n",
        "    # List to store all true labels\n",
        "    all_labels = []\n",
        "\n",
        "    # Set progress bar description (corrected logic)\n",
        "    if mode == \"Test\" and epoch is None:\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # and epoch is not None (implicitly for this branch after the first)\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Val\": # epoch should not be None for validation\n",
        "        desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    else: # Fallback, though ideally all cases are covered\n",
        "        desc = f\"Processing [{mode} {current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    # Do not calculate gradients within this block to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through each batch in the data loader\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            # Unpack data\n",
        "            (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "             pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "             labels) = batch\n",
        "\n",
        "            # Get current batch size\n",
        "            current_batch_size = ctx_a_feat.shape[0]\n",
        "            # If batch is empty, skip\n",
        "            if current_batch_size == 0: continue\n",
        "\n",
        "            # Move data to device\n",
        "            batch_data_on_device = [t.to(device) for t in batch[:-1]]\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            # Model forward pass, ignore contrastive features (not needed during validation/testing)\n",
        "            logits, _ = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "            # Calculate BCE loss\n",
        "            bce_loss = bce_criterion(logits, labels)\n",
        "            # Accumulate BCE loss\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            # Get predicted class (index of the max value in logits)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Store prediction results (convert to numpy array)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            # Store true labels (convert to numpy array)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # If data loader is empty or no labels were collected\n",
        "    if len(data_loader) == 0 or len(all_labels) == 0 :\n",
        "        print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty.\")\n",
        "        if mode == \"Val\": return 0.0, 0.0 # Validation mode returns 0.0 accuracy, 0.0 F1\n",
        "        return 0.0, 0.0, 0.0 # Test mode returns 0.0 loss, 0.0 accuracy, 0.0 F1\n",
        "\n",
        "    # Calculate average BCE loss\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    # Calculate accuracy (if label list is not empty)\n",
        "    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
        "    # Calculate F1 score (if label list is not empty), use 'binary' because it's binary classification, zero_division handles boundary cases\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0) if all_labels else 0.0\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # If validation mode, return accuracy and F1\n",
        "    if mode == \"Val\": return accuracy, f1\n",
        "    # If test mode, return average loss, accuracy, and F1\n",
        "    return avg_bce_loss, accuracy, f1\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameter Configuration ---\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # SENTENCE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    # SAMPLE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    HIER_LSTM_DROPOUT = 0.3\n",
        "\n",
        "    PROJECTOR_OUTPUT_DIM = 1024\n",
        "\n",
        "    MAX_BERT_LEN_FOR_PART_DATASET = 512\n",
        "    TEXT_LSTM_HIDDEN_SIZE_IN_STREAM = 256   # This version of the code still uses this parameter\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    STREAM_CA_SA_HEADS = 1\n",
        "    STREAM_DROPOUT_RATE = 0.3\n",
        "\n",
        "    FINAL_CROSS_ATTENTION_HEADS = 1         # Used for all attention modules in the new final fusion structure\n",
        "    MLP_HIDDEN_DIM = 256\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    BATCH_SIZE = 16 # Warning: The new final fusion structure is more complex, may need to reduce this value\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 4 # It is recommended to increase epochs for actual use\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"BERT model used: {BERT_MODEL_NAME_FOR_MAIN}\")\n",
        "    print(f\"Hierarchical LSTM: Sentence-level hidden dim {SENTENCE_LSTM_HIDDEN_DIM_CONFIG}, Sample-level hidden dim {SAMPLE_LSTM_HIDDEN_DIM_CONFIG}, Dropout {HIER_LSTM_DROPOUT}\")\n",
        "    print(f\"Projector output dimension (Stream processor audio/video input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"Max BERT length for context/punchline part: {MAX_BERT_LEN_FOR_PART_DATASET}\")\n",
        "    print(f\"Stream processor internal text LSTM hidden size: {TEXT_LSTM_HIDDEN_SIZE_IN_STREAM}\") # Keep printing as this param is still in model def\n",
        "    print(f\"Attention token dimension: {ATTENTION_TOKEN_DIM}, Tokens per modality: {NUM_ATTENTION_TOKENS_PER_MODAL}\")\n",
        "    print(f\"Stream processor attention heads: {STREAM_CA_SA_HEADS}, Stream processor text FC Dropout rate: {STREAM_DROPOUT_RATE}\")\n",
        "    print(f\"Final fusion stage attention heads: {FINAL_CROSS_ATTENTION_HEADS}, MLP hidden dimension: {MLP_HIDDEN_DIM}\")\n",
        "    print(f\"Training parameters: Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}, Epochs {NUM_EPOCHS}\")\n",
        "    print(f\"Contrastive loss: Temperature {TEMPERATURE_CONTRASTIVE}, Weight {CONTRASTIVE_LOSS_WEIGHT}\")\n",
        "    print(\"\\n !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \\n\")\n",
        "\n",
        "    # --- Load Raw Data ---\n",
        "    print(\"Loading raw data pickle files...\")\n",
        "    # Ensure paths are correct\n",
        "    # Example: data_folds_path = \"path_to_your_gdrive/Project_CCS2-main/sdk_features/data_folds.pkl\"\n",
        "    # Replace with your actual paths\n",
        "    # To run locally, you might need to download these files or adjust paths\n",
        "    # For demonstration, we'll assume files might not exist and add checks or placeholders.\n",
        "    try:\n",
        "        data_folds = load_pickle(data_folds_path)\n",
        "        language_sdk = load_pickle(language_file)\n",
        "        covarep_sdk = load_pickle(covarep_file)\n",
        "        openface_sdk = load_pickle(openface_file)\n",
        "        humor_label_sdk = load_pickle(humor_label_file)\n",
        "        print(\"Raw data loading complete.\")\n",
        "\n",
        "        train_ids = data_folds['train']\n",
        "        dev_ids = data_folds['dev']\n",
        "        test_ids = data_folds['test']\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: One or more data files not found. Please check paths and ensure files exist.\")\n",
        "        print(\"Using placeholder data for demonstration.\")\n",
        "        # Placeholder data for demonstration if files are missing\n",
        "        train_ids, dev_ids, test_ids = ['h1','h2'], ['h3'], ['h4']\n",
        "        language_sdk = {\n",
        "            f'h{i}': {'punchline_sentence': f'Punchline {i}', 'context_sentences': [f'Context sent {i}.1', f'Context sent {i}.2']} for i in range(1, 5)\n",
        "        }\n",
        "        covarep_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _AUDIO_WORD_DIM_CONST).astype(np.float32) if i % 2 == 0 else [], # Some empty\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _AUDIO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        openface_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _VIDEO_WORD_DIM_CONST).astype(np.float32),\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _VIDEO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        humor_label_sdk = {f'h{i}': float(i % 2) for i in range(1,5)}\n",
        "\n",
        "\n",
        "    print(\"Extracting features and labels...\")\n",
        "    (train_ps, train_cs, train_cvp_p, train_cvp_c, train_of_p, train_of_c, train_labels) = \\\n",
        "        extract_features_and_labels(train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (dev_ps, dev_cs, dev_cvp_p, dev_cvp_c, dev_of_p, dev_of_c, dev_labels) = \\\n",
        "        extract_features_and_labels(dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (test_ps, test_cs, test_cvp_p, test_cvp_c, test_of_p, test_of_c, test_labels) = \\\n",
        "        extract_features_and_labels(test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    print(\"Feature and label extraction complete.\")\n",
        "\n",
        "    print(\"Structuring data for new dataset format (context/punchline split)...\")\n",
        "    train_sample_data_dicts = concatenate_multimodal_data_for_dataset(train_cvp_c, train_of_c, train_cs, train_cvp_p, train_of_p, train_ps)\n",
        "    dev_sample_data_dicts = concatenate_multimodal_data_for_dataset(dev_cvp_c, dev_of_c, dev_cs, dev_cvp_p, dev_of_p, dev_ps)\n",
        "    test_sample_data_dicts = concatenate_multimodal_data_for_dataset(test_cvp_c, test_of_c, test_cs, test_cvp_p, test_of_p, test_ps)\n",
        "    print(\"Data structuring complete.\")\n",
        "\n",
        "    print(\"Initializing BERT tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    _bert_temp_model = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = _bert_temp_model.config.hidden_size\n",
        "    del _bert_temp_model\n",
        "    print(f\"Actual BERT hidden size: {BERT_HIDDEN_SIZE_ACTUAL}\")\n",
        "\n",
        "    print(\"Creating CustomFeatureDatasetContextPunchline instances...\")\n",
        "    train_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        train_sample_data_dicts, train_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        dev_sample_data_dicts, dev_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    test_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        test_sample_data_dicts, test_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=custom_collate_fn_context_punchline, drop_last=True if BATCH_SIZE > 1 and len(train_dataset) > BATCH_SIZE else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    print(f\"Dataloaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        {'name': 'AT',  'audio': True,  'video': False,  'text': True},\n",
        "        {'name': 'VT',  'audio': False,  'video': True,  'text': True},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    for config_idx, model_config_iter in enumerate(modality_configurations):\n",
        "        config_name = model_config_iter['name']\n",
        "        print(f\"\\n--- Starting processing for model config: {config_name} ---\")\n",
        "\n",
        "        model = ContextPunchlineHumorModelNew(\n",
        "            bert_model_name_or_path=BERT_MODEL_NAME_FOR_MAIN,\n",
        "            audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=SAMPLE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            hier_lstm_dropout=HIER_LSTM_DROPOUT,\n",
        "            projector_output_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size_actual=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len_for_lstm=MAX_BERT_LEN_FOR_PART_DATASET, # This is max_bert_len_for_part\n",
        "            text_lstm_hidden_size_in_stream=TEXT_LSTM_HIDDEN_SIZE_IN_STREAM,\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            stream_ca_sa_heads=STREAM_CA_SA_HEADS,\n",
        "            stream_dropout_rate=STREAM_DROPOUT_RATE,\n",
        "            final_cross_attention_heads=FINAL_CROSS_ATTENTION_HEADS,\n",
        "            mlp_hidden_dim=MLP_HIDDEN_DIM,\n",
        "            num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        print(\"Freezing BERT parameters in the main model...\")\n",
        "        for param in model.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen.\")\n",
        "\n",
        "        bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "        optimizer_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = AdamW(optimizer_params, lr=LEARNING_RATE)\n",
        "        scheduler = None\n",
        "        if len(train_loader) > 0 and NUM_EPOCHS > 0:\n",
        "            num_training_steps_per_epoch = len(train_loader)\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs.\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_val_f1_at_best_acc = 0.0 # Store F1 at the point of best accuracy\n",
        "        best_model_state_path = f\"best_model_{config_name}.pth\"\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader for {config_name} is empty. Skipping training.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_new_model(model, train_loader, optimizer, scheduler, bce_criterion,\n",
        "                                contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS,\n",
        "                                CONTRASTIVE_LOSS_WEIGHT, model_config_iter, bert_tokenizer_global)\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy, val_f1 = validate_or_test_new_model(model, val_loader, bce_criterion, DEVICE,\n",
        "                                                                    epoch, NUM_EPOCHS, model_config_iter,\n",
        "                                                                    bert_tokenizer_global, mode=\"Val\")\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        best_val_f1_at_best_acc = val_f1 # Save F1 at this best accuracy point\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f} (F1: {best_val_f1_at_best_acc:.4f}). Saving model...\")\n",
        "                        torch.save(model.state_dict(), best_model_state_path)\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (corresponding F1: {best_val_f1_at_best_acc:.4f})\")\n",
        "\n",
        "        print(f\"\\nStarting test phase for {config_name}...\")\n",
        "        test_accuracy, test_f1, test_loss = 0.0, 0.0, 0.0 # Initialize\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader for {config_name} is empty. Skipping test.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                                               'test_acc': 0.0, 'test_f1':0.0, 'test_loss': 0.0}\n",
        "        else:\n",
        "            if os.path.exists(best_model_state_path) and best_val_accuracy_for_config > 0: # Check if model was saved\n",
        "                print(f\"Loading best model state from {best_model_state_path} for testing.\")\n",
        "                model.load_state_dict(torch.load(best_model_state_path, map_location=DEVICE))\n",
        "            elif best_val_accuracy_for_config == 0 and len(train_loader) > 0 : # Was trained, but no improvement or no val\n",
        "                print(f\"No best validation model saved (or validation accuracy was 0), using model from last training epoch for testing.\")\n",
        "            elif len(train_loader) == 0: # Not trained\n",
        "                print(f\"No training was performed for {config_name}. Testing with initialized model (results might be poor).\")\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test_new_model(\n",
        "                model, test_loader, bce_criterion, DEVICE, epoch=None, num_epochs=NUM_EPOCHS, # epoch=None for final test\n",
        "                current_modality_config=model_config_iter, tokenizer_for_padding=bert_tokenizer_global, mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                'test_acc': test_accuracy, 'test_f1': test_f1, 'test_loss': test_loss,\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy: {results.get('val_acc', 0.0):.4f} (Corresponding Val F1: {results.get('val_f1', 0.0):.4f})\")\n",
        "        print(f\"  Test Set Accuracy: {results.get('test_acc', 0.0):.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results.get('test_f1', 0.0):.4f}\")\n",
        "        print(f\"  Test Set Loss: {results.get('test_loss', 0.0):.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8b1e4aaa269f42a1b452c8e16a531faf",
            "8f44be6e500144adbf9ac23b4fc281ff",
            "4ae4a6213e594fe4ab87e64dfb2dfbd2",
            "bdc325b0304e4d3e86696fa18f87b7a5",
            "67000fc1f945470c86326f682723b942",
            "483b484e1e5a4ab5b0db193a1eb3a752",
            "257c73981c484558a681fe3cb04d7747",
            "31dc782270e8475e90cf7186483fceca",
            "529d59e38e5141aa94523e9eba4fced4",
            "5875f60bf0374be7a1329428b4dca9e4",
            "f8ce9509299a4db894199c41c23b9103",
            "613c9619666a4a45b870d3f2d5815c9d",
            "613add5fb0b94c2fbcaaf6e818997a69",
            "92b5a4d1314a4de2ad10471ebebdd217",
            "793db10ccf144c10b7eaad8e01cc62ed",
            "64ed9078a3c64a9f877b84083d1d27af",
            "0181fbba39b54e499114f48d5d098b60",
            "81f5d5c720a340cc902be5032a21cda3",
            "041878effed0407082f6d005b8e107ef",
            "c02fed0594a44775a812820ac61ba679",
            "4e7f8c1f9def4c128aafab95273fda41",
            "60b99912686d43f1adc370cbc2a0dcb1",
            "b517c8ebf62845d99a37b1ef4afaefff",
            "065520d93ec34a16a7654dab1ec37939",
            "d34c1181bfc741c4ab99334e450b210d",
            "6bf447e13c0041bf8ca55b530f9f7ee9",
            "8ec64f55b39b402185f00fbc74785ec5",
            "931bab3a75484b28933cb20be22689ee",
            "9bd5da951e594a2e9688a7b95af36c1e",
            "e8501d6ba016448e9ff0998f39353da7",
            "10ee829191824bc994dac22715c13783",
            "da39dc695fe147a690adaf6c09b61724",
            "8c2606724754402c84b12d8e1cad14bd",
            "cc986c7eb08e4ce7b512cd4b3fab0811",
            "8941192a49ee425594558a6cf43eb793",
            "9843de38e3224bccad34c1bdee071a38",
            "be5cece29fca441bba80ff8ea07cb36c",
            "dcaa17a91a874c98ad3b6baadfcb2ec2",
            "3d76dea8d42b49668fb4bc41bff3b363",
            "899de7e330124fa5bfd88c7bcf1d8741",
            "fc939cd396714f1e8bb31ffb830a4eda",
            "f37d604ef7c1487e9b516a9d83c541b9",
            "f96848e17815481082ace5138314aa78",
            "6adda259908141668cdfe33eaf4abcaa",
            "22444258554740d49673ff2d45f89d09",
            "67b0a191196d4eaea0a152eb3e1854a2",
            "dae77240d5fe4f27b8a4f5ffae6beef4",
            "0595b200744045aebf5fd7bbd89d7a26",
            "ad7ad8672ace41acace0a5cc19f0bd60",
            "7f5ba8c79fbb4870be89f8503606bf01",
            "e579d30bf21b42a998b00985f417390a",
            "5396e080a38c41c8b91e12b8b5c8b055",
            "975ea9ea2a6d45669420b1cfd9a1e2e6",
            "774fa74fbe7c4e6d81ee85f73612824a",
            "eeb7d529cd834b5a9b0cef9f723aaf83"
          ]
        },
        "id": "eDcHwQmLKL4Z",
        "outputId": "71f593b2-0a28-40e8-8ea3-e1b23ae2e36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT model used: bert-base-uncased\n",
            "Hierarchical LSTM: Sentence-level hidden dim 256, Sample-level hidden dim 512, Dropout 0.3\n",
            "Projector output dimension (Stream processor audio/video input): 1024\n",
            "Max BERT length for context/punchline part: 512\n",
            "Stream processor internal text LSTM hidden size: 256\n",
            "Attention token dimension: 32, Tokens per modality: 16\n",
            "Stream processor attention heads: 1, Stream processor text FC Dropout rate: 0.3\n",
            "Final fusion stage attention heads: 1, MLP hidden dimension: 256\n",
            "Training parameters: Batch size 16, Learning rate 5e-05, Epochs 4\n",
            "Contrastive loss: Temperature 0.5, Weight 0.03\n",
            "\n",
            " !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \n",
            "\n",
            "Loading raw data pickle files...\n",
            "Raw data loading complete.\n",
            "Extracting features and labels...\n",
            "Feature and label extraction complete.\n",
            "Structuring data for new dataset format (context/punchline split)...\n",
            "Data structuring complete.\n",
            "Initializing BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b1e4aaa269f42a1b452c8e16a531faf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "613c9619666a4a45b870d3f2d5815c9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b517c8ebf62845d99a37b1ef4afaefff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc986c7eb08e4ce7b512cd4b3fab0811"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22444258554740d49673ff2d45f89d09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual BERT hidden size: 768\n",
            "Creating CustomFeatureDatasetContextPunchline instances...\n",
            "Dataloaders created. Train batches: 475, Val batches: 62, Test batches: 63\n",
            "\n",
            "--- Starting processing for model config: AV ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for AV... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Train Avg Loss: 0.7971, BCE: 0.6932, SimCLR: 3.4647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Val Avg BCE: 0.6885, Accuracy: 0.5449, F1: 0.4147\n",
            "Epoch 1 (AV): New best validation accuracy: 0.5449 (F1: 0.4147). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Train Avg Loss: 0.7682, BCE: 0.6793, SimCLR: 2.9603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Val Avg BCE: 0.6837, Accuracy: 0.5500, F1: 0.5253\n",
            "Epoch 2 (AV): New best validation accuracy: 0.5500 (F1: 0.5253). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Train Avg Loss: 0.7614, BCE: 0.6739, SimCLR: 2.9166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Val Avg BCE: 0.6810, Accuracy: 0.5694, F1: 0.5763\n",
            "Epoch 3 (AV): New best validation accuracy: 0.5694 (F1: 0.5763). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Train Avg Loss: 0.7567, BCE: 0.6698, SimCLR: 2.8956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Val Avg BCE: 0.6809, Accuracy: 0.5633, F1: 0.5437\n",
            "Training for AV complete. Best validation accuracy for this config: 0.5694 (corresponding F1: 0.5763)\n",
            "\n",
            "Starting test phase for AV...\n",
            "Loading best model state from best_model_AV.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (AV) Test Avg BCE: 0.6762, Accuracy: 0.5765, F1: 0.5987\n",
            "Final test results for AV -> Avg BCE Loss: 0.6762, Accuracy: 0.5765, F1: 0.5987\n",
            "\n",
            "--- Starting processing for model config: AT ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for AT... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Train Avg Loss: 0.7662, BCE: 0.6599, SimCLR: 3.5433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Val Avg BCE: 0.6219, Accuracy: 0.6653, F1: 0.7177\n",
            "Epoch 1 (AT): New best validation accuracy: 0.6653 (F1: 0.7177). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Train Avg Loss: 0.6965, BCE: 0.5975, SimCLR: 3.2987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Val Avg BCE: 0.5994, Accuracy: 0.6816, F1: 0.7111\n",
            "Epoch 2 (AT): New best validation accuracy: 0.6816 (F1: 0.7111). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Train Avg Loss: 0.6414, BCE: 0.5459, SimCLR: 3.1840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Val Avg BCE: 0.6204, Accuracy: 0.6765, F1: 0.6558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Train Avg Loss: 0.5147, BCE: 0.4185, SimCLR: 3.2051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Val Avg BCE: 0.6634, Accuracy: 0.6827, F1: 0.6702\n",
            "Epoch 4 (AT): New best validation accuracy: 0.6827 (F1: 0.6702). Saving model...\n",
            "Training for AT complete. Best validation accuracy for this config: 0.6827 (corresponding F1: 0.6702)\n",
            "\n",
            "Starting test phase for AT...\n",
            "Loading best model state from best_model_AT.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (AT) Test Avg BCE: 0.6396, Accuracy: 0.6771, F1: 0.6701\n",
            "Final test results for AT -> Avg BCE Loss: 0.6396, Accuracy: 0.6771, F1: 0.6701\n",
            "\n",
            "--- Starting processing for model config: VT ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for VT... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Train Avg Loss: 0.7721, BCE: 0.6651, SimCLR: 3.5682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Val Avg BCE: 0.6174, Accuracy: 0.6816, F1: 0.7158\n",
            "Epoch 1 (VT): New best validation accuracy: 0.6816 (F1: 0.7158). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Train Avg Loss: 0.6951, BCE: 0.5917, SimCLR: 3.4454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Val Avg BCE: 0.5993, Accuracy: 0.6776, F1: 0.7122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Train Avg Loss: 0.6251, BCE: 0.5233, SimCLR: 3.3918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Val Avg BCE: 0.6028, Accuracy: 0.6786, F1: 0.6884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Train Avg Loss: 0.4559, BCE: 0.3511, SimCLR: 3.4942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Val Avg BCE: 0.7389, Accuracy: 0.6541, F1: 0.6435\n",
            "Training for VT complete. Best validation accuracy for this config: 0.6816 (corresponding F1: 0.7158)\n",
            "\n",
            "Starting test phase for VT...\n",
            "Loading best model state from best_model_VT.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (VT) Test Avg BCE: 0.6129, Accuracy: 0.6851, F1: 0.7167\n",
            "Final test results for VT -> Avg BCE Loss: 0.6129, Accuracy: 0.6851, F1: 0.7167\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AV\n",
            "  Best Validation Accuracy: 0.5694 (Corresponding Val F1: 0.5763)\n",
            "  Test Set Accuracy: 0.5765\n",
            "  Test Set F1 Score: 0.5987\n",
            "  Test Set Loss: 0.6762\n",
            "------------------------------\n",
            "Configuration: AT\n",
            "  Best Validation Accuracy: 0.6827 (Corresponding Val F1: 0.6702)\n",
            "  Test Set Accuracy: 0.6771\n",
            "  Test Set F1 Score: 0.6701\n",
            "  Test Set Loss: 0.6396\n",
            "------------------------------\n",
            "Configuration: VT\n",
            "  Best Validation Accuracy: 0.6816 (Corresponding Val F1: 0.7158)\n",
            "  Test Set Accuracy: 0.6851\n",
            "  Test Set F1 Score: 0.7167\n",
            "  Test Set Loss: 0.6129\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-head attention, new loss function, remove last self-attention layer\n",
        "# Import pickle module, used for serializing and deserializing Python object structures\n",
        "import pickle\n",
        "# Import numpy library, used for scientific computing, especially array operations\n",
        "import numpy as np\n",
        "# Import PyTorch library, an open-source machine learning framework\n",
        "import torch\n",
        "# Import PyTorch's neural network module\n",
        "import torch.nn as nn\n",
        "# Import PyTorch's neural network functional library\n",
        "import torch.nn.functional as F\n",
        "# Import Dataset and DataLoader classes from PyTorch, used for data loading\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Import auto tokenizer, auto model, and learning rate scheduler from transformers library\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "# Import AdamW optimizer from PyTorch\n",
        "from torch.optim import AdamW\n",
        "# Import accuracy and F1 score calculation functions from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Import copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "import copy\n",
        "# Import tqdm library, used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# Import os module, used for file path operations, etc.\n",
        "import os\n",
        "# Import functions for handling variable-length sequences from PyTorch's RNN utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "# Google Drive mount path (example)\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\"\n",
        "# Base path where feature files are located (example)\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\")\n",
        "\n",
        "# Path to the dataset split file\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "# Path to the OpenFace feature file\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "# Path to the COVAREP feature file\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "# Path to the language feature file\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "# Path to the humor label file\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "\n",
        "# Audio word-level feature dimension constant\n",
        "_AUDIO_WORD_DIM_CONST = 81\n",
        "# Video word-level feature dimension constant\n",
        "_VIDEO_WORD_DIM_CONST = 371\n",
        "# Hidden dimension of sentence-level LSTM in Hierarchical LSTM (Modified to align with Script_B's configuration idea)\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "# Hidden dimension of sample-level LSTM in Hierarchical LSTM (also its output dimension, projector layer input dimension) (Modified to align with Script_B)\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "# Helper function to load pickle files\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        # Open file in binary read mode\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            # Load pickle data\n",
        "            return pickle.load(f)\n",
        "    # Handle possible UnicodeDecodeError\n",
        "    except UnicodeDecodeError:\n",
        "        # If UnicodeDecodeError occurs, try opening with latin1 encoding\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    # Handle other possible exceptions\n",
        "    except Exception as e:\n",
        "        print(f'Cannot load data {pickle_file}: {e}')\n",
        "        # Raise exception\n",
        "        raise\n",
        "\n",
        "# Helper function to safely prepare feature data for np.array()\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    # If input data is None, return an empty list\n",
        "    if feature_data is None: return []\n",
        "    # If input data is a numpy array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's an empty numpy array, return an empty list\n",
        "        if feature_data.size == 0: return []\n",
        "        # Return non-empty numpy array\n",
        "        return feature_data\n",
        "    # If input data is a list\n",
        "    if isinstance(feature_data, list):\n",
        "        # If it's an empty list, return an empty list\n",
        "        if not feature_data: return []\n",
        "        # Return non-empty list\n",
        "        return feature_data\n",
        "    # Other unexpected types, return an empty list (can add a warning)\n",
        "    return []\n",
        "\n",
        "# Function to extract features and labels\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    # Initialize lists to store various features and labels\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "    # Iterate through the ID list\n",
        "    for hid in id_list:\n",
        "        # Add punchline text\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        # Add context text list\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP (audio) feature processing\n",
        "        # Prepare COVAREP features for the punchline\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline audio features to a float32 numpy array and add\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp, dtype=np.float32))\n",
        "        # Process context COVAREP features (one feature array per sentence)\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp, dtype=np.float32))\n",
        "        # Add the list of processed context audio features\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace (video) feature processing\n",
        "        # Prepare OpenFace features for the punchline\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        # Convert the prepared punchline video features to a float32 numpy array and add\n",
        "        of_p_list.append(np.array(prepared_punchline_of, dtype=np.float32))\n",
        "        # Process context OpenFace features\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of, dtype=np.float32))\n",
        "        # Add the list of processed context video features\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        # Add labels\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    # Return all extracted features and labels, specifying the dtype for numpy arrays\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object), np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object), np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object), np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "# Prepare data for the new dataset structure: output a list of samples, each sample is a dictionary containing all sentence features/texts\n",
        "# Among them, the features/text of the punchline will be the last item in the corresponding modality list\n",
        "def concatenate_multimodal_data_for_dataset(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    # Get the number of samples (based on the number of context sentences)\n",
        "    num_samples = len(cs)\n",
        "    # List to store all sample data\n",
        "    all_samples_data = []\n",
        "    # Iterate through each sample\n",
        "    for i in range(num_samples):\n",
        "        # Data dictionary for a single sample, containing 'audio', 'video', 'text' keys\n",
        "        sample_data = {'audio': [], 'video': [], 'text': []}\n",
        "\n",
        "        # Audio data processing\n",
        "        # Extract context audio features, ensuring they are valid numpy arrays (word count > 0, correct dimension)\n",
        "        current_sample_audio = [s for s in list(cvp_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _AUDIO_WORD_DIM_CONST]\n",
        "        # Get punchline audio features\n",
        "        punchline_audio = cvp_p[i]\n",
        "        # Append punchline audio features to the end of the list, if valid\n",
        "        if isinstance(punchline_audio, np.ndarray) and punchline_audio.ndim == 2 and punchline_audio.shape[0] > 0 and punchline_audio.shape[1] == _AUDIO_WORD_DIM_CONST:\n",
        "            current_sample_audio.append(punchline_audio)\n",
        "        # If the current audio list is empty (both context and punchline are invalid or missing), add a placeholder for the punchline (single sample, correct dimension)\n",
        "        elif not current_sample_audio:\n",
        "            current_sample_audio.append(np.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        # Store the processed audio feature list into the sample data dictionary\n",
        "        sample_data['audio'] = current_sample_audio\n",
        "\n",
        "        # Video data processing (logic same as audio)\n",
        "        current_sample_video = [s for s in list(of_c[i]) if isinstance(s, np.ndarray) and s.ndim == 2 and s.shape[0] > 0 and s.shape[1] == _VIDEO_WORD_DIM_CONST]\n",
        "        punchline_video = of_p[i]\n",
        "        if isinstance(punchline_video, np.ndarray) and punchline_video.ndim == 2 and punchline_video.shape[0] > 0 and punchline_video.shape[1] == _VIDEO_WORD_DIM_CONST:\n",
        "            current_sample_video.append(punchline_video)\n",
        "        elif not current_sample_video:\n",
        "            current_sample_video.append(np.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=np.float32))\n",
        "        sample_data['video'] = current_sample_video\n",
        "\n",
        "        # Text data processing\n",
        "        # Extract context sentence text list\n",
        "        current_sample_text = [s for s in list(cs[i]) if isinstance(s, str)]\n",
        "        # Get punchline text\n",
        "        punchline_text_str = ps[i]\n",
        "        # If the punchline text is a string, append it\n",
        "        if isinstance(punchline_text_str, str):\n",
        "            current_sample_text.append(punchline_text_str)\n",
        "        # If the current text list is empty (both context and punchline are invalid or missing), add an empty string as a punchline placeholder\n",
        "        elif not current_sample_text:\n",
        "            current_sample_text.append(\"\")\n",
        "        sample_data['text'] = current_sample_text\n",
        "\n",
        "        # Add the current sample's data dictionary to the total list\n",
        "        all_samples_data.append(sample_data)\n",
        "    # Return the list containing all sample data\n",
        "    return all_samples_data\n",
        "\n",
        "\n",
        "# --- Dataset Class: Modified for Context/Punchline Splitting ---\n",
        "class CustomFeatureDatasetContextPunchline(Dataset):\n",
        "    # Initialization function\n",
        "    def __init__(self, list_of_sample_data_dicts, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len_for_part=512,\n",
        "                 audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST):\n",
        "\n",
        "        # List of sample data dictionaries (each element is a sample, containing 'audio', 'video', 'text' keys)\n",
        "        self.list_of_sample_data_dicts = list_of_sample_data_dicts\n",
        "        # List of labels, converted to torch.long type\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long)\n",
        "        # BERT tokenizer\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        # Maximum BERT length for each part (context/punchline)\n",
        "        self.max_bert_len_for_part = max_bert_len_for_part\n",
        "        # Audio word feature dimension\n",
        "        self.audio_word_dim = audio_word_dim\n",
        "        # Video word feature dimension\n",
        "        self.video_word_dim = video_word_dim\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    # Helper function to tokenize the text part\n",
        "    def _tokenize_text_part(self, text_sentences_list):\n",
        "        # If the text list is empty\n",
        "        if not text_sentences_list:\n",
        "            # If the tokenizer has a pad token, use it, otherwise an empty string might be tokenized into special tokens\n",
        "            processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "        else:\n",
        "            # Join all sentences in the sentence list with spaces\n",
        "            processed_text = \" \".join(text_sentences_list)\n",
        "            # If it's only whitespace or empty after joining\n",
        "            if not processed_text.strip():\n",
        "                processed_text = self.tokenizer.pad_token if self.tokenizer.pad_token is not None else \"\"\n",
        "\n",
        "        # Call the tokenizer to tokenize\n",
        "        bert_inputs = self.tokenizer(\n",
        "            processed_text, add_special_tokens=True, return_attention_mask=True, # Add special tokens, return attention_mask\n",
        "            max_length=self.max_bert_len_for_part, padding='max_length', truncation=True, # Max length, pad to max length, truncate\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        )\n",
        "        # Return input_ids and attention_mask, and remove the batch dimension (because this is single sample processing)\n",
        "        return bert_inputs[\"input_ids\"].squeeze(0), bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    # Helper function to process audio/video parts\n",
        "    # all_sentences_features_for_sample: List of all sentence features for the entire sample (list of numpy arrays)\n",
        "    # part_sentences_indices: Indices in the total sentence list that the current part (context or punchline) should contain\n",
        "    # word_dim: Word feature dimension for audio or video\n",
        "    def _process_av_part(self, all_sentences_features_for_sample, part_sentences_indices, word_dim):\n",
        "        # List to store feature tensors of all sentences in this part\n",
        "        part_features_list = []\n",
        "        # If the sample itself does not have any sentence features (e.g., the entire sample is empty)\n",
        "        if not all_sentences_features_for_sample:\n",
        "            # Add a placeholder tensor (1 word, specified dimension)\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "            return part_features_list\n",
        "\n",
        "        # Iterate through the sentence indices of the specified part\n",
        "        for sent_idx in part_sentences_indices:\n",
        "            # Ensure the index is within the valid range\n",
        "            if 0 <= sent_idx < len(all_sentences_features_for_sample):\n",
        "                # Get features of a single sentence (numpy array)\n",
        "                sent_feat = all_sentences_features_for_sample[sent_idx]\n",
        "                # Validate feature validity: is a numpy array, 2D, word count > 0, correct dimension\n",
        "                if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0 and sent_feat.shape[1] == word_dim:\n",
        "                    # Convert to PyTorch tensor and add to the list\n",
        "                    part_features_list.append(torch.as_tensor(sent_feat, dtype=torch.float32))\n",
        "\n",
        "        # If this part is empty after processing (e.g., all sentences are invalid or indices are out of range, or the specified index list is empty)\n",
        "        if not part_features_list:\n",
        "            # Add a placeholder tensor for this part\n",
        "            part_features_list.append(torch.zeros((1, word_dim), dtype=torch.float32))\n",
        "        return part_features_list\n",
        "\n",
        "\n",
        "    # Method to get single sample data\n",
        "    def __getitem__(self, index):\n",
        "        # Get the sample data dictionary for the current index\n",
        "        sample_data = self.list_of_sample_data_dicts[index]\n",
        "        # Audio: list of numpy arrays (sentence features)\n",
        "        audio_all_sents_raw = sample_data['audio']\n",
        "        # Video: list of numpy arrays (sentence features)\n",
        "        video_all_sents_raw = sample_data['video']\n",
        "        # Text: list of sentence strings\n",
        "        text_all_sents_str = sample_data['text']\n",
        "        # Get label\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        # Determine the total number of sentences based on the number of text sentences\n",
        "        n_total_sents = len(text_all_sents_str)\n",
        "\n",
        "        # Prepare placeholder input_ids and attention_mask for empty text parts\n",
        "        empty_ids, empty_mask = self._tokenize_text_part([])\n",
        "\n",
        "        # Case 1: If the sample has no sentences at all (n_total_sents == 0)\n",
        "        if n_total_sents == 0:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is empty/placeholder\n",
        "            pl_audio_part = self._process_av_part([], [], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = empty_ids, empty_mask\n",
        "\n",
        "        # Case 2: If there is only one sentence, treat it as only punchline, context is empty\n",
        "        elif n_total_sents == 1:\n",
        "            # Context part is empty/placeholder\n",
        "            ctx_audio_part = self._process_av_part([], [], self.audio_word_dim) # Passing an empty index list will result in a placeholder\n",
        "            ctx_video_part = self._process_av_part([], [], self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = empty_ids, empty_mask\n",
        "            # Punchline part is this one sentence (index 0)\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, [0], self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, [0], self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[0]])\n",
        "\n",
        "        # Case 3: If there are multiple sentences, split into context and punchline\n",
        "        else:\n",
        "            # Context sentence indices: from 0 to the second to last\n",
        "            ctx_indices = list(range(n_total_sents - 1))\n",
        "            # Punchline sentence index: only the last one\n",
        "            pl_indices = [n_total_sents - 1]\n",
        "\n",
        "            # Process context part\n",
        "            ctx_audio_part = self._process_av_part(audio_all_sents_raw, ctx_indices, self.audio_word_dim)\n",
        "            ctx_video_part = self._process_av_part(video_all_sents_raw, ctx_indices, self.video_word_dim)\n",
        "            ctx_input_ids, ctx_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in ctx_indices])\n",
        "\n",
        "            # Process punchline part\n",
        "            pl_audio_part = self._process_av_part(audio_all_sents_raw, pl_indices, self.audio_word_dim)\n",
        "            pl_video_part = self._process_av_part(video_all_sents_raw, pl_indices, self.video_word_dim)\n",
        "            pl_input_ids, pl_attention_mask = self._tokenize_text_part([text_all_sents_str[i] for i in pl_indices])\n",
        "\n",
        "        # Return a tuple of context data, punchline data, and label\n",
        "        return (ctx_audio_part, ctx_video_part, ctx_input_ids, ctx_attention_mask,\n",
        "                pl_audio_part, pl_video_part, pl_input_ids, pl_attention_mask,\n",
        "                label)\n",
        "\n",
        "# --- Custom Collate Function for Context/Punchline Data ---\n",
        "def custom_collate_fn_context_punchline(batch):\n",
        "    # batch is a list where each element is the tuple returned by __getitem__\n",
        "    # Unpack batch data into respective lists\n",
        "    (ctx_audio_list, ctx_video_list, ctx_ids_list, ctx_mask_list,\n",
        "     pl_audio_list, pl_video_list, pl_ids_list, pl_mask_list,\n",
        "     labels_list) = zip(*batch)\n",
        "\n",
        "    # Directly stack text IDs, masks, and labels (they are already fixed-size tensors)\n",
        "    batched_ctx_ids = torch.stack(ctx_ids_list)\n",
        "    batched_ctx_masks = torch.stack(ctx_mask_list)\n",
        "    batched_pl_ids = torch.stack(pl_ids_list)\n",
        "    batched_pl_masks = torch.stack(pl_mask_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Helper function to process a list of audio/video data for a part (e.g., context audio)\n",
        "    # part_data_list: A list of samples, where each sample is a list of sentence tensors\n",
        "    # word_dim_const: Word feature dimension of this modality\n",
        "    def _collate_av_part(part_data_list, word_dim_const):\n",
        "        # Get the number of sentences in each sample\n",
        "        sample_lengths = [len(sample) for sample in part_data_list]\n",
        "        # Maximum number of sentences in the batch, 0 if empty\n",
        "        max_sents = max(sample_lengths) if sample_lengths else 0\n",
        "\n",
        "        # Get the word count of each sentence and find the maximum word count\n",
        "        sentence_word_counts_flat = []\n",
        "        for sample in part_data_list: # Iterate through each sample\n",
        "            for sentence_tensor in sample: # Iterate through each sentence tensor in the sample\n",
        "                sentence_word_counts_flat.append(sentence_tensor.shape[0]) # Add the word count of this sentence\n",
        "        # Maximum number of words in the batch, 0 if empty\n",
        "        max_words = max(sentence_word_counts_flat) if sentence_word_counts_flat else 0\n",
        "\n",
        "        # Ensure max_words and max_sents are at least 1 to avoid zero dimensions in tensors\n",
        "        max_words = max(1, max_words)\n",
        "        max_sents = max(1, max_sents)\n",
        "\n",
        "        # Create padded feature tensor and length tensor\n",
        "        # padded_features: (batch_size, max_sentences, max_words, feature_dimension)\n",
        "        # sentence_lengths_tensor: (batch_size, max_sentences) - records the actual word count of each sentence\n",
        "        padded_features = torch.zeros(len(part_data_list), max_sents, max_words, word_dim_const)\n",
        "        sentence_lengths_tensor = torch.zeros(len(part_data_list), max_sents, dtype=torch.long)\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, sample in enumerate(part_data_list):\n",
        "            # Iterate through each sentence tensor in the sample\n",
        "            for j, sentence_tensor in enumerate(sample):\n",
        "                # Word count of the current sentence\n",
        "                num_words = sentence_tensor.shape[0]\n",
        "                # Pad only if there are words\n",
        "                if num_words > 0:\n",
        "                    # Pad features into the padded_features tensor\n",
        "                    padded_features[i, j, :num_words, :] = sentence_tensor\n",
        "                    # Record the actual word count into the sentence_lengths_tensor tensor\n",
        "                    sentence_lengths_tensor[i, j] = num_words\n",
        "        # Return padded features, list of sentence counts per sample (as tensor), and word counts per sentence tensor\n",
        "        return padded_features, torch.tensor(sample_lengths, dtype=torch.long), sentence_lengths_tensor\n",
        "\n",
        "    # Process audio and video data for context and punchline separately\n",
        "    # ctx_padded_audio: (B, S_ctx_max, W_ctx_max, D_audio)\n",
        "    # ctx_audio_sl: (B,) - Actual number of sentences per sample for context\n",
        "    # ctx_audio_ssl: (B, S_ctx_max) - Actual word count of each sentence per sample for context\n",
        "    ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl = _collate_av_part(ctx_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    ctx_padded_video, ctx_video_sl, ctx_video_ssl = _collate_av_part(ctx_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "    pl_padded_audio, pl_audio_sl, pl_audio_ssl = _collate_av_part(pl_audio_list, _AUDIO_WORD_DIM_CONST)\n",
        "    pl_padded_video, pl_video_sl, pl_video_ssl = _collate_av_part(pl_video_list, _VIDEO_WORD_DIM_CONST)\n",
        "\n",
        "    # Return all processed batch data\n",
        "    return (ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl, # Context audio (features, sample sentence count, words per sentence)\n",
        "            ctx_padded_video, ctx_video_sl, ctx_video_ssl, # Context video\n",
        "            batched_ctx_ids, batched_ctx_masks,             # Context text\n",
        "            pl_padded_audio, pl_audio_sl, pl_audio_ssl,     # Punchline audio\n",
        "            pl_padded_video, pl_video_sl, pl_video_ssl,     # Punchline video\n",
        "            batched_pl_ids, batched_pl_masks,               # Punchline text\n",
        "            batched_labels)                                 # Labels\n",
        "\n",
        "\n",
        "# --- Hierarchical LSTM Aggregator ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Hidden dimension of sentence-level LSTM\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        # Hidden dimension of sample-level LSTM\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        # Sentence-level LSTM: input word embeddings, output sentence representation\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed, output dimension will become 2*hidden_dim\n",
        "\n",
        "        # If sentence LSTM is bidirectional, the input dimension of sample LSTM needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        # Sample-level LSTM: input sentence representations, output sample representation\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Forward propagation function\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sentences, max_words, word_dimension)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sentences) - actual word count per sentence\n",
        "\n",
        "        # Get the shape of the feature tensor\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "        # Final output dimension of sample LSTM (considering bidirectional case)\n",
        "        final_output_dim_sample = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "\n",
        "        # Handle the extreme case where all inputs in the batch are empty\n",
        "        # If max_sentences or max_words is 0, or batch_size is 0, or all sample_lengths are 0\n",
        "        if max_sents == 0 or max_words == 0 or batch_size == 0 or torch.all(sample_lengths == 0):\n",
        "            # Return a zero tensor with shape (batch_size, final_output_dim_sample)\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sentence dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        # If all sentences are empty (all lengths are 0)\n",
        "        if not torch.any(valid_sents_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sentence features and corresponding lengths\n",
        "        sents_features_packed_data = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed_data = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack padded sequence (length tensor needs to be moved to CPU for packing)\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed_data, sents_word_lengths_packed_data.cpu(),\n",
        "                                                batch_first=True, enforce_sorted=False)\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers*num_directions, B*S_valid, sentence_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get the actual output dimension of sentence LSTM (considering bidirectional)\n",
        "        sent_hidden_dim_actual = self.sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "        # Get the hidden state of the last time step (for unidirectional LSTM, take the last layer; for bidirectional, concatenate the last time steps of the last two layers)\n",
        "        # Output shape: (B*S_valid, sentence_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "        # Apply dropout to sentence embeddings\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Put valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Create a zero tensor with shape (B*S, actual_sentence_hidden_dim)\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        # Fill valid sentence embeddings into corresponding positions\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent), reshape to sample LSTM input format\n",
        "        sample_features_for_sample_lstm = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack padded sequence (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0 (i.e., samples with actual sentence count of 0)\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        # If all samples are empty (actual sentence counts are all 0)\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            # Return a zero tensor matching the shape of the sample LSTM output\n",
        "            return torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "\n",
        "        # Get valid sample features and corresponding lengths\n",
        "        sample_features_packed_input_data = sample_features_for_sample_lstm[valid_sample_indices]\n",
        "        sample_lengths_packed_data = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input_data, sample_lengths_packed_data.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers*num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get the hidden state of the last time step\n",
        "        # Output shape: (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            # Concatenate the forward and backward hidden states of the last time step of the bidirectional LSTM\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Unidirectional LSTM, take the hidden state of the last time step of the last layer\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "        # Apply dropout to sample embeddings\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Put valid sample embeddings back to their original positions, use zero vectors for empty samples\n",
        "        # Create a zero tensor with shape (B, final_output_dim_sample_lstm)\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim_sample, device=features.device)\n",
        "        # Fill valid sample embeddings into corresponding positions\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "        # Return final sample embeddings\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "# --- GLU Linear Layer ---\n",
        "class GLULinear(nn.Module):\n",
        "    # Initialization function, input dimension and output dimension\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GLULinear, self).__init__()\n",
        "        # The first linear layer is followed by a GELU activation function\n",
        "        self.layer1 = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU())\n",
        "        # The second linear layer\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    # Forward propagation function\n",
        "    def forward(self, x):\n",
        "        # Element-wise multiplication of the outputs of the two linear layers\n",
        "        return self.layer1(x) * self.layer2(x)\n",
        "\n",
        "# --- Advanced Cross-Attention/Self-Attention Module ---\n",
        "class MultiHeadAttentionModule(nn.Module):\n",
        "    # Initialization function\n",
        "    # dim: feature dimension, num_heads: number of attention heads\n",
        "    def __init__(self, dim, num_heads=1):\n",
        "        super(MultiHeadAttentionModule, self).__init__()\n",
        "        # Feature dimension\n",
        "        self.dim = dim\n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.head_dim = dim // num_heads\n",
        "        # Ensure dimension is divisible by the number of heads\n",
        "        if self.head_dim * num_heads != self.dim:\n",
        "            raise ValueError(\"dim must be divisible by num_heads\")\n",
        "\n",
        "        # Linear layer to generate Key\n",
        "        self.K_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Value\n",
        "        self.V_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Linear layer to generate Query\n",
        "        self.Q_layer = nn.Linear(dim, dim, bias=False)\n",
        "        # Softmax layer, used to calculate attention weights\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        # Fully connected layer before output\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "    # Forward propagation function\n",
        "    # feat1_query is Query, feat2_key_value is Key and Value\n",
        "    # mask: optional attention mask\n",
        "    def forward(self, feat1_query, feat2_key_value, mask=None):\n",
        "        # Query shape: (batch_size, Query_sequence_length, Query_dimension)\n",
        "        B_q, N_q, C_q = feat1_query.shape\n",
        "        # Key/Value shape: (batch_size, Key/Value_sequence_length, Key/Value_dimension)\n",
        "        B_kv, N_kv, C_kv = feat2_key_value.shape\n",
        "\n",
        "        # Check if batch sizes of Query and Key/Value match\n",
        "        if B_q != B_kv: raise ValueError(f\"Batch sizes do not match: Query is {B_q}, Key/Value is {B_kv}\")\n",
        "\n",
        "        # Generate Q, K, V and adjust shape for multi-head: (batch, num_heads, sequence_length, head_dimension)\n",
        "        # Q: (B, N_q, C_q) -> (B, N_q, num_heads, head_dim) -> (B, num_heads, N_q, head_dim)\n",
        "        Q = self.Q_layer(feat1_query).reshape(B_q, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        K = self.K_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: (B, N_kv, C_kv) -> (B, N_kv, num_heads, head_dim) -> (B, num_heads, N_kv, head_dim)\n",
        "        V = self.V_layer(feat2_key_value).reshape(B_kv, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate dot product of Q and K_transpose and scale ( scaled_dot_product = (Q @ K.T) / sqrt(head_dim) )\n",
        "        # dots shape: (B, num_heads, N_q, N_kv)\n",
        "        dots = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # If a mask is provided (usually a padding mask for K,V sequence, shape B, N_kv)\n",
        "        if mask is not None:\n",
        "            # unsqueeze expands the mask to (B, 1, 1, N_kv) to match the shape of dots (B, nH, N_q, N_kv) for broadcasting\n",
        "            # Fill positions in dots where mask is 0 (i.e., padding positions) with a very small value, so their weight approaches 0 after softmax\n",
        "            dots = dots.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "\n",
        "        # Calculate attention weights (attn_weights shape: B, num_heads, N_q, N_kv)\n",
        "        attn_weights = self.attend(dots)\n",
        "        # Attention weights weighted V (out shape: B, num_heads, N_q, head_dim)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # Permute and merge multi-head results: (B, num_heads, N_q, head_dim) -> (B, N_q, num_heads, head_dim) -> (B, N_q, dim)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B_q, N_q, self.dim)\n",
        "        # Pass through output fully connected layer\n",
        "        out = self.fc_out(out)\n",
        "        # Return final output\n",
        "        return out\n",
        "\n",
        "# --- Adapted Single Stream Processor ---\n",
        "class AdaptedSingleStreamProcessor(nn.Module):\n",
        "    # Initialization function\n",
        "    # audio_video_input_dim: Input dimension after audio/video projection\n",
        "    # bert_hidden_size: BERT's hidden layer size\n",
        "    # max_bert_len_for_lstm: Maximum input sequence length expected by the internal text LSTM\n",
        "    # lstm_hidden_size: Hidden size of the internal text LSTM\n",
        "    # attention_token_dim: Dimension of attention tokens\n",
        "    # num_attention_tokens_per_modal: Number of tokens output after processing each modality\n",
        "    # active_modalities: Tuple of active modalities, e.g., ('audio', 'video', 'text')\n",
        "    # num_ca_sa_heads: Number of heads for cross-attention and self-attention modules\n",
        "    # dropout_rate: Dropout rate for the text FC part\n",
        "    def __init__(self, audio_video_input_dim, bert_hidden_size, max_bert_len_for_lstm,\n",
        "                 lstm_hidden_size, attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 active_modalities=('audio', 'video', 'text'), num_ca_sa_heads=1, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        # Number of tokens output after processing each modality\n",
        "        self.n_tokens_per_modal = num_attention_tokens_per_modal\n",
        "        # Dimension of attention tokens\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        # Maximum input sequence length expected by the internal text LSTM\n",
        "        self.max_bert_len_for_lstm_input = max_bert_len_for_lstm\n",
        "        # Active modalities\n",
        "        self.active_modalities = active_modalities\n",
        "        # Flattened feature dimension output by each modality processor (N * C)\n",
        "        self.expected_feature_dim_after_mod_proc = self.n_tokens_per_modal * self.attention_token_dim\n",
        "\n",
        "        # Audio feature processor: receives projected features, maps to NxC token representation\n",
        "        self.audio_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc) # Layer normalization\n",
        "        )\n",
        "        # Video feature processor: logic same as audio\n",
        "        self.vision_feat_processor_to_tokens = nn.Sequential(\n",
        "            GLULinear(audio_video_input_dim, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "        # Text processing: BERT hidden state -> LSTM -> Fully connected layer -> NxC token representation\n",
        "        # Text LSTM processor\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        # Text FC processor, maps LSTM output to token representation\n",
        "        self.text_fc_processor_to_tokens = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), # Dropout layer\n",
        "            # LSTM output is (B, S_lstm, H_lstm), after reshape it's (B, S_lstm * H_lstm)\n",
        "            GLULinear(lstm_hidden_size * self.max_bert_len_for_lstm_input, 1024),\n",
        "            GLULinear(1024, self.expected_feature_dim_after_mod_proc),\n",
        "            nn.LayerNorm(self.expected_feature_dim_after_mod_proc)\n",
        "        )\n",
        "\n",
        "        # Attention module instantiation\n",
        "        # ZA: Audio cross-attention (query is concatenation of all modalities, key/value are audio tokens)\n",
        "        self.ZA = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZV: Video cross-attention\n",
        "        self.ZV = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # ZT: Text cross-attention\n",
        "        self.ZT = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # SA_stream: Intra-stream self-attention\n",
        "        self.SA_stream = MultiHeadAttentionModule(dim=attention_token_dim, num_heads=num_ca_sa_heads)\n",
        "        # Final output dimension of this stream processor (after averaging SA output, or, dimension of a single token)\n",
        "        self.output_final_dim = attention_token_dim\n",
        "\n",
        "    # Forward propagation function\n",
        "    # audio_input_proj, vision_input_proj from Hierarchical LSTM + Projector layer (B, D_projector)\n",
        "    # text_sequence_input_bert is BERT's hidden state (B, S_bert, D_bert)\n",
        "    def forward(self, audio_input_proj, vision_input_proj, text_sequence_input_bert):\n",
        "        # Dynamically determine batch size\n",
        "        b = 0\n",
        "        if audio_input_proj is not None and audio_input_proj.nelement() > 0: b = audio_input_proj.shape[0]\n",
        "        elif vision_input_proj is not None and vision_input_proj.nelement() > 0: b = vision_input_proj.shape[0]\n",
        "        elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: b = text_sequence_input_bert.shape[0]\n",
        "\n",
        "        # Handle empty batch (all inputs are empty or None)\n",
        "        if b == 0:\n",
        "            dev = torch.device(\"cpu\") # Default device\n",
        "            # Try to get device from valid input\n",
        "            if audio_input_proj is not None and audio_input_proj.nelement() > 0: dev = audio_input_proj.device\n",
        "            elif vision_input_proj is not None and vision_input_proj.nelement() > 0: dev = vision_input_proj.device\n",
        "            elif text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0: dev = text_sequence_input_bert.device\n",
        "\n",
        "            # Create empty flat features and stream output\n",
        "            empty_flat = torch.zeros(0, self.expected_feature_dim_after_mod_proc, device=dev)\n",
        "            empty_stream_out = torch.zeros(0, 1, self.output_final_dim, device=dev)\n",
        "            # Return empty flat features for contrastive loss and empty stream output\n",
        "            return empty_flat, empty_flat, empty_flat, empty_stream_out\n",
        "\n",
        "        # Get current device (ensure at least one valid input to determine device)\n",
        "        device = audio_input_proj.device if audio_input_proj is not None and audio_input_proj.nelement() > 0 else \\\n",
        "                 (vision_input_proj.device if vision_input_proj is not None and vision_input_proj.nelement() > 0 else \\\n",
        "                  text_sequence_input_bert.device)\n",
        "\n",
        "        # Initialize flat features for contrastive loss (audio_f_flat) and token features for attention (audio_f_tokens)\n",
        "        # Audio processing\n",
        "        audio_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        audio_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        # If audio modality is active, input is not empty, and input is not all zeros (indicates actual content)\n",
        "        if 'audio' in self.active_modalities and audio_input_proj is not None and audio_input_proj.nelement() > 0 and audio_input_proj.abs().sum() > 1e-9 :\n",
        "            audio_f_flat = self.audio_feat_processor_to_tokens(audio_input_proj) # (B, N*C)\n",
        "            audio_f_tokens = audio_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Video processing (logic same as audio)\n",
        "        vis_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        vis_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'video' in self.active_modalities and vision_input_proj is not None and vision_input_proj.nelement() > 0 and vision_input_proj.abs().sum() > 1e-9:\n",
        "            vis_f_flat = self.vision_feat_processor_to_tokens(vision_input_proj)\n",
        "            vis_f_tokens = vis_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim)\n",
        "\n",
        "        # Text processing\n",
        "        text_f_flat = torch.zeros(b, self.expected_feature_dim_after_mod_proc, device=device)\n",
        "        text_f_tokens = torch.zeros(b, self.n_tokens_per_modal, self.attention_token_dim, device=device)\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert is not None and text_sequence_input_bert.nelement() > 0 and text_sequence_input_bert.abs().sum() > 1e-9:\n",
        "            # Get current BERT output sequence length\n",
        "            current_bert_seq_len = text_sequence_input_bert.shape[1]\n",
        "            text_sequence_input_bert_adjusted = text_sequence_input_bert\n",
        "            # Adjust BERT output sequence length to match LSTM expected input\n",
        "            if current_bert_seq_len != self.max_bert_len_for_lstm_input:\n",
        "                if current_bert_seq_len > self.max_bert_len_for_lstm_input: # Truncate if too long\n",
        "                    text_sequence_input_bert_adjusted = text_sequence_input_bert[:, :self.max_bert_len_for_lstm_input, :]\n",
        "                else: # Pad with zeros if too short\n",
        "                    padding_needed = self.max_bert_len_for_lstm_input - current_bert_seq_len\n",
        "                    # Create padding tensor (B, padding_needed, D_bert)\n",
        "                    padding_tensor = torch.zeros(b, padding_needed, text_sequence_input_bert.shape[2], device=device)\n",
        "                    # Concatenate original BERT output and padding tensor\n",
        "                    text_sequence_input_bert_adjusted = torch.cat([text_sequence_input_bert, padding_tensor], dim=1)\n",
        "\n",
        "            # Pass through text LSTM\n",
        "            lstm_output, _ = self.text_lstm_processor(text_sequence_input_bert_adjusted) # (B, S_lstm, H_lstm)\n",
        "            # Flatten LSTM output: (B, S_lstm * H_lstm)\n",
        "            text_f_flat_from_lstm = lstm_output.reshape(b, -1)\n",
        "            # Process flattened LSTM output through FC layer\n",
        "            text_f_flat = self.text_fc_processor_to_tokens(text_f_flat_from_lstm) # (B, N*C)\n",
        "            # Reshape to token form\n",
        "            text_f_tokens = text_f_flat.view(b, self.n_tokens_per_modal, self.attention_token_dim) # (B, N, C)\n",
        "\n",
        "        # Collect tokens from active modalities with content\n",
        "        active_mod_token_lists = []\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(audio_f_tokens)\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(vis_f_tokens)\n",
        "        if 'text'  in self.active_modalities and text_f_tokens.abs().sum() > 1e-9: active_mod_token_lists.append(text_f_tokens)\n",
        "\n",
        "        # If there are no active modalities with content\n",
        "        if not active_mod_token_lists:\n",
        "            # Return flat features and zero stream output (because there's no content for attention calculation)\n",
        "            return audio_f_flat, vis_f_flat, text_f_flat, torch.zeros(b, 1, self.output_final_dim, device=device)\n",
        "\n",
        "        # Concatenate tokens of active modalities as Query for cross-attention\n",
        "        # query_for_modality_ca shape: (B, num_active_modalities * N, C_token)\n",
        "        query_for_modality_ca = torch.cat(active_mod_token_lists, dim=1)\n",
        "\n",
        "        # Perform inter-modality cross-attention\n",
        "        # Initialize result tensor\n",
        "        res_za, res_zv, res_zt = torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca), torch.zeros_like(query_for_modality_ca)\n",
        "        # If audio is active and has content\n",
        "        if 'audio' in self.active_modalities and audio_f_tokens.abs().sum() > 1e-9:\n",
        "            # query_for_modality_ca as Query, audio_f_tokens as Key and Value\n",
        "            res_za = self.ZA(query_for_modality_ca, audio_f_tokens)\n",
        "        # If video is active and has content\n",
        "        if 'video' in self.active_modalities and vis_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zv = self.ZV(query_for_modality_ca, vis_f_tokens)\n",
        "        # If text is active and has content\n",
        "        if 'text' in self.active_modalities and text_f_tokens.abs().sum() > 1e-9:\n",
        "            res_zt = self.ZT(query_for_modality_ca, text_f_tokens)\n",
        "\n",
        "        # Merge cross-attention results (element-wise addition)\n",
        "        feat_after_mod_ca = res_za + res_zv + res_zt\n",
        "        # Intra-stream self-attention, with residual connection\n",
        "        # feat_after_mod_ca as Query, Key, and Value\n",
        "        feat_after_sa_stream = self.SA_stream(feat_after_mod_ca, feat_after_mod_ca) + feat_after_mod_ca\n",
        "        # Average the features after self-attention along the sequence dimension to get the final stream representation\n",
        "        stream_output_representation = torch.mean(feat_after_sa_stream, dim=1) # (B, C_token)\n",
        "\n",
        "        # Return flat features for contrastive loss, and the final stream output representation (add a dimension to match the expected (B, 1, C_token) shape)\n",
        "        return audio_f_flat, vis_f_flat, text_f_flat, stream_output_representation.unsqueeze(1)\n",
        "\n",
        "\n",
        "# --- Main Model: ContextPunchlineHumorModelNew ---\n",
        "class ContextPunchlineHumorModelNew(nn.Module):\n",
        "    # Initialization function\n",
        "    def __init__(self,\n",
        "                 bert_model_name_or_path,\n",
        "                 audio_word_dim, video_word_dim,\n",
        "                 sentence_lstm_hidden_dim, sample_lstm_hidden_dim, hier_lstm_dropout,\n",
        "                 projector_output_dim,\n",
        "                 bert_hidden_size_actual, max_bert_len_for_lstm,\n",
        "                 text_lstm_hidden_size_in_stream,\n",
        "                 attention_token_dim, num_attention_tokens_per_modal,\n",
        "                 stream_ca_sa_heads, stream_dropout_rate,\n",
        "                 final_cross_attention_heads, # MODIFIED: This will now be used for the new final fusion heads\n",
        "                 mlp_hidden_dim, num_classes,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(bert_model_name_or_path)\n",
        "\n",
        "        self.ctx_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.ctx_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.ctx_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.context_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        self.pl_audio_hier_lstm = HierarchicalLSTMAggregator(audio_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_video_hier_lstm = HierarchicalLSTMAggregator(video_word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim, dropout_rate=hier_lstm_dropout)\n",
        "        self.pl_audio_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.pl_video_projector = nn.Linear(sample_lstm_hidden_dim, projector_output_dim)\n",
        "        self.punchline_processor = AdaptedSingleStreamProcessor(\n",
        "            audio_video_input_dim=projector_output_dim,\n",
        "            bert_hidden_size=bert_hidden_size_actual,\n",
        "            max_bert_len_for_lstm=max_bert_len_for_lstm,\n",
        "            lstm_hidden_size=text_lstm_hidden_size_in_stream,\n",
        "            attention_token_dim=attention_token_dim,\n",
        "            num_attention_tokens_per_modal=num_attention_tokens_per_modal,\n",
        "            active_modalities=('audio', 'video', 'text'),\n",
        "            num_ca_sa_heads=stream_ca_sa_heads,\n",
        "            dropout_rate=stream_dropout_rate\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Final Fusion ---\n",
        "        self.final_fusion_input_dim = attention_token_dim\n",
        "\n",
        "        self.final_ca_query_streams_on_ctx_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_ca_query_streams_on_pl_kv = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        self.final_fusion_sa_after_ca_sum = MultiHeadAttentionModule(\n",
        "            dim=self.final_fusion_input_dim,\n",
        "            num_heads=final_cross_attention_heads\n",
        "        )\n",
        "        # REMOVED: self.cross_attention_final\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.final_fusion_input_dim, mlp_hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(mlp_hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self,\n",
        "                ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl,\n",
        "                ctx_padded_video, ctx_video_sl, ctx_video_ssl,\n",
        "                ctx_input_ids, ctx_attention_mask,\n",
        "                pl_padded_audio, pl_audio_sl, pl_audio_ssl,\n",
        "                pl_padded_video, pl_video_sl, pl_video_ssl,\n",
        "                pl_input_ids, pl_attention_mask,\n",
        "                current_modality_config=None, tokenizer_for_padding=None\n",
        "                ):\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_a = self.ctx_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_a_vec = torch.zeros(ctx_padded_audio.shape[0], actual_hier_lstm_output_dim_ctx_a, device=ctx_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(ctx_audio_sl > 0):\n",
        "                ctx_a_vec = self.ctx_audio_hier_lstm(ctx_padded_audio, ctx_audio_sl, ctx_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_ctx_v = self.ctx_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.ctx_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        ctx_v_vec = torch.zeros(ctx_padded_video.shape[0], actual_hier_lstm_output_dim_ctx_v, device=ctx_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(ctx_video_sl > 0):\n",
        "                ctx_v_vec = self.ctx_video_hier_lstm(ctx_padded_video, ctx_video_sl, ctx_video_ssl)\n",
        "\n",
        "        ctx_a_proj = self.ctx_audio_projector(ctx_a_vec)\n",
        "        ctx_v_proj = self.ctx_video_projector(ctx_v_vec)\n",
        "\n",
        "        ctx_bert_hs = torch.zeros(ctx_input_ids.shape[0], ctx_input_ids.shape[1], self.bert_model.config.hidden_size, device=ctx_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(ctx_attention_mask.sum(dim=1) > 0):\n",
        "                ctx_bert_outputs = self.bert_model(input_ids=ctx_input_ids, attention_mask=ctx_attention_mask)\n",
        "                ctx_bert_hs = ctx_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        ctx_audio_f_flat, ctx_vis_f_flat, ctx_text_f_flat, ctx_stream_repr = self.context_processor(ctx_a_proj, ctx_v_proj, ctx_bert_hs)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_a = self.pl_audio_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_audio_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_a_vec = torch.zeros(pl_padded_audio.shape[0], actual_hier_lstm_output_dim_pl_a, device=pl_padded_audio.device)\n",
        "        if current_modality_config is None or current_modality_config.get('audio', True):\n",
        "            if torch.any(pl_audio_sl > 0):\n",
        "                pl_a_vec = self.pl_audio_hier_lstm(pl_padded_audio, pl_audio_sl, pl_audio_ssl)\n",
        "\n",
        "        actual_hier_lstm_output_dim_pl_v = self.pl_video_hier_lstm.sample_lstm_hidden_dim * (2 if self.pl_video_hier_lstm.sample_lstm.bidirectional else 1)\n",
        "        pl_v_vec = torch.zeros(pl_padded_video.shape[0], actual_hier_lstm_output_dim_pl_v, device=pl_padded_video.device)\n",
        "        if current_modality_config is None or current_modality_config.get('video', True):\n",
        "            if torch.any(pl_video_sl > 0):\n",
        "                pl_v_vec = self.pl_video_hier_lstm(pl_padded_video, pl_video_sl, pl_video_ssl)\n",
        "\n",
        "        pl_a_proj = self.pl_audio_projector(pl_a_vec)\n",
        "        pl_v_proj = self.pl_video_projector(pl_v_vec)\n",
        "\n",
        "        pl_bert_hs = torch.zeros(pl_input_ids.shape[0], pl_input_ids.shape[1], self.bert_model.config.hidden_size, device=pl_input_ids.device)\n",
        "        if current_modality_config is None or current_modality_config.get('text', True):\n",
        "            if torch.any(pl_attention_mask.sum(dim=1) > 0):\n",
        "                pl_bert_outputs = self.bert_model(input_ids=pl_input_ids, attention_mask=pl_attention_mask)\n",
        "                pl_bert_hs = pl_bert_outputs.last_hidden_state.to(torch.float32)\n",
        "        pl_audio_f_flat, pl_vis_f_flat, pl_text_f_flat, pl_stream_repr = self.punchline_processor(pl_a_proj, pl_v_proj, pl_bert_hs)\n",
        "\n",
        "        # --- MODIFIED: New final fusion logic ---\n",
        "        streams_query = torch.cat((ctx_stream_repr, pl_stream_repr), dim=1)\n",
        "        res_ca_ctx = self.final_ca_query_streams_on_ctx_kv(streams_query, ctx_stream_repr)\n",
        "        res_ca_pl = self.final_ca_query_streams_on_pl_kv(streams_query, pl_stream_repr)\n",
        "        fused_after_ca = res_ca_ctx + res_ca_pl\n",
        "        fused_after_sa = self.final_fusion_sa_after_ca_sum(fused_after_ca, fused_after_ca)\n",
        "        fused_after_sa = fused_after_sa + fused_after_ca # Residual connection for the self-attention on fused representations\n",
        "        fused_representation = torch.mean(fused_after_sa, dim=1)\n",
        "\n",
        "        mlp_out = self.mlp(fused_representation)\n",
        "        logits = self.classifier(mlp_out)\n",
        "\n",
        "        contrastive_features = {\n",
        "            'ctx_audio': ctx_audio_f_flat, 'ctx_video': ctx_vis_f_flat, 'ctx_text': ctx_text_f_flat,\n",
        "            'pl_audio': pl_audio_f_flat, 'pl_video': pl_vis_f_flat, 'pl_text': pl_text_f_flat\n",
        "        }\n",
        "        return logits, contrastive_features\n",
        "\n",
        "\n",
        "# --- Contrastive Loss Function ---\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    # Initialization function, temperature coefficient\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        # Use CrossEntropyLoss to calculate loss (SimCLR style)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Forward propagation function\n",
        "    # emb_i, emb_j are embeddings from different modalities or views (B, D)\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        # Get batch size\n",
        "        batch_size = emb_i.shape[0]\n",
        "        # Contrastive loss requires at least 2 samples to compute, otherwise return 0 loss\n",
        "        if batch_size <= 1:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # Check if embeddings are all zeros, if so, loss is 0 (to avoid NaN)\n",
        "        # If the sum of absolute values of all elements in either embedding tensor is less than a very small value, it is considered empty or all zeros\n",
        "        if emb_i.abs().sum() < 1e-9 or emb_j.abs().sum() < 1e-9:\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "\n",
        "        # L2 normalize embedding vectors\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "\n",
        "        # Concatenate the two groups of normalized embeddings along the batch dimension: (2*B, D)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Calculate similarity matrix (cosine similarity between all sample pairs, then divide by temperature)\n",
        "        # (2*B, D) @ (D, 2*B) -> (2*B, 2*B)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "\n",
        "        # Create labels: for each sample in z_i, its positive sample is the corresponding sample in z_j\n",
        "        # For example, row similarity_matrix[0] is the similarity of z_i[0] with all representations\n",
        "        # Its positive sample z_j[0] has index batch_size + 0 in representations\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        # For each sample in z_j, its positive sample is the corresponding sample in z_i\n",
        "        # For example, row similarity_matrix[batch_size+0] is the similarity of z_j[0] with all representations\n",
        "        # Its positive sample z_i[0] has index 0 in representations\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Calculate loss, separately for z_i querying z_j and z_j querying z_i\n",
        "        # loss_i: z_i as anchor, corresponding sample in z_j as positive\n",
        "        # similarity_matrix[:batch_size] is the similarity of z_i with all representations (B, 2*B)\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        # loss_j: z_j as anchor, corresponding sample in z_i as positive\n",
        "        # similarity_matrix[batch_size:] is the similarity of z_j with all representations (B, 2*B)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        # Return average loss\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "\n",
        "# --- Modified Training Function (Only contrastive loss calculation method is changed) ---\n",
        "def train_new_model(model, data_loader, optimizer, scheduler,\n",
        "                    bce_criterion, contrastive_loss_fn, device, epoch, num_epochs,\n",
        "                    contrastive_loss_weight, current_modality_config, tokenizer_for_padding):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # The BERT part of the model is already globally frozen externally, no explicit model.bert_model.eval() needed here\n",
        "\n",
        "    # Initialize total BCE loss, total contrastive loss, total loss\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0 # Used to accumulate final_simclr_loss_for_batch for each batch\n",
        "    total_loss = 0\n",
        "    # Create tqdm progress bar to display training progress\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    # Iterate through each batch in the data loader\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack data from collate_fn\n",
        "        (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "         pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "         labels) = batch\n",
        "\n",
        "        # Get current batch size\n",
        "        current_batch_size = ctx_a_feat.shape[0]\n",
        "        # If batch is empty, skip\n",
        "        if current_batch_size == 0: continue\n",
        "\n",
        "        # Move data to the specified device (excluding the last label)\n",
        "        batch_data_on_device = []\n",
        "        for tensor_item in batch[:-1]:\n",
        "            batch_data_on_device.append(tensor_item.to(device))\n",
        "        # Move labels to device and convert to long type\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        # Clear optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model forward pass, get classification logits and contrastive features\n",
        "        logits, contrastive_feats = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "        # Calculate BCE classification loss\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # --- Contrastive Loss Calculation (modified to averaging method) ---\n",
        "        final_simclr_loss_for_batch = torch.tensor(0.0, device=device) # Initialize contrastive loss for this batch\n",
        "        if current_batch_size > 1 and contrastive_loss_weight > 0:\n",
        "            accumulated_contrastive_loss_components = []\n",
        "\n",
        "            # Contrastive loss for the context stream\n",
        "            ctx_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_video'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_audio'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_audio'], contrastive_feats['ctx_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['ctx_video'].nelement() > 0 and contrastive_feats['ctx_text'].nelement() > 0:\n",
        "                    ctx_individual_losses.append(contrastive_loss_fn(contrastive_feats['ctx_video'], contrastive_feats['ctx_text']))\n",
        "\n",
        "            if ctx_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(ctx_individual_losses)))\n",
        "\n",
        "            # Contrastive loss for the punchline stream\n",
        "            pl_individual_losses = []\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('video', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_video'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_video']))\n",
        "            if current_modality_config.get('audio', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_audio'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_audio'], contrastive_feats['pl_text']))\n",
        "            if current_modality_config.get('video', True) and current_modality_config.get('text', True):\n",
        "                if contrastive_feats['pl_video'].nelement() > 0 and contrastive_feats['pl_text'].nelement() > 0:\n",
        "                    pl_individual_losses.append(contrastive_loss_fn(contrastive_feats['pl_video'], contrastive_feats['pl_text']))\n",
        "\n",
        "            if pl_individual_losses: # Calculate average only if the list is not empty\n",
        "                accumulated_contrastive_loss_components.append(torch.mean(torch.stack(pl_individual_losses)))\n",
        "\n",
        "            # Calculate the final contrastive loss (if multiple components exist, take their average)\n",
        "            if accumulated_contrastive_loss_components:\n",
        "                final_simclr_loss_for_batch = torch.mean(torch.stack(accumulated_contrastive_loss_components))\n",
        "            # else: final_simclr_loss_for_batch remains its initial value of 0.0\n",
        "\n",
        "        # Total loss = BCE loss + contrastive_loss_weight * calculated batch contrastive loss\n",
        "        current_loss = bce_loss + contrastive_loss_weight * final_simclr_loss_for_batch\n",
        "\n",
        "        # Backpropagate to calculate gradients\n",
        "        current_loss.backward()\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        # If a learning rate scheduler is used\n",
        "        if scheduler is not None:\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss values (item() gets scalar value)\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += final_simclr_loss_for_batch.item() # Accumulate the calculated batch contrastive loss\n",
        "        total_loss += current_loss.item()\n",
        "        # Update progress bar display information\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{final_simclr_loss_for_batch.item():.4f}\")\n",
        "\n",
        "    # If data loader is not empty\n",
        "    if len(data_loader) > 0:\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        # Print average training loss for the current epoch\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "\n",
        "\n",
        "# --- Validation/Test Function (Added F1 Score) ---\n",
        "def validate_or_test_new_model(model, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                               current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize total BCE loss\n",
        "    total_bce_loss = 0\n",
        "    # List to store all prediction results\n",
        "    all_preds = []\n",
        "    # List to store all true labels\n",
        "    all_labels = []\n",
        "\n",
        "    # Set progress bar description (corrected logic)\n",
        "    if mode == \"Test\" and epoch is None:\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # and epoch is not None (implicitly for this branch after the first)\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Val\": # epoch should not be None for validation\n",
        "        desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    else: # Fallback, though ideally all cases are covered\n",
        "        desc = f\"Processing [{mode} {current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    # Do not calculate gradients within this block to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through each batch in the data loader\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            # Unpack data\n",
        "            (ctx_a_feat, ctx_a_sl, ctx_a_ssl, ctx_v_feat, ctx_v_sl, ctx_v_ssl, ctx_ids, ctx_mask,\n",
        "             pl_a_feat, pl_a_sl, pl_a_ssl, pl_v_feat, pl_v_sl, pl_v_ssl, pl_ids, pl_mask,\n",
        "             labels) = batch\n",
        "\n",
        "            # Get current batch size\n",
        "            current_batch_size = ctx_a_feat.shape[0]\n",
        "            # If batch is empty, skip\n",
        "            if current_batch_size == 0: continue\n",
        "\n",
        "            # Move data to device\n",
        "            batch_data_on_device = [t.to(device) for t in batch[:-1]]\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            # Model forward pass, ignore contrastive features (not needed during validation/testing)\n",
        "            logits, _ = model(*batch_data_on_device, current_modality_config=current_modality_config, tokenizer_for_padding=tokenizer_for_padding)\n",
        "\n",
        "            # Calculate BCE loss\n",
        "            bce_loss = bce_criterion(logits, labels)\n",
        "            # Accumulate BCE loss\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            # Get predicted class (index of the max value in logits)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Store prediction results (convert to numpy array)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            # Store true labels (convert to numpy array)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # If data loader is empty or no labels were collected\n",
        "    if len(data_loader) == 0 or len(all_labels) == 0 :\n",
        "        print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty.\")\n",
        "        if mode == \"Val\": return 0.0, 0.0 # Validation mode returns 0.0 accuracy, 0.0 F1\n",
        "        return 0.0, 0.0, 0.0 # Test mode returns 0.0 loss, 0.0 accuracy, 0.0 F1\n",
        "\n",
        "    # Calculate average BCE loss\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    # Calculate accuracy (if label list is not empty)\n",
        "    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
        "    # Calculate F1 score (if label list is not empty), use 'binary' because it's binary classification, zero_division handles boundary cases\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0) if all_labels else 0.0\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Epoch {epoch+1 if epoch is not None else 'N/A'} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # If validation mode, return accuracy and F1\n",
        "    if mode == \"Val\": return accuracy, f1\n",
        "    # If test mode, return average loss, accuracy, and F1\n",
        "    return avg_bce_loss, accuracy, f1\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameter Configuration ---\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # SENTENCE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    # SAMPLE_LSTM_HIDDEN_DIM_CONFIG defined globally\n",
        "    HIER_LSTM_DROPOUT = 0.3\n",
        "\n",
        "    PROJECTOR_OUTPUT_DIM = 1024\n",
        "\n",
        "    MAX_BERT_LEN_FOR_PART_DATASET = 512\n",
        "    TEXT_LSTM_HIDDEN_SIZE_IN_STREAM = 256   # This version of the code still uses this parameter\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    STREAM_CA_SA_HEADS = 1\n",
        "    STREAM_DROPOUT_RATE = 0.3\n",
        "\n",
        "    FINAL_CROSS_ATTENTION_HEADS = 1         # Used for all attention modules in the new final fusion structure\n",
        "    MLP_HIDDEN_DIM = 256\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    BATCH_SIZE = 16 # Warning: The new final fusion structure is more complex, may need to reduce this value\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 4 # It is recommended to increase epochs for actual use\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"BERT model used: {BERT_MODEL_NAME_FOR_MAIN}\")\n",
        "    print(f\"Hierarchical LSTM: Sentence-level hidden dim {SENTENCE_LSTM_HIDDEN_DIM_CONFIG}, Sample-level hidden dim {SAMPLE_LSTM_HIDDEN_DIM_CONFIG}, Dropout {HIER_LSTM_DROPOUT}\")\n",
        "    print(f\"Projector output dimension (Stream processor audio/video input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"Max BERT length for context/punchline part: {MAX_BERT_LEN_FOR_PART_DATASET}\")\n",
        "    print(f\"Stream processor internal text LSTM hidden size: {TEXT_LSTM_HIDDEN_SIZE_IN_STREAM}\") # Keep printing as this param is still in model def\n",
        "    print(f\"Attention token dimension: {ATTENTION_TOKEN_DIM}, Tokens per modality: {NUM_ATTENTION_TOKENS_PER_MODAL}\")\n",
        "    print(f\"Stream processor attention heads: {STREAM_CA_SA_HEADS}, Stream processor text FC Dropout rate: {STREAM_DROPOUT_RATE}\")\n",
        "    print(f\"Final fusion stage attention heads: {FINAL_CROSS_ATTENTION_HEADS}, MLP hidden dimension: {MLP_HIDDEN_DIM}\")\n",
        "    print(f\"Training parameters: Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}, Epochs {NUM_EPOCHS}\")\n",
        "    print(f\"Contrastive loss: Temperature {TEMPERATURE_CONTRASTIVE}, Weight {CONTRASTIVE_LOSS_WEIGHT}\")\n",
        "    print(\"\\n !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \\n\")\n",
        "\n",
        "    # --- Load Raw Data ---\n",
        "    print(\"Loading raw data pickle files...\")\n",
        "    # Ensure paths are correct\n",
        "    # Example: data_folds_path = \"path_to_your_gdrive/Project_CCS2-main/sdk_features/data_folds.pkl\"\n",
        "    # Replace with your actual paths\n",
        "    # To run locally, you might need to download these files or adjust paths\n",
        "    # For demonstration, we'll assume files might not exist and add checks or placeholders.\n",
        "    try:\n",
        "        data_folds = load_pickle(data_folds_path)\n",
        "        language_sdk = load_pickle(language_file)\n",
        "        covarep_sdk = load_pickle(covarep_file)\n",
        "        openface_sdk = load_pickle(openface_file)\n",
        "        humor_label_sdk = load_pickle(humor_label_file)\n",
        "        print(\"Raw data loading complete.\")\n",
        "\n",
        "        train_ids = data_folds['train']\n",
        "        dev_ids = data_folds['dev']\n",
        "        test_ids = data_folds['test']\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: One or more data files not found. Please check paths and ensure files exist.\")\n",
        "        print(\"Using placeholder data for demonstration.\")\n",
        "        # Placeholder data for demonstration if files are missing\n",
        "        train_ids, dev_ids, test_ids = ['h1','h2'], ['h3'], ['h4']\n",
        "        language_sdk = {\n",
        "            f'h{i}': {'punchline_sentence': f'Punchline {i}', 'context_sentences': [f'Context sent {i}.1', f'Context sent {i}.2']} for i in range(1, 5)\n",
        "        }\n",
        "        covarep_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _AUDIO_WORD_DIM_CONST).astype(np.float32) if i % 2 == 0 else [], # Some empty\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _AUDIO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        openface_sdk = {\n",
        "            f'h{i}': {\n",
        "                'punchline_features': np.random.rand(5, _VIDEO_WORD_DIM_CONST).astype(np.float32),\n",
        "                'context_features': [np.random.rand(np.random.randint(3,7), _VIDEO_WORD_DIM_CONST).astype(np.float32) for _ in range(2)]\n",
        "            } for i in range(1,5)\n",
        "        }\n",
        "        humor_label_sdk = {f'h{i}': float(i % 2) for i in range(1,5)}\n",
        "\n",
        "\n",
        "    print(\"Extracting features and labels...\")\n",
        "    (train_ps, train_cs, train_cvp_p, train_cvp_c, train_of_p, train_of_c, train_labels) = \\\n",
        "        extract_features_and_labels(train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (dev_ps, dev_cs, dev_cvp_p, dev_cvp_c, dev_of_p, dev_of_c, dev_labels) = \\\n",
        "        extract_features_and_labels(dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    (test_ps, test_cs, test_cvp_p, test_cvp_c, test_of_p, test_of_c, test_labels) = \\\n",
        "        extract_features_and_labels(test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "    print(\"Feature and label extraction complete.\")\n",
        "\n",
        "    print(\"Structuring data for new dataset format (context/punchline split)...\")\n",
        "    train_sample_data_dicts = concatenate_multimodal_data_for_dataset(train_cvp_c, train_of_c, train_cs, train_cvp_p, train_of_p, train_ps)\n",
        "    dev_sample_data_dicts = concatenate_multimodal_data_for_dataset(dev_cvp_c, dev_of_c, dev_cs, dev_cvp_p, dev_of_p, dev_ps)\n",
        "    test_sample_data_dicts = concatenate_multimodal_data_for_dataset(test_cvp_c, test_of_c, test_cs, test_cvp_p, test_of_p, test_ps)\n",
        "    print(\"Data structuring complete.\")\n",
        "\n",
        "    print(\"Initializing BERT tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    _bert_temp_model = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = _bert_temp_model.config.hidden_size\n",
        "    del _bert_temp_model\n",
        "    print(f\"Actual BERT hidden size: {BERT_HIDDEN_SIZE_ACTUAL}\")\n",
        "\n",
        "    print(\"Creating CustomFeatureDatasetContextPunchline instances...\")\n",
        "    train_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        train_sample_data_dicts, train_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        dev_sample_data_dicts, dev_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "    test_dataset = CustomFeatureDatasetContextPunchline(\n",
        "        test_sample_data_dicts, test_labels, bert_tokenizer_global,\n",
        "        max_bert_len_for_part=MAX_BERT_LEN_FOR_PART_DATASET\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=custom_collate_fn_context_punchline, drop_last=True if BATCH_SIZE > 1 and len(train_dataset) > BATCH_SIZE else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_context_punchline)\n",
        "    print(f\"Dataloaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "        {'name': 'V',   'audio': False, 'video': True, 'text': False},\n",
        "        {'name': 'A',   'audio': True, 'video': False, 'text': False},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    for config_idx, model_config_iter in enumerate(modality_configurations):\n",
        "        config_name = model_config_iter['name']\n",
        "        print(f\"\\n--- Starting processing for model config: {config_name} ---\")\n",
        "\n",
        "        model = ContextPunchlineHumorModelNew(\n",
        "            bert_model_name_or_path=BERT_MODEL_NAME_FOR_MAIN,\n",
        "            audio_word_dim=_AUDIO_WORD_DIM_CONST, video_word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=SAMPLE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            hier_lstm_dropout=HIER_LSTM_DROPOUT,\n",
        "            projector_output_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size_actual=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len_for_lstm=MAX_BERT_LEN_FOR_PART_DATASET, # This is max_bert_len_for_part\n",
        "            text_lstm_hidden_size_in_stream=TEXT_LSTM_HIDDEN_SIZE_IN_STREAM,\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            stream_ca_sa_heads=STREAM_CA_SA_HEADS,\n",
        "            stream_dropout_rate=STREAM_DROPOUT_RATE,\n",
        "            final_cross_attention_heads=FINAL_CROSS_ATTENTION_HEADS,\n",
        "            mlp_hidden_dim=MLP_HIDDEN_DIM,\n",
        "            num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        print(\"Freezing BERT parameters in the main model...\")\n",
        "        for param in model.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen.\")\n",
        "\n",
        "        bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "        optimizer_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = AdamW(optimizer_params, lr=LEARNING_RATE)\n",
        "        scheduler = None\n",
        "        if len(train_loader) > 0 and NUM_EPOCHS > 0:\n",
        "            num_training_steps_per_epoch = len(train_loader)\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs.\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_val_f1_at_best_acc = 0.0 # Store F1 at the point of best accuracy\n",
        "        best_model_state_path = f\"best_model_{config_name}.pth\"\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader for {config_name} is empty. Skipping training.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_new_model(model, train_loader, optimizer, scheduler, bce_criterion,\n",
        "                                contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS,\n",
        "                                CONTRASTIVE_LOSS_WEIGHT, model_config_iter, bert_tokenizer_global)\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy, val_f1 = validate_or_test_new_model(model, val_loader, bce_criterion, DEVICE,\n",
        "                                                                    epoch, NUM_EPOCHS, model_config_iter,\n",
        "                                                                    bert_tokenizer_global, mode=\"Val\")\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        best_val_f1_at_best_acc = val_f1 # Save F1 at this best accuracy point\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f} (F1: {best_val_f1_at_best_acc:.4f}). Saving model...\")\n",
        "                        torch.save(model.state_dict(), best_model_state_path)\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (corresponding F1: {best_val_f1_at_best_acc:.4f})\")\n",
        "\n",
        "        print(f\"\\nStarting test phase for {config_name}...\")\n",
        "        test_accuracy, test_f1, test_loss = 0.0, 0.0, 0.0 # Initialize\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader for {config_name} is empty. Skipping test.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                                               'test_acc': 0.0, 'test_f1':0.0, 'test_loss': 0.0}\n",
        "        else:\n",
        "            if os.path.exists(best_model_state_path) and best_val_accuracy_for_config > 0: # Check if model was saved\n",
        "                print(f\"Loading best model state from {best_model_state_path} for testing.\")\n",
        "                model.load_state_dict(torch.load(best_model_state_path, map_location=DEVICE))\n",
        "            elif best_val_accuracy_for_config == 0 and len(train_loader) > 0 : # Was trained, but no improvement or no val\n",
        "                print(f\"No best validation model saved (or validation accuracy was 0), using model from last training epoch for testing.\")\n",
        "            elif len(train_loader) == 0: # Not trained\n",
        "                print(f\"No training was performed for {config_name}. Testing with initialized model (results might be poor).\")\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test_new_model(\n",
        "                model, test_loader, bce_criterion, DEVICE, epoch=None, num_epochs=NUM_EPOCHS, # epoch=None for final test\n",
        "                current_modality_config=model_config_iter, tokenizer_for_padding=bert_tokenizer_global, mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config, 'val_f1': best_val_f1_at_best_acc,\n",
        "                'test_acc': test_accuracy, 'test_f1': test_f1, 'test_loss': test_loss,\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy: {results.get('val_acc', 0.0):.4f} (Corresponding Val F1: {results.get('val_f1', 0.0):.4f})\")\n",
        "        print(f\"  Test Set Accuracy: {results.get('test_acc', 0.0):.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results.get('test_f1', 0.0):.4f}\")\n",
        "        print(f\"  Test Set Loss: {results.get('test_loss', 0.0):.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GeRy76_YocK",
        "outputId": "24ff4cd1-ee76-4d15-d5fb-02a920f9b6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT model used: bert-base-uncased\n",
            "Hierarchical LSTM: Sentence-level hidden dim 256, Sample-level hidden dim 512, Dropout 0.3\n",
            "Projector output dimension (Stream processor audio/video input): 1024\n",
            "Max BERT length for context/punchline part: 512\n",
            "Stream processor internal text LSTM hidden size: 256\n",
            "Attention token dimension: 32, Tokens per modality: 16\n",
            "Stream processor attention heads: 1, Stream processor text FC Dropout rate: 0.3\n",
            "Final fusion stage attention heads: 1, MLP hidden dimension: 256\n",
            "Training parameters: Batch size 16, Learning rate 5e-05, Epochs 4\n",
            "Contrastive loss: Temperature 0.5, Weight 0.03\n",
            "\n",
            " !!! WARNING: The new final fusion structure (mimicking ASP) is more complex than the original single cross-attention and may significantly increase VRAM consumption and computation time. If you encounter OOM, try drastically reducing BATCH_SIZE first. !!! \n",
            "\n",
            "Loading raw data pickle files...\n",
            "Raw data loading complete.\n",
            "Extracting features and labels...\n",
            "Feature and label extraction complete.\n",
            "Structuring data for new dataset format (context/punchline split)...\n",
            "Data structuring complete.\n",
            "Initializing BERT tokenizer...\n",
            "Actual BERT hidden size: 768\n",
            "Creating CustomFeatureDatasetContextPunchline instances...\n",
            "Dataloaders created. Train batches: 475, Val batches: 62, Test batches: 63\n",
            "\n",
            "--- Starting processing for model config: T ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for T... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Train Avg Loss: 0.6682, BCE: 0.6682, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Val Avg BCE: 0.6325, Accuracy: 0.6520, F1: 0.7073\n",
            "Epoch 1 (T): New best validation accuracy: 0.6520 (F1: 0.7073). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Train Avg Loss: 0.6091, BCE: 0.6091, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Val Avg BCE: 0.6110, Accuracy: 0.6765, F1: 0.6466\n",
            "Epoch 2 (T): New best validation accuracy: 0.6765 (F1: 0.6466). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Train Avg Loss: 0.5794, BCE: 0.5794, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Val Avg BCE: 0.6014, Accuracy: 0.6857, F1: 0.6737\n",
            "Epoch 3 (T): New best validation accuracy: 0.6857 (F1: 0.6737). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Train Avg Loss: 0.5449, BCE: 0.5449, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Val Avg BCE: 0.6125, Accuracy: 0.6776, F1: 0.6565\n",
            "Training for T complete. Best validation accuracy for this config: 0.6857 (corresponding F1: 0.6737)\n",
            "\n",
            "Starting test phase for T...\n",
            "Loading best model state from best_model_T.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (T) Test Avg BCE: 0.5895, Accuracy: 0.6710, F1: 0.6687\n",
            "Final test results for T -> Avg BCE Loss: 0.5895, Accuracy: 0.6710, F1: 0.6687\n",
            "\n",
            "--- Starting processing for model config: V ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for V... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Train Avg Loss: 0.6940, BCE: 0.6940, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Val Avg BCE: 0.6932, Accuracy: 0.5041, F1: 0.6703\n",
            "Epoch 1 (V): New best validation accuracy: 0.5041 (F1: 0.6703). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Train Avg Loss: 0.6860, BCE: 0.6860, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Val Avg BCE: 0.6929, Accuracy: 0.5469, F1: 0.4264\n",
            "Epoch 2 (V): New best validation accuracy: 0.5469 (F1: 0.4264). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Train Avg Loss: 0.6721, BCE: 0.6721, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Val Avg BCE: 0.6871, Accuracy: 0.5541, F1: 0.5051\n",
            "Epoch 3 (V): New best validation accuracy: 0.5541 (F1: 0.5051). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Train Avg Loss: 0.6694, BCE: 0.6694, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Val Avg BCE: 0.6867, Accuracy: 0.5571, F1: 0.5303\n",
            "Epoch 4 (V): New best validation accuracy: 0.5571 (F1: 0.5303). Saving model...\n",
            "Training for V complete. Best validation accuracy for this config: 0.5571 (corresponding F1: 0.5303)\n",
            "\n",
            "Starting test phase for V...\n",
            "Loading best model state from best_model_V.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (V) Test Avg BCE: 0.6672, Accuracy: 0.6016, F1: 0.5909\n",
            "Final test results for V -> Avg BCE Loss: 0.6672, Accuracy: 0.6016, F1: 0.5909\n",
            "\n",
            "--- Starting processing for model config: A ---\n",
            "Freezing BERT parameters in the main model...\n",
            "BERT parameters frozen.\n",
            "Starting training for A... Total 4 epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Train Avg Loss: 0.6942, BCE: 0.6942, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Val Avg BCE: 0.6933, Accuracy: 0.4959, F1: 0.0000\n",
            "Epoch 1 (A): New best validation accuracy: 0.4959 (F1: 0.0000). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Train Avg Loss: 0.6934, BCE: 0.6934, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Val Avg BCE: 0.6932, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Train Avg Loss: 0.6937, BCE: 0.6937, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Val Avg BCE: 0.6931, Accuracy: 0.5041, F1: 0.6703\n",
            "Epoch 3 (A): New best validation accuracy: 0.5041 (F1: 0.6703). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Train Avg Loss: 0.6934, BCE: 0.6934, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n",
            "Training for A complete. Best validation accuracy for this config: 0.5041 (corresponding F1: 0.6703)\n",
            "\n",
            "Starting test phase for A...\n",
            "Loading best model state from best_model_A.pth for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch N/A (A) Test Avg BCE: 0.6932, Accuracy: 0.4930, F1: 0.6604\n",
            "Final test results for A -> Avg BCE Loss: 0.6932, Accuracy: 0.4930, F1: 0.6604\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: T\n",
            "  Best Validation Accuracy: 0.6857 (Corresponding Val F1: 0.6737)\n",
            "  Test Set Accuracy: 0.6710\n",
            "  Test Set F1 Score: 0.6687\n",
            "  Test Set Loss: 0.5895\n",
            "------------------------------\n",
            "Configuration: V\n",
            "  Best Validation Accuracy: 0.5571 (Corresponding Val F1: 0.5303)\n",
            "  Test Set Accuracy: 0.6016\n",
            "  Test Set F1 Score: 0.5909\n",
            "  Test Set Loss: 0.6672\n",
            "------------------------------\n",
            "Configuration: A\n",
            "  Best Validation Accuracy: 0.5041 (Corresponding Val F1: 0.6703)\n",
            "  Test Set Accuracy: 0.4930\n",
            "  Test Set F1 Score: 0.6604\n",
            "  Test Set Loss: 0.6932\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}
