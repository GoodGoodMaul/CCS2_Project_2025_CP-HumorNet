{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0cb7e9084634f10a3d31a4360b5e370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cac396aef2424601b87a6fb29b0b65a3",
              "IPY_MODEL_d7316e7a391c405c8790dc31bc2fad9f",
              "IPY_MODEL_3a853dad383e41a7b274377205763274"
            ],
            "layout": "IPY_MODEL_5ab007aecce44b2dac2923432587cfde"
          }
        },
        "cac396aef2424601b87a6fb29b0b65a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0526f919f08e43ccb23ed512609311cc",
            "placeholder": "​",
            "style": "IPY_MODEL_7a43439fd77047059d8bca6ad6ec2fd5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d7316e7a391c405c8790dc31bc2fad9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_232dae75cf1b46718c4cb7aba34795c5",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a05209e107b40a0b63646e9d4285052",
            "value": 48
          }
        },
        "3a853dad383e41a7b274377205763274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b7e58894cc4cd587c70a7b46b6cac7",
            "placeholder": "​",
            "style": "IPY_MODEL_66023d065d2d4affa1a66244f4061c71",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.15kB/s]"
          }
        },
        "5ab007aecce44b2dac2923432587cfde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0526f919f08e43ccb23ed512609311cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a43439fd77047059d8bca6ad6ec2fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "232dae75cf1b46718c4cb7aba34795c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a05209e107b40a0b63646e9d4285052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5b7e58894cc4cd587c70a7b46b6cac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66023d065d2d4affa1a66244f4061c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efe56728c62147b5af4eec8cfd4f069c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88f7193094e44df88d670f6d26cc551b",
              "IPY_MODEL_207248e7f77e42d28e5a9bbfac4a7064",
              "IPY_MODEL_bea8d481d9984082a8f14fbeadb0d521"
            ],
            "layout": "IPY_MODEL_2d2a4b02690c4f90a9cfa73b0a7ed250"
          }
        },
        "88f7193094e44df88d670f6d26cc551b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a139747606714a84829270b1a276573c",
            "placeholder": "​",
            "style": "IPY_MODEL_0f02bd73e91c45539a972f99bc72caa0",
            "value": "config.json: 100%"
          }
        },
        "207248e7f77e42d28e5a9bbfac4a7064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8a0a8cdd964eeeb61e4b4b9dda6e37",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01fe332c6f4e4dfaad57e5a6c9090f6d",
            "value": 570
          }
        },
        "bea8d481d9984082a8f14fbeadb0d521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd47a81344944b4985b76e22c873ecf3",
            "placeholder": "​",
            "style": "IPY_MODEL_646f3d0e247142de9890c91c6125b74f",
            "value": " 570/570 [00:00&lt;00:00, 69.8kB/s]"
          }
        },
        "2d2a4b02690c4f90a9cfa73b0a7ed250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a139747606714a84829270b1a276573c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f02bd73e91c45539a972f99bc72caa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8a0a8cdd964eeeb61e4b4b9dda6e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01fe332c6f4e4dfaad57e5a6c9090f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd47a81344944b4985b76e22c873ecf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646f3d0e247142de9890c91c6125b74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0be11616d52c416fbf66de0058803813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97ec601c610146fe9bff6012e460884a",
              "IPY_MODEL_67a7f81647284832aceaa37fec45e172",
              "IPY_MODEL_57915fbb1d64499dbe4b2b4e6a7839ce"
            ],
            "layout": "IPY_MODEL_9daa31d568de4416947c5c784f2f4997"
          }
        },
        "97ec601c610146fe9bff6012e460884a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c862485b7449bd8ffe2655bff3f053",
            "placeholder": "​",
            "style": "IPY_MODEL_665e40ca60594a4eaeec719f12392d1f",
            "value": "vocab.txt: 100%"
          }
        },
        "67a7f81647284832aceaa37fec45e172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72446547dd0d460eb0be1c7b54c0ff2e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a618e9e3004447e964e87b2ae89cfed",
            "value": 231508
          }
        },
        "57915fbb1d64499dbe4b2b4e6a7839ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c942f6ebe781455ea8c02fae0925879c",
            "placeholder": "​",
            "style": "IPY_MODEL_27e5fda0b4534609a25e31b206f3e9bd",
            "value": " 232k/232k [00:00&lt;00:00, 16.2MB/s]"
          }
        },
        "9daa31d568de4416947c5c784f2f4997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c862485b7449bd8ffe2655bff3f053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665e40ca60594a4eaeec719f12392d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72446547dd0d460eb0be1c7b54c0ff2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a618e9e3004447e964e87b2ae89cfed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c942f6ebe781455ea8c02fae0925879c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e5fda0b4534609a25e31b206f3e9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e6c9df99a947518c5cba0226a7ebc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ad63f0b36154aeeafb4d28fc9ebcade",
              "IPY_MODEL_8d4af11ceb984e3799fa26e6fdd73d9a",
              "IPY_MODEL_51e4620136da4f0089c305338b3e4925"
            ],
            "layout": "IPY_MODEL_d3efc42f8a984fe8973e25cdd814d1f9"
          }
        },
        "9ad63f0b36154aeeafb4d28fc9ebcade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2efd85fa3fcb4261a19546892ad5c330",
            "placeholder": "​",
            "style": "IPY_MODEL_ad978cc6b6b34ae0ad2b300a36c455ae",
            "value": "tokenizer.json: 100%"
          }
        },
        "8d4af11ceb984e3799fa26e6fdd73d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af58486e62e24602a85c80857ce9d492",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c85aa58ae3034353aa3112f188c1392a",
            "value": 466062
          }
        },
        "51e4620136da4f0089c305338b3e4925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebed43d64316477ba9713a9726d92aff",
            "placeholder": "​",
            "style": "IPY_MODEL_86ea3b6bc2cd424b98f236752bcafe52",
            "value": " 466k/466k [00:00&lt;00:00, 18.4MB/s]"
          }
        },
        "d3efc42f8a984fe8973e25cdd814d1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efd85fa3fcb4261a19546892ad5c330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad978cc6b6b34ae0ad2b300a36c455ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af58486e62e24602a85c80857ce9d492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85aa58ae3034353aa3112f188c1392a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebed43d64316477ba9713a9726d92aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ea3b6bc2cd424b98f236752bcafe52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76f7f91a579445b0aa2c6dc38f0a197f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55a3fab26ac541f0a2744c34c6f79a25",
              "IPY_MODEL_bc22ad6c81034bf698dad2bf77f65c97",
              "IPY_MODEL_389f6323fe694d289cd3a6ada5139c7f"
            ],
            "layout": "IPY_MODEL_8074991d595e468c924fad7851de02f6"
          }
        },
        "55a3fab26ac541f0a2744c34c6f79a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64d0c158c86f432291bd55408dc42a7e",
            "placeholder": "​",
            "style": "IPY_MODEL_bcbdad6dbb7e4b3b837ecad1e0cb8e84",
            "value": "model.safetensors: 100%"
          }
        },
        "bc22ad6c81034bf698dad2bf77f65c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f414cd74c948629fc01cc77c836b24",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bfcd93f374f447b8d88ed8c0465a580",
            "value": 440449768
          }
        },
        "389f6323fe694d289cd3a6ada5139c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b9939ac2db404eada3ec478ef168e0",
            "placeholder": "​",
            "style": "IPY_MODEL_27f01ff9898c452aa95d40c538952476",
            "value": " 440M/440M [00:00&lt;00:00, 484MB/s]"
          }
        },
        "8074991d595e468c924fad7851de02f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d0c158c86f432291bd55408dc42a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcbdad6dbb7e4b3b837ecad1e0cb8e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f414cd74c948629fc01cc77c836b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bfcd93f374f447b8d88ed8c0465a580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23b9939ac2db404eada3ec478ef168e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27f01ff9898c452aa95d40c538952476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80e7e774711f4111a3b92c2a85f37db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b84446b04db9451b91e64ccfcb260113",
              "IPY_MODEL_7a4308e945244b4eb5a8faf8da4e08ab",
              "IPY_MODEL_dc4983f4bc194c7a913a7152d8030a81"
            ],
            "layout": "IPY_MODEL_5efaa63fb4f34267936a55746d914e5c"
          }
        },
        "b84446b04db9451b91e64ccfcb260113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e45ad75cc1644f3faa1bd467b961fa27",
            "placeholder": "​",
            "style": "IPY_MODEL_f31d4da2a93e484db5ab0b0a2d5a2a6c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7a4308e945244b4eb5a8faf8da4e08ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5388cf6a3c274c3cb2ff04b8416ebbe2",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d635e1897ef482d89e28cc79759ee07",
            "value": 48
          }
        },
        "dc4983f4bc194c7a913a7152d8030a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_462ec7d1e73b4abcbd089113a99e78c5",
            "placeholder": "​",
            "style": "IPY_MODEL_fac942e4072442a59a9b51a8071da28c",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.97kB/s]"
          }
        },
        "5efaa63fb4f34267936a55746d914e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45ad75cc1644f3faa1bd467b961fa27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f31d4da2a93e484db5ab0b0a2d5a2a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5388cf6a3c274c3cb2ff04b8416ebbe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d635e1897ef482d89e28cc79759ee07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "462ec7d1e73b4abcbd089113a99e78c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac942e4072442a59a9b51a8071da28c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71aaed2087b7453ea2663a25e28ee895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b249a08d5d84b35861336a1b215a8d3",
              "IPY_MODEL_d8d2006bf7f54b4fb0c49558f65c4a6f",
              "IPY_MODEL_fbe6006eb5744462b6254816efb31cb3"
            ],
            "layout": "IPY_MODEL_65bdce2918bd4da386626546e975bad6"
          }
        },
        "8b249a08d5d84b35861336a1b215a8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a6e174bdd2e47968367d151bcba0abc",
            "placeholder": "​",
            "style": "IPY_MODEL_8e74bc3560a04c2b879121eef986b644",
            "value": "config.json: 100%"
          }
        },
        "d8d2006bf7f54b4fb0c49558f65c4a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37b9debfa974b94bbf6d7f3c99d2b12",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccb45754122d4c0980d9be64ce804b1d",
            "value": 570
          }
        },
        "fbe6006eb5744462b6254816efb31cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1670855f79bd4478b0c833f3fa6231ec",
            "placeholder": "​",
            "style": "IPY_MODEL_2a77be1c348b42b1948ab76b95a24809",
            "value": " 570/570 [00:00&lt;00:00, 77.9kB/s]"
          }
        },
        "65bdce2918bd4da386626546e975bad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6e174bdd2e47968367d151bcba0abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e74bc3560a04c2b879121eef986b644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f37b9debfa974b94bbf6d7f3c99d2b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb45754122d4c0980d9be64ce804b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1670855f79bd4478b0c833f3fa6231ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a77be1c348b42b1948ab76b95a24809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b800fe7d7b884fcdab6b83ddad97bec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_319cb0bcde3e4984a8d35a24a7045393",
              "IPY_MODEL_99835fc5f7d34e44a2a2df58a172318b",
              "IPY_MODEL_0c3140606a1946d08fced86ac5c02cfb"
            ],
            "layout": "IPY_MODEL_d1fdc3fd39824a9cbd33836262721c8a"
          }
        },
        "319cb0bcde3e4984a8d35a24a7045393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb90eeb85de4c20b6afb9aba52a9e4c",
            "placeholder": "​",
            "style": "IPY_MODEL_61bf6ff388f64718a4ae764ad7e75ccb",
            "value": "vocab.txt: 100%"
          }
        },
        "99835fc5f7d34e44a2a2df58a172318b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8f78913eb046979960f32f9f4d7fd2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5891e362a41441eca652b183a9fa51b5",
            "value": 231508
          }
        },
        "0c3140606a1946d08fced86ac5c02cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41271c656b2e4efeb0b5299907ff810f",
            "placeholder": "​",
            "style": "IPY_MODEL_6019dad1df5349e7b456d5fa3d48e175",
            "value": " 232k/232k [00:00&lt;00:00, 3.46MB/s]"
          }
        },
        "d1fdc3fd39824a9cbd33836262721c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb90eeb85de4c20b6afb9aba52a9e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61bf6ff388f64718a4ae764ad7e75ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd8f78913eb046979960f32f9f4d7fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5891e362a41441eca652b183a9fa51b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41271c656b2e4efeb0b5299907ff810f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6019dad1df5349e7b456d5fa3d48e175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff602ac182a342c89eea873a02efab6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_578e50bfbe8b4d1987a96d119308150e",
              "IPY_MODEL_4c09f424cc1a4c4aab274c51403538e6",
              "IPY_MODEL_c11c5f1c280740779d1a3d678b605d7a"
            ],
            "layout": "IPY_MODEL_d585ed4a581a41d5b205ef545bc71cb1"
          }
        },
        "578e50bfbe8b4d1987a96d119308150e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a6c379616049a5ac17400155e0a73f",
            "placeholder": "​",
            "style": "IPY_MODEL_1352ee6ca7ae4927bd23bee507a0210f",
            "value": "tokenizer.json: 100%"
          }
        },
        "4c09f424cc1a4c4aab274c51403538e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590bdf64ce4c47fa9e3ce4bbcc12fb85",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44c2229b2e5749d18350db16bad1bd51",
            "value": 466062
          }
        },
        "c11c5f1c280740779d1a3d678b605d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f277baac84a4b0b83e8c4c635a5e739",
            "placeholder": "​",
            "style": "IPY_MODEL_a43c9d2d4bf44a88962ee04a8fd9ee08",
            "value": " 466k/466k [00:00&lt;00:00, 6.50MB/s]"
          }
        },
        "d585ed4a581a41d5b205ef545bc71cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a6c379616049a5ac17400155e0a73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1352ee6ca7ae4927bd23bee507a0210f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "590bdf64ce4c47fa9e3ce4bbcc12fb85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c2229b2e5749d18350db16bad1bd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f277baac84a4b0b83e8c4c635a5e739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a43c9d2d4bf44a88962ee04a8fd9ee08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37f6a0c41926433b9afbe7c973bc65a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1324cacaef55401f97608293ce6613a5",
              "IPY_MODEL_0c702f48cce94aed9100ec67fdb2e937",
              "IPY_MODEL_39ce899ada6443d78a6a4b2fb9b3c660"
            ],
            "layout": "IPY_MODEL_8df2e77b894d47b69043393140ac5324"
          }
        },
        "1324cacaef55401f97608293ce6613a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ebae6d5ee6345c090ca1e1740801190",
            "placeholder": "​",
            "style": "IPY_MODEL_631ad8da3b2e4986be617b510031295a",
            "value": "model.safetensors: 100%"
          }
        },
        "0c702f48cce94aed9100ec67fdb2e937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f2771611e94c76b975e8e1eebe579d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aea217685ac24e279a7be37da3ac653d",
            "value": 440449768
          }
        },
        "39ce899ada6443d78a6a4b2fb9b3c660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2252b13bf2f8446893a21219960f1097",
            "placeholder": "​",
            "style": "IPY_MODEL_296cf0b1352d4f60aa624e676b9f58c7",
            "value": " 440M/440M [00:01&lt;00:00, 274MB/s]"
          }
        },
        "8df2e77b894d47b69043393140ac5324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebae6d5ee6345c090ca1e1740801190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631ad8da3b2e4986be617b510031295a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81f2771611e94c76b975e8e1eebe579d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea217685ac24e279a7be37da3ac653d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2252b13bf2f8446893a21219960f1097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "296cf0b1352d4f60aa624e676b9f58c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vtFjQaJEdgv",
        "outputId": "2f1849b4-8ee4-4c47-ecb6-6eac1f08d646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original loss function, audio and video feature projection layers participate in training\n",
        "import pickle # Import the pickle module, used for serializing and deserializing Python object structures\n",
        "import numpy as np # Import the numpy library, used for scientific computing, especially array operations\n",
        "import torch # Import the PyTorch library, an open-source machine learning framework\n",
        "import torch.nn as nn # Import PyTorch's neural network module\n",
        "import torch.nn.functional as F # Import PyTorch's neural network function library\n",
        "from torch.utils.data import Dataset, DataLoader # From PyTorch, import Dataset and DataLoader classes for data loading\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup # From transformers library, import auto tokenizer, auto model, and learning rate scheduler\n",
        "from torch.optim import AdamW # From PyTorch, import AdamW optimizer\n",
        "from sklearn.metrics import accuracy_score, f1_score # From scikit-learn, import accuracy calculation function\n",
        "import copy # Import the copy module, used for deep copying objects (e.g., when saving the best model)\n",
        "from tqdm import tqdm # Import the tqdm library, used to display progress bars\n",
        "import os # Import the os module, used for file path operations, etc.\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence # Used for handling variable-length sequences\n",
        "\n",
        "# --- File Path Configuration (Please ensure these paths are correct in your environment) ---\n",
        "DRIVE_MOUNT_PATH = \"/content/gdrive/MyDrive/\" # Google Drive mount path\n",
        "BASE_PROJECT_PATH = os.path.join(DRIVE_MOUNT_PATH, \"Project_CCS2-main/sdk_features/\") # Base path where feature files are located\n",
        "\n",
        "data_folds_path = os.path.join(BASE_PROJECT_PATH, \"data_folds.pkl\")\n",
        "openface_file = os.path.join(BASE_PROJECT_PATH, \"openface_features_sdk.pkl\")\n",
        "covarep_file = os.path.join(BASE_PROJECT_PATH, \"covarep_features_sdk.pkl\")\n",
        "language_file = os.path.join(BASE_PROJECT_PATH, \"language_sdk.pkl\")\n",
        "humor_label_file = os.path.join(BASE_PROJECT_PATH, \"humor_label_sdk.pkl\")\n",
        "# word_embedding_list_file = os.path.join(BASE_PROJECT_PATH, \"word_embedding_list.pkl\") # Not used directly, kept\n",
        "_AUDIO_WORD_DIM_CONST = 81 # Define constant for audio word-level feature dimension\n",
        "_VIDEO_WORD_DIM_CONST = 371 # Define constant for video word-level feature dimension\n",
        "# New: Define hidden dimension for hierarchical LSTM, will be used by subsequent models\n",
        "SENTENCE_LSTM_HIDDEN_DIM_CONFIG = 256\n",
        "SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except UnicodeDecodeError:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print(f'Unable to load data {pickle_file}: {e}')\n",
        "        raise\n",
        "\n",
        "# def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "# ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "# labels_list = []\n",
        "# for hid in id_list:\n",
        "# ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "# cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "# # Ensure features are numpy arrays, even if empty\n",
        "# cvp_p_list.append(np.array(covarep_sdk[hid]['punchline_features'] if covarep_sdk[hid]['punchline_features'] else []))\n",
        "# cvp_c_list.append([np.array(sent if sent else []) for sent in covarep_sdk[hid]['context_features']])\n",
        "# of_p_list.append(np.array(openface_sdk[hid]['punchline_features'] if openface_sdk[hid]['punchline_features'] else []))\n",
        "# of_c_list.append([np.array(sent if sent else []) for sent in openface_sdk[hid]['context_features']])\n",
        "# labels_list.append(humor_label_sdk[hid])\n",
        "# return (\n",
        "# np.array(ps_list, dtype=object),\n",
        "# np.array(cs_list, dtype=object),\n",
        "# np.array(cvp_p_list, dtype=object),\n",
        "# np.array(cvp_c_list, dtype=object),\n",
        "# np.array(of_p_list, dtype=object),\n",
        "# np.array(of_c_list, dtype=object),\n",
        "# np.array(labels_list, dtype=np.float32) # Labels will be converted to long in Dataset\n",
        "# )\n",
        "def _prepare_feature_for_numpy(feature_data):\n",
        "    \"\"\"\n",
        "    Helper function to prepare feature data to be safely passed to np.array().\n",
        "    If the input is None, an empty list, or an empty NumPy array, it returns an empty list,\n",
        "    so that np.array() creates an empty numerical array.\n",
        "    Otherwise, returns the original data (if it's a list or an existing NumPy array).\n",
        "    \"\"\"\n",
        "    if feature_data is None:\n",
        "        return []  # None -> empty list for np.array\n",
        "    if isinstance(feature_data, np.ndarray):\n",
        "        # If it's a NumPy array, check its size\n",
        "        if feature_data.size == 0:\n",
        "            return []  # Empty NumPy array -> empty list for np.array\n",
        "        return feature_data # Non-empty NumPy array, np.array() will copy it\n",
        "    if isinstance(feature_data, list):\n",
        "        if not feature_data: # Empty list\n",
        "            return []\n",
        "        return feature_data # Non-empty list\n",
        "\n",
        "    # For other unexpected types, can print a warning and return an empty list\n",
        "    # print(f\"Warning: Unexpected feature type {type(feature_data)}, treating as empty.\")\n",
        "    return []\n",
        "\n",
        "# Then use it in the extract_features_and_labels function like this:\n",
        "def extract_features_and_labels(id_list, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk):\n",
        "    ps_list, cs_list, cvp_p_list, cvp_c_list, of_p_list, of_c_list = [], [], [], [], [], []\n",
        "    labels_list = []\n",
        "\n",
        "    for hid in id_list:\n",
        "        ps_list.append(language_sdk[hid]['punchline_sentence'])\n",
        "        cs_list.append(language_sdk[hid]['context_sentences'])\n",
        "\n",
        "        # COVAREP features\n",
        "        prepared_punchline_cvp = _prepare_feature_for_numpy(covarep_sdk[hid]['punchline_features'])\n",
        "        cvp_p_list.append(np.array(prepared_punchline_cvp))\n",
        "\n",
        "        processed_sents_cvp = []\n",
        "        for sent_feat in covarep_sdk[hid]['context_features']:\n",
        "            prepared_sent_cvp = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_cvp.append(np.array(prepared_sent_cvp))\n",
        "        cvp_c_list.append(processed_sents_cvp)\n",
        "\n",
        "        # OpenFace features\n",
        "        prepared_punchline_of = _prepare_feature_for_numpy(openface_sdk[hid]['punchline_features'])\n",
        "        of_p_list.append(np.array(prepared_punchline_of))\n",
        "\n",
        "        processed_sents_of = []\n",
        "        for sent_feat in openface_sdk[hid]['context_features']:\n",
        "            prepared_sent_of = _prepare_feature_for_numpy(sent_feat)\n",
        "            processed_sents_of.append(np.array(prepared_sent_of))\n",
        "        of_c_list.append(processed_sents_of)\n",
        "\n",
        "        labels_list.append(humor_label_sdk[hid])\n",
        "\n",
        "    return (\n",
        "        np.array(ps_list, dtype=object),\n",
        "        np.array(cs_list, dtype=object),\n",
        "        np.array(cvp_p_list, dtype=object),\n",
        "        np.array(cvp_c_list, dtype=object),\n",
        "        np.array(of_p_list, dtype=object),\n",
        "        np.array(of_c_list, dtype=object),\n",
        "        np.array(labels_list, dtype=np.float32)\n",
        "    )\n",
        "\n",
        "def concatenate_multimodal_data(cvp_c, of_c, cs, cvp_p, of_p, ps):\n",
        "    num_samples = len(cvp_c)\n",
        "    if not (len(of_c) == num_samples and \\\n",
        "            len(cs) == num_samples and \\\n",
        "            len(cvp_p) == num_samples and \\\n",
        "            len(of_p) == num_samples and \\\n",
        "            len(ps) == num_samples):\n",
        "        raise ValueError(\"All input lists must have the same number of samples.\")\n",
        "    concatenated_audio_features = []\n",
        "    concatenated_video_features = []\n",
        "    concatenated_text_features = []\n",
        "    for i in range(num_samples):\n",
        "        sample_cvp_c_sentence_features = list(cvp_c[i])\n",
        "        sample_of_c_sentence_features = list(of_c[i])\n",
        "        sample_cs_sentence_texts = list(cs[i])\n",
        "        punchline_audio_features = cvp_p[i]\n",
        "        punchline_video_features = of_p[i]\n",
        "        punchline_text = ps[i]\n",
        "        current_sample_audio = sample_cvp_c_sentence_features.copy()\n",
        "        current_sample_audio.append(punchline_audio_features)\n",
        "        concatenated_audio_features.append(current_sample_audio)\n",
        "        current_sample_video = sample_of_c_sentence_features.copy()\n",
        "        current_sample_video.append(punchline_video_features)\n",
        "        concatenated_video_features.append(current_sample_video)\n",
        "        current_sample_text = sample_cs_sentence_texts.copy()\n",
        "        current_sample_text.append(punchline_text)\n",
        "        concatenated_text_features.append(current_sample_text)\n",
        "    return concatenated_audio_features, concatenated_video_features, concatenated_text_features\n",
        "\n",
        "\n",
        "# --- Modify CustomFeatureDataset ---\n",
        "# It is now only responsible for providing raw (or near-raw) feature data and text data\n",
        "class CustomFeatureDataset(Dataset):\n",
        "    def __init__(self, list_of_audio_sample_data, list_of_video_sample_data,\n",
        "                 list_of_text_sentence_lists_per_sample, list_of_labels,\n",
        "                 bert_tokenizer, max_bert_len=512):\n",
        "\n",
        "        if not (len(list_of_audio_sample_data) == len(list_of_video_sample_data) == \\\n",
        "                len(list_of_text_sentence_lists_per_sample) == len(list_of_labels)):\n",
        "            raise ValueError(\"All input data lists must have the same length.\")\n",
        "\n",
        "        self.list_of_audio_sample_data = list_of_audio_sample_data\n",
        "        self.list_of_video_sample_data = list_of_video_sample_data\n",
        "        self.list_of_text_sentence_lists_per_sample = list_of_text_sentence_lists_per_sample\n",
        "        self.list_of_labels = torch.tensor(list_of_labels, dtype=torch.long) # Ensure labels are Long type\n",
        "        self.tokenizer = bert_tokenizer\n",
        "        self.max_bert_len = max_bert_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns a list of sentences for each sample, where each sentence is word features (num_words, feature_dim)\n",
        "        # collate_fn will handle this variable-length data\n",
        "        audio_sample_sentences_raw = [torch.as_tensor(sent_feat, dtype=torch.float32)\n",
        "                                      for sent_feat in self.list_of_audio_sample_data[index]\n",
        "                                      if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0]\n",
        "        # If the processed list is empty (e.g., all sentences are empty or incorrectly formatted), provide a placeholder\n",
        "        if not audio_sample_sentences_raw:\n",
        "             audio_sample_sentences_raw = [torch.zeros((1, _AUDIO_WORD_DIM_CONST), dtype=torch.float32)]\n",
        "\n",
        "\n",
        "        video_sample_sentences_raw = [torch.as_tensor(sent_feat, dtype=torch.float32)\n",
        "                                      for sent_feat in self.list_of_video_sample_data[index]\n",
        "                                      if isinstance(sent_feat, np.ndarray) and sent_feat.ndim == 2 and sent_feat.shape[0] > 0]\n",
        "        if not video_sample_sentences_raw:\n",
        "            video_sample_sentences_raw = [torch.zeros((1, _VIDEO_WORD_DIM_CONST), dtype=torch.float32)]\n",
        "\n",
        "\n",
        "        text_sentences_for_this_sample = self.list_of_text_sentence_lists_per_sample[index]\n",
        "        label = self.list_of_labels[index]\n",
        "\n",
        "        if not text_sentences_for_this_sample:\n",
        "            concatenated_text_for_bert = \"\"\n",
        "        else:\n",
        "            if not all(isinstance(s, str) for s in text_sentences_for_this_sample):\n",
        "                concatenated_text_for_bert = \"\"\n",
        "            else:\n",
        "                concatenated_text_for_bert = \" \".join(text_sentences_for_this_sample)\n",
        "\n",
        "        bert_inputs = self.tokenizer(\n",
        "            concatenated_text_for_bert, add_special_tokens=True, return_attention_mask=True,\n",
        "            max_length=self.max_bert_len, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = bert_inputs[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = bert_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return audio_sample_sentences_raw, video_sample_sentences_raw, input_ids, attention_mask, label\n",
        "\n",
        "# --- New: Custom Collate Function ---\n",
        "def custom_collate_fn(batch):\n",
        "    audio_data_raw, video_data_raw, text_ids_list, text_masks_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Process text and labels (they are already tensors)\n",
        "    batched_text_ids = torch.stack(text_ids_list)\n",
        "    batched_text_masks = torch.stack(text_masks_list)\n",
        "    batched_labels = torch.stack(labels_list)\n",
        "\n",
        "    # Process audio data (list of lists of tensors)\n",
        "    # Goal: Create a tensor of shape (B, S_max, W_max, D_audio) and length information\n",
        "    # S_max: max number of sentences in a sample in the batch, W_max: max number of words in a sentence in the batch\n",
        "\n",
        "    # Audio processing\n",
        "    audio_sample_lengths = [len(sample) for sample in audio_data_raw]\n",
        "    max_sents_audio = max(audio_sample_lengths) if audio_sample_lengths else 0\n",
        "\n",
        "    # Get word counts for each sentence and find the maximum word count\n",
        "    audio_sentence_word_counts_flat = []\n",
        "    for sample in audio_data_raw:\n",
        "        for sentence_tensor in sample:\n",
        "            audio_sentence_word_counts_flat.append(sentence_tensor.shape[0])\n",
        "    max_words_audio = max(audio_sentence_word_counts_flat) if audio_sentence_word_counts_flat else 0\n",
        "\n",
        "    # Create padded audio tensor and length tensors\n",
        "    # padded_features: (batch_size, max_sents, max_words, feat_dim)\n",
        "    # sentence_lengths: (batch_size, max_sents) - records actual word count for each sentence\n",
        "    # sample_lengths: (batch_size) - records actual sentence count for each sample (already obtained as audio_sample_lengths)\n",
        "    padded_audio_features = torch.zeros(len(audio_data_raw), max_sents_audio, max_words_audio, _AUDIO_WORD_DIM_CONST)\n",
        "    audio_sentence_lengths = torch.zeros(len(audio_data_raw), max_sents_audio, dtype=torch.long)\n",
        "\n",
        "    for i, sample in enumerate(audio_data_raw):\n",
        "        for j, sentence_tensor in enumerate(sample):\n",
        "            num_words = sentence_tensor.shape[0]\n",
        "            if num_words > 0: # Only pad if there are words\n",
        "                padded_audio_features[i, j, :num_words, :] = sentence_tensor\n",
        "                audio_sentence_lengths[i, j] = num_words\n",
        "\n",
        "    # Video processing (similar to audio)\n",
        "    video_sample_lengths = [len(sample) for sample in video_data_raw]\n",
        "    max_sents_video = max(video_sample_lengths) if video_sample_lengths else 0\n",
        "    video_sentence_word_counts_flat = []\n",
        "    for sample in video_data_raw:\n",
        "        for sentence_tensor in sample:\n",
        "            video_sentence_word_counts_flat.append(sentence_tensor.shape[0])\n",
        "    max_words_video = max(video_sentence_word_counts_flat) if video_sentence_word_counts_flat else 0\n",
        "\n",
        "    padded_video_features = torch.zeros(len(video_data_raw), max_sents_video, max_words_video, _VIDEO_WORD_DIM_CONST)\n",
        "    video_sentence_lengths = torch.zeros(len(video_data_raw), max_sents_video, dtype=torch.long)\n",
        "\n",
        "    for i, sample in enumerate(video_data_raw):\n",
        "        for j, sentence_tensor in enumerate(sample):\n",
        "            num_words = sentence_tensor.shape[0]\n",
        "            if num_words > 0:\n",
        "                padded_video_features[i, j, :num_words, :] = sentence_tensor\n",
        "                video_sentence_lengths[i, j] = num_words\n",
        "\n",
        "    return (padded_audio_features, torch.tensor(audio_sample_lengths, dtype=torch.long), audio_sentence_lengths,\n",
        "            padded_video_features, torch.tensor(video_sample_lengths, dtype=torch.long), video_sentence_lengths,\n",
        "            batched_text_ids, batched_text_masks, batched_labels)\n",
        "\n",
        "\n",
        "# --- New: Hierarchical LSTM Aggregator (Trainable) ---\n",
        "class HierarchicalLSTMAggregator(nn.Module):\n",
        "    def __init__(self, word_dim, sentence_lstm_hidden_dim, sample_lstm_hidden_dim,\n",
        "                 sentence_lstm_layers=1, sample_lstm_layers=1, dropout_rate=0.3): # Added dropout\n",
        "        super().__init__()\n",
        "        self.sentence_lstm_hidden_dim = sentence_lstm_hidden_dim\n",
        "        self.sample_lstm_hidden_dim = sample_lstm_hidden_dim\n",
        "\n",
        "        self.sentence_lstm = nn.LSTM(word_dim, sentence_lstm_hidden_dim,\n",
        "                                     num_layers=sentence_lstm_layers, batch_first=True,\n",
        "                                     bidirectional=False) # Can be set to True if needed\n",
        "\n",
        "        # If sentence_lstm is bidirectional, input dimension for sample_lstm needs to be multiplied by 2\n",
        "        sample_lstm_input_dim = sentence_lstm_hidden_dim * (2 if self.sentence_lstm.bidirectional else 1)\n",
        "\n",
        "        self.sample_lstm = nn.LSTM(sample_lstm_input_dim, sample_lstm_hidden_dim,\n",
        "                                   num_layers=sample_lstm_layers, batch_first=True,\n",
        "                                   bidirectional=False) # Can be set to True if needed\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, features, sample_lengths, sentence_lengths):\n",
        "        # features: (batch_size, max_sents, max_words, word_dim)\n",
        "        # sample_lengths: (batch_size) - actual number of sentences per sample\n",
        "        # sentence_lengths: (batch_size, max_sents) - actual number of words per sentence\n",
        "\n",
        "        batch_size, max_sents, max_words, _ = features.shape\n",
        "\n",
        "        # 1. Process sentence level\n",
        "        # Merge batch and sents dimensions to pass through LSTM at once\n",
        "        # (B, S, W, D) -> (B*S, W, D)\n",
        "        sents_features = features.view(batch_size * max_sents, max_words, -1)\n",
        "        # (B, S) -> (B*S)\n",
        "        sents_word_lengths = sentence_lengths.view(batch_size * max_sents)\n",
        "\n",
        "        # Filter out sentences with length 0 to avoid pack_padded_sequence error\n",
        "        valid_sents_indices = sents_word_lengths > 0\n",
        "        if not torch.any(valid_sents_indices): # If all sentences are empty\n",
        "            # Return a zero tensor with shape matching sample_lstm output\n",
        "            final_output_dim = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "            return torch.zeros(batch_size, final_output_dim, device=features.device)\n",
        "\n",
        "        sents_features_packed = sents_features[valid_sents_indices]\n",
        "        sents_word_lengths_packed = sents_word_lengths[valid_sents_indices]\n",
        "\n",
        "        # Pack variable length sequences\n",
        "        packed_sents_input = pack_padded_sequence(sents_features_packed, sents_word_lengths_packed.cpu(),\n",
        "                                                  batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sentence LSTM\n",
        "        # h_n_sent: (num_layers * num_directions, B*S_valid, sent_hidden_dim)\n",
        "        _, (h_n_sent, _) = self.sentence_lstm(packed_sents_input)\n",
        "\n",
        "        # Get hidden state of the last time step (for unidirectional LSTM, take the last layer)\n",
        "        # (B*S_valid, sent_hidden_dim)\n",
        "        if self.sentence_lstm.bidirectional:\n",
        "            # Concatenate hidden states of the last time step from both directions of bidirectional LSTM\n",
        "            sentence_embeddings_valid = torch.cat((h_n_sent[-2,:,:], h_n_sent[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            sentence_embeddings_valid = h_n_sent[-1,:,:]\n",
        "\n",
        "        sentence_embeddings_valid = self.dropout(sentence_embeddings_valid)\n",
        "\n",
        "        # Place valid sentence embeddings back to their original positions, use zero vectors for empty sentences\n",
        "        # Output shape: (B*S, sent_hidden_dim_actual)\n",
        "        sent_hidden_dim_actual = sentence_embeddings_valid.shape[-1]\n",
        "        all_sentence_embeddings = torch.zeros(batch_size * max_sents, sent_hidden_dim_actual, device=features.device)\n",
        "        all_sentence_embeddings[valid_sents_indices] = sentence_embeddings_valid\n",
        "\n",
        "        # (B*S, H_sent) -> (B, S, H_sent)\n",
        "        sample_features = all_sentence_embeddings.view(batch_size, max_sents, sent_hidden_dim_actual)\n",
        "\n",
        "        # 2. Process sample level\n",
        "        # Pack variable length sequences (based on actual number of sentences per sample, sample_lengths)\n",
        "        # Filter out samples with length 0\n",
        "        valid_sample_indices = sample_lengths > 0\n",
        "        if not torch.any(valid_sample_indices):\n",
        "            final_output_dim = self.sample_lstm_hidden_dim * (2 if self.sample_lstm.bidirectional else 1)\n",
        "            return torch.zeros(batch_size, final_output_dim, device=features.device)\n",
        "\n",
        "        sample_features_packed_input = sample_features[valid_sample_indices]\n",
        "        sample_lengths_packed = sample_lengths[valid_sample_indices]\n",
        "\n",
        "        packed_sample_input = pack_padded_sequence(sample_features_packed_input, sample_lengths_packed.cpu(),\n",
        "                                                   batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through sample LSTM\n",
        "        # h_n_sample: (num_layers * num_directions, B_valid, sample_hidden_dim)\n",
        "        _, (h_n_sample, _) = self.sample_lstm(packed_sample_input)\n",
        "\n",
        "        # Get hidden state of the last time step\n",
        "        # (B_valid, sample_hidden_dim)\n",
        "        if self.sample_lstm.bidirectional:\n",
        "            sample_embeddings_valid = torch.cat((h_n_sample[-2,:,:], h_n_sample[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            sample_embeddings_valid = h_n_sample[-1,:,:]\n",
        "\n",
        "        sample_embeddings_valid = self.dropout(sample_embeddings_valid)\n",
        "\n",
        "        # Place valid sample embeddings back to their original positions\n",
        "        final_output_dim = sample_embeddings_valid.shape[-1]\n",
        "        final_sample_embeddings = torch.zeros(batch_size, final_output_dim, device=features.device)\n",
        "        final_sample_embeddings[valid_sample_indices] = sample_embeddings_valid\n",
        "\n",
        "        return final_sample_embeddings\n",
        "\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(linear, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.layer2 = nn.Linear(input_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x) * self.layer2(x)\n",
        "        return x\n",
        "\n",
        "class CA_SA(nn.Module):\n",
        "    def __init__(self, dim=32):\n",
        "        super(CA_SA, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.K = nn.Linear(dim, dim, bias=False)\n",
        "        self.V = nn.Linear(dim, dim, bias=False)\n",
        "        self.Q = nn.Linear(dim, dim, bias=False)\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "    def forward(self, feat1, feat2):\n",
        "        K = self.K(feat2)\n",
        "        V = self.V(feat2)\n",
        "        Q = self.Q(feat1)\n",
        "        dots = torch.bmm(Q, K.permute(0, 2, 1))\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.bmm(attn, V)\n",
        "        return out\n",
        "\n",
        "class MultimodalFusionLSTMHead(nn.Module):\n",
        "    def __init__(self, projected_audio_video_dim=1024, # This is the dimension from the trainable projection layer\n",
        "                 bert_hidden_size=768,\n",
        "                 max_bert_len=512,\n",
        "                 lstm_hidden_size=256, # Hidden size of text LSTM\n",
        "                 attention_token_dim=32, num_attention_tokens_per_modal=16, num_classes=2,\n",
        "                 active_modalities=('audio', 'video', 'text')):\n",
        "        super().__init__()\n",
        "        self.n = num_attention_tokens_per_modal\n",
        "        self.attention_token_dim = attention_token_dim\n",
        "        self.max_bert_len = max_bert_len\n",
        "        self.active_modalities = active_modalities\n",
        "        self.expected_feature_dim_for_attention = self.n * self.attention_token_dim # 512\n",
        "\n",
        "        # Audio feature processor: input projected_audio_video_dim (e.g., 1024), output expected_feature_dim_for_attention (e.g., 512)\n",
        "        self.audio_feat_processor = nn.Sequential(\n",
        "            linear(projected_audio_video_dim, 1024),\n",
        "            linear(1024, self.expected_feature_dim_for_attention),\n",
        "            nn.LayerNorm(self.expected_feature_dim_for_attention)\n",
        "        )\n",
        "        self.vision_feat_processor = nn.Sequential(\n",
        "            linear(projected_audio_video_dim, 1024),\n",
        "            linear(1024, self.expected_feature_dim_for_attention),\n",
        "            nn.LayerNorm(self.expected_feature_dim_for_attention)\n",
        "        )\n",
        "        self.text_lstm_processor = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True)\n",
        "        self.text_fc_processor = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            linear(lstm_hidden_size * self.max_bert_len, 1024), # Flatten LSTM output\n",
        "            linear(1024, self.expected_feature_dim_for_attention),\n",
        "            nn.LayerNorm(self.expected_feature_dim_for_attention)\n",
        "        )\n",
        "        self.ZA = CA_SA(dim=attention_token_dim)\n",
        "        self.ZV = CA_SA(dim=attention_token_dim)\n",
        "        self.ZT = CA_SA(dim=attention_token_dim)\n",
        "        self.SA = CA_SA(dim=attention_token_dim)\n",
        "        self.pre = nn.Sequential(\n",
        "            linear(self.expected_feature_dim_for_attention, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_input_projected, vision_input_projected, text_sequence_input_bert):\n",
        "        b = audio_input_projected.shape[0] if audio_input_projected.nelement() > 0 else (vision_input_projected.shape[0] if vision_input_projected.nelement() > 0 else text_sequence_input_bert.shape[0])\n",
        "        device = audio_input_projected.device if audio_input_projected.nelement() > 0 else (vision_input_projected.device if vision_input_projected.nelement() > 0 else text_sequence_input_bert.device)\n",
        "\n",
        "\n",
        "        if 'audio' in self.active_modalities and audio_input_projected.nelement() > 0 :\n",
        "            audio_f = self.audio_feat_processor(audio_input_projected)\n",
        "        else:\n",
        "            audio_f = torch.zeros(b, self.expected_feature_dim_for_attention, device=device)\n",
        "\n",
        "        if 'video' in self.active_modalities and vision_input_projected.nelement() > 0:\n",
        "            vis_f = self.vision_feat_processor(vision_input_projected)\n",
        "        else:\n",
        "            vis_f = torch.zeros(b, self.expected_feature_dim_for_attention, device=device)\n",
        "\n",
        "        if 'text' in self.active_modalities and text_sequence_input_bert.nelement() > 0 :\n",
        "            lstm_output, (h_n, c_n) = self.text_lstm_processor(text_sequence_input_bert)\n",
        "            if lstm_output.shape[1] != self.max_bert_len:\n",
        "                if lstm_output.nelement() == 0 and self.max_bert_len > 0 :\n",
        "                    lstm_output = torch.zeros(b, self.max_bert_len, lstm_output.shape[2] if lstm_output.ndim > 2 else self.text_lstm_processor.hidden_size, device=device)\n",
        "                elif lstm_output.shape[1] != self.max_bert_len :\n",
        "                    raise ValueError(f\"LSTM output sequence length {lstm_output.shape[1]} does not match expected max_bert_len {self.max_bert_len}\")\n",
        "            text_f_processed = lstm_output.reshape(b, -1)\n",
        "            text_f = self.text_fc_processor(text_f_processed)\n",
        "        else:\n",
        "            text_f = torch.zeros(b, self.expected_feature_dim_for_attention, device=device)\n",
        "\n",
        "        audio_for_attn = audio_f.view(b, self.n, self.attention_token_dim)\n",
        "        vis_for_attn = vis_f.view(b, self.n, self.attention_token_dim)\n",
        "        sub_for_attn = text_f.view(b, self.n, self.attention_token_dim)\n",
        "        active_processed_features_for_query = []\n",
        "        if 'audio' in self.active_modalities:\n",
        "            active_processed_features_for_query.append(F.normalize(audio_f, dim=1))\n",
        "        if 'video' in self.active_modalities:\n",
        "            active_processed_features_for_query.append(F.normalize(vis_f, dim=1))\n",
        "        if 'text' in self.active_modalities:\n",
        "            active_processed_features_for_query.append(F.normalize(text_f, dim=1))\n",
        "\n",
        "        if not active_processed_features_for_query:\n",
        "            final_feat_to_classify = torch.zeros(b, self.expected_feature_dim_for_attention, device=device)\n",
        "        else:\n",
        "            z_feat_concatenated = torch.cat(active_processed_features_for_query, dim=1)\n",
        "            num_active_modalities_for_query = len(active_processed_features_for_query)\n",
        "            z_feat_for_query = z_feat_concatenated.view(b, num_active_modalities_for_query * self.n, self.attention_token_dim)\n",
        "            feat_ZA_res = self.ZA(z_feat_for_query, audio_for_attn) if 'audio' in self.active_modalities else torch.zeros_like(z_feat_for_query)\n",
        "            feat_ZV_res = self.ZV(z_feat_for_query, vis_for_attn)   if 'video' in self.active_modalities else torch.zeros_like(z_feat_for_query)\n",
        "            feat_ZT_res = self.ZT(z_feat_for_query, sub_for_attn)   if 'text'  in self.active_modalities else torch.zeros_like(z_feat_for_query)\n",
        "            feat_after_ca = feat_ZA_res + feat_ZV_res + feat_ZT_res\n",
        "            feat_after_sa = self.SA(feat_after_ca, feat_after_ca) + feat_after_ca\n",
        "            if num_active_modalities_for_query > 0:\n",
        "                chunks = feat_after_sa.chunk(num_active_modalities_for_query, dim=1)\n",
        "                final_feat_to_classify_list = [chunk.reshape(b, -1) for chunk in chunks]\n",
        "                final_feat_to_classify = torch.stack(final_feat_to_classify_list).sum(dim=0)\n",
        "                if final_feat_to_classify.shape[1] != self.expected_feature_dim_for_attention:\n",
        "                        # This case should theoretically not happen, as reshape(-1) will become n * attention_token_dim\n",
        "                        # If it happens, a linear layer might be needed to adjust dimensions\n",
        "                        # print(f\"Warning: Dimension mismatch before final classifier. Expected {self.expected_feature_dim_for_attention}, got {final_feat_to_classify.shape[1]}\")\n",
        "                        # As a temporary solution, if dimensions don't match, a zero vector could be returned or an error raised, but this indicates preceding logic might need adjustment\n",
        "                        # raise ValueError(\"Dimension mismatch\")\n",
        "                        pass # Assume dimensions are correct\n",
        "            else:\n",
        "                final_feat_to_classify = torch.zeros(b, self.expected_feature_dim_for_attention, device=device)\n",
        "        prob_logits = self.pre(final_feat_to_classify)\n",
        "        return audio_f, vis_f, text_f, prob_logits\n",
        "\n",
        "class ContrastiveLossELI5(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    def forward(self, emb_i, emb_j):\n",
        "        batch_size = emb_i.shape[0]\n",
        "        if batch_size <= 1: # Contrastive loss requires at least 2 samples\n",
        "            return torch.tensor(0.0, device=emb_i.device, requires_grad=True)\n",
        "        z_i = F.normalize(emb_i, dim=1)\n",
        "        z_j = F.normalize(emb_j, dim=1)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "        labels_i_to_j = torch.arange(batch_size, device=emb_i.device) + batch_size\n",
        "        labels_j_to_i = torch.arange(batch_size, device=emb_i.device)\n",
        "\n",
        "        # Mask out self-similarity on the diagonal, avoid calculating similarity of a sample with itself as a positive sample\n",
        "        # For NT-Xent loss, the diagonal (self vs self) usually doesn't directly participate in loss calculation,\n",
        "        # because positive samples are corresponding samples from different augmented views.\n",
        "        # The logits for CrossEntropyLoss are similarity scores, and labels point to the position of positive samples.\n",
        "        # In similarity_matrix, (i, i+bs) and (i+bs, i) are positive pairs.\n",
        "        loss_i = self.criterion(similarity_matrix[:batch_size], labels_i_to_j)\n",
        "        loss_j = self.criterion(similarity_matrix[batch_size:], labels_j_to_i)\n",
        "        return (loss_i + loss_j) / 2.0\n",
        "\n",
        "# --- Modify train and validate/test functions to include new hierarchical LSTM modules ---\n",
        "def train(audio_hier_lstm, video_hier_lstm, # New hierarchical LSTM modules\n",
        "          audio_projector, video_projector, text_feature_extractor,\n",
        "          clf_head, data_loader, optimizer, scheduler,bce_criterion, contrastive_loss_fn, device, epoch, num_epochs, contrastive_loss_weight,\n",
        "          current_modality_config, tokenizer_for_padding):\n",
        "\n",
        "    audio_hier_lstm.train() # Set to training mode\n",
        "    video_hier_lstm.train() # Set to training mode\n",
        "    audio_projector.train()\n",
        "    video_projector.train()\n",
        "    text_feature_extractor.train() # Although usually frozen, keep mode consistent\n",
        "    clf_head.train()\n",
        "\n",
        "    total_bce_loss = 0\n",
        "    total_simclr_loss = 0\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train {current_modality_config['name']}]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Get data from collate_fn\n",
        "        (padded_audio_features, audio_sample_lengths, audio_sentence_lengths,\n",
        "         padded_video_features, video_sample_lengths, video_sentence_lengths,\n",
        "         bert_input_ids, bert_attention_mask, label_data) = batch\n",
        "\n",
        "        current_batch_size = padded_audio_features.shape[0]\n",
        "        if current_batch_size == 0:\n",
        "            continue\n",
        "\n",
        "        # Process data according to modality configuration (zero out)\n",
        "        if not current_modality_config.get('audio', False):\n",
        "            padded_audio_features = torch.zeros_like(padded_audio_features)\n",
        "            audio_sample_lengths = torch.zeros_like(audio_sample_lengths)\n",
        "            audio_sentence_lengths = torch.zeros_like(audio_sentence_lengths)\n",
        "        if not current_modality_config.get('video', False):\n",
        "            padded_video_features = torch.zeros_like(padded_video_features)\n",
        "            video_sample_lengths = torch.zeros_like(video_sample_lengths)\n",
        "            video_sentence_lengths = torch.zeros_like(video_sentence_lengths)\n",
        "        if not current_modality_config.get('text', False):\n",
        "            pad_token_id = tokenizer_for_padding.pad_token_id if tokenizer_for_padding.pad_token_id is not None else 0\n",
        "            bert_input_ids = torch.full_like(bert_input_ids, pad_token_id)\n",
        "            bert_attention_mask = torch.zeros_like(bert_attention_mask)\n",
        "\n",
        "        # Move data to device\n",
        "        padded_audio_features = padded_audio_features.to(device)\n",
        "        audio_sample_lengths = audio_sample_lengths.to(device)\n",
        "        audio_sentence_lengths = audio_sentence_lengths.to(device)\n",
        "        padded_video_features = padded_video_features.to(device)\n",
        "        video_sample_lengths = video_sample_lengths.to(device)\n",
        "        video_sentence_lengths = video_sentence_lengths.to(device)\n",
        "        bert_input_ids = bert_input_ids.to(device)\n",
        "        bert_attention_mask = bert_attention_mask.to(device)\n",
        "        label_data = label_data.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1. Get audio/video sample vectors through hierarchical LSTM\n",
        "        # Only calculate if modality is active and there is actual content, otherwise use zero vectors\n",
        "        audio_sample_vectors = torch.zeros(current_batch_size, SAMPLE_LSTM_HIDDEN_DIM_CONFIG, device=device) # Assume fixed dimension\n",
        "        if current_modality_config.get('audio', False) and torch.any(audio_sample_lengths > 0):\n",
        "            audio_sample_vectors = audio_hier_lstm(padded_audio_features, audio_sample_lengths, audio_sentence_lengths)\n",
        "\n",
        "        video_sample_vectors = torch.zeros(current_batch_size, SAMPLE_LSTM_HIDDEN_DIM_CONFIG, device=device) # Assume fixed dimension\n",
        "        if current_modality_config.get('video', False) and torch.any(video_sample_lengths > 0):\n",
        "            video_sample_vectors = video_hier_lstm(padded_video_features, video_sample_lengths, video_sentence_lengths)\n",
        "\n",
        "        # 2. Through projection layer\n",
        "        projected_audio = audio_projector(audio_sample_vectors)\n",
        "        projected_video = video_projector(video_sample_vectors)\n",
        "\n",
        "        # 3. BERT Features\n",
        "        # BERT parameters are frozen, so no gradients will flow back here if requires_grad=False for BERT\n",
        "        bert_outputs = text_feature_extractor(input_ids=bert_input_ids, attention_mask=bert_attention_mask)\n",
        "        text_sequence_features = bert_outputs.last_hidden_state.to(torch.float32)\n",
        "\n",
        "        # 4. Fusion head\n",
        "        feat_audio, feat_vision, feat_text, prob_logits = clf_head(projected_audio, projected_video, text_sequence_features)\n",
        "\n",
        "        bce_loss = bce_criterion(prob_logits, label_data)\n",
        "        calculated_simclr_loss_sum = torch.tensor(0.0, device=device)\n",
        "        audio_active = current_modality_config.get('audio', False) and feat_audio.nelement() > 0 and torch.any(audio_sample_lengths > 0)\n",
        "        video_active = current_modality_config.get('video', False) and feat_vision.nelement() > 0 and torch.any(video_sample_lengths > 0)\n",
        "        text_active = current_modality_config.get('text', False) and feat_text.nelement() > 0\n",
        "\n",
        "        if current_batch_size > 1: # Contrastive loss requires at least 2 samples\n",
        "            if audio_active and video_active:\n",
        "                calculated_simclr_loss_sum += contrastive_loss_fn(feat_audio, feat_vision)\n",
        "            if audio_active and text_active:\n",
        "                calculated_simclr_loss_sum += contrastive_loss_fn(feat_audio, feat_text)\n",
        "            if video_active and text_active:\n",
        "                calculated_simclr_loss_sum += contrastive_loss_fn(feat_vision, feat_text)\n",
        "\n",
        "        current_loss = bce_loss + contrastive_loss_weight * calculated_simclr_loss_sum\n",
        "        current_loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_bce_loss += bce_loss.item()\n",
        "        total_simclr_loss += calculated_simclr_loss_sum.item()\n",
        "        total_loss += current_loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{current_loss.item():.4f}\", bce=f\"{bce_loss.item():.4f}\", simclr=f\"{calculated_simclr_loss_sum.item():.4f}\")\n",
        "\n",
        "    if not progress_bar.iterable or len(progress_bar.iterable) == 0:\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train: DataLoader is empty or was not processed.\")\n",
        "        return\n",
        "    if len(data_loader) > 0:\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "        avg_simclr_loss = total_simclr_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train Avg Loss: {avg_loss:.4f}, BCE: {avg_bce_loss:.4f}, SimCLR: {avg_simclr_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1} ({current_modality_config['name']}) Train: DataLoader was empty. No average losses to report.\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score # Ensure this import is present at the top of your file\n",
        "from tqdm import tqdm # Ensure this import is present\n",
        "import torch # Ensure this import is present\n",
        "\n",
        "# Constants used in the function, ensure they are defined in your script scope\n",
        "# SAMPLE_LSTM_HIDDEN_DIM_CONFIG = 512 # Example value\n",
        "\n",
        "def validate_or_test(audio_hier_lstm, video_hier_lstm,\n",
        "                     audio_projector, video_projector, text_feature_extractor,\n",
        "                     clf_head, data_loader, bce_criterion, device, epoch, num_epochs,\n",
        "                     current_modality_config, tokenizer_for_padding, mode=\"Val\"):\n",
        "\n",
        "    audio_hier_lstm.eval()\n",
        "    video_hier_lstm.eval()\n",
        "    audio_projector.eval()\n",
        "    video_projector.eval()\n",
        "    text_feature_extractor.eval()\n",
        "    clf_head.eval()\n",
        "\n",
        "    total_bce_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    desc = f\"Epoch {epoch+1}/{num_epochs} [{mode} {current_modality_config['name']}]\"\n",
        "    if mode == \"Test\" and epoch is None : # For final test run not tied to an epoch\n",
        "        desc = f\"Final Test [{current_modality_config['name']}]\"\n",
        "    elif mode == \"Test\": # For test run after a specific epoch\n",
        "        desc = f\"Test after Epoch {epoch+1} [{current_modality_config['name']}]\"\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=desc, leave=False)):\n",
        "            (padded_audio_features, audio_sample_lengths, audio_sentence_lengths,\n",
        "             padded_video_features, video_sample_lengths, video_sentence_lengths,\n",
        "             bert_input_ids, bert_attention_mask, label_data) = batch\n",
        "\n",
        "            current_batch_size = padded_audio_features.shape[0]\n",
        "            if current_batch_size == 0:\n",
        "                continue\n",
        "\n",
        "            if not current_modality_config.get('audio', False):\n",
        "                padded_audio_features.zero_()\n",
        "                audio_sample_lengths.zero_()\n",
        "                audio_sentence_lengths.zero_()\n",
        "            if not current_modality_config.get('video', False):\n",
        "                padded_video_features.zero_()\n",
        "                video_sample_lengths.zero_()\n",
        "                video_sentence_lengths.zero_()\n",
        "            if not current_modality_config.get('text', False):\n",
        "                pad_token_id = tokenizer_for_padding.pad_token_id if tokenizer_for_padding.pad_token_id is not None else 0\n",
        "                bert_input_ids.fill_(pad_token_id)\n",
        "                bert_attention_mask.zero_()\n",
        "\n",
        "            padded_audio_features = padded_audio_features.to(device)\n",
        "            audio_sample_lengths = audio_sample_lengths.to(device)\n",
        "            audio_sentence_lengths = audio_sentence_lengths.to(device)\n",
        "            padded_video_features = padded_video_features.to(device)\n",
        "            video_sample_lengths = video_sample_lengths.to(device)\n",
        "            video_sentence_lengths = video_sentence_lengths.to(device)\n",
        "            bert_input_ids = bert_input_ids.to(device)\n",
        "            bert_attention_mask = bert_attention_mask.to(device)\n",
        "            label_data = label_data.to(device).long()\n",
        "\n",
        "            # Assuming SAMPLE_LSTM_HIDDEN_DIM_CONFIG is globally defined or passed correctly\n",
        "            # And HierarchicalLSTMAggregator has sample_lstm_bidirectional attribute\n",
        "            audio_sample_vec_dim = SAMPLE_LSTM_HIDDEN_DIM_CONFIG * (2 if hasattr(audio_hier_lstm, 'sample_lstm_bidirectional') and audio_hier_lstm.sample_lstm_bidirectional else 1)\n",
        "            video_sample_vec_dim = SAMPLE_LSTM_HIDDEN_DIM_CONFIG * (2 if hasattr(video_hier_lstm, 'sample_lstm_bidirectional') and video_hier_lstm.sample_lstm_bidirectional else 1)\n",
        "\n",
        "\n",
        "            audio_sample_vectors = torch.zeros(current_batch_size, audio_sample_vec_dim, device=device)\n",
        "            if current_modality_config.get('audio', False) and torch.any(audio_sample_lengths > 0):\n",
        "                audio_sample_vectors = audio_hier_lstm(padded_audio_features, audio_sample_lengths, audio_sentence_lengths)\n",
        "\n",
        "            video_sample_vectors = torch.zeros(current_batch_size, video_sample_vec_dim, device=device)\n",
        "            if current_modality_config.get('video', False) and torch.any(video_sample_lengths > 0):\n",
        "                video_sample_vectors = video_hier_lstm(padded_video_features, video_sample_lengths, video_sentence_lengths)\n",
        "\n",
        "            projected_audio = audio_projector(audio_sample_vectors)\n",
        "            projected_video = video_projector(video_sample_vectors)\n",
        "            bert_outputs = text_feature_extractor(input_ids=bert_input_ids, attention_mask=bert_attention_mask)\n",
        "            text_sequence_features = bert_outputs.last_hidden_state.to(torch.float32)\n",
        "            _, _, _, prob_logits = clf_head(projected_audio, projected_video, text_sequence_features)\n",
        "\n",
        "            if prob_logits.shape[0] == 0: # If clf_head returns empty logits for some reason\n",
        "                continue\n",
        "\n",
        "            bce_loss = bce_criterion(prob_logits, label_data)\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            preds = torch.argmax(prob_logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(label_data.cpu().numpy())\n",
        "\n",
        "    epoch_display = epoch + 1 if epoch is not None else 'N/A'\n",
        "\n",
        "    if len(data_loader) == 0 or not all_labels: # Check if all_labels is empty\n",
        "        print(f\"Epoch {epoch_display} ({current_modality_config['name']}) {mode}: DataLoader or collected labels are empty. Cannot compute metrics.\")\n",
        "        if mode == \"Val\":\n",
        "            return 0.0  # Accuracy\n",
        "        else: # Test mode\n",
        "            return 0.0, 0.0, 0.0  # BCE Loss, Accuracy, F1 Score\n",
        "\n",
        "    avg_bce_loss = total_bce_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    # Calculate F1 score, typically for binary classification, use average='binary'.\n",
        "    # zero_division=0 handles cases where precision or recall is 0 for a class, resulting in F1=0.\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch_display} ({current_modality_config['name']}) {mode} Avg BCE: {avg_bce_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    if mode == \"Val\":\n",
        "        return accuracy # For validation, return only accuracy as per original structure\n",
        "    else: # Test mode\n",
        "        return avg_bce_loss, accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_folds = load_pickle(data_folds_path)\n",
        "language_sdk = load_pickle(language_file)\n",
        "covarep_sdk = load_pickle(covarep_file)\n",
        "openface_sdk = load_pickle(openface_file)\n",
        "humor_label_sdk = load_pickle(humor_label_file)\n",
        "print(\"Raw data loading complete.\")\n",
        "\n",
        "train_ids = data_folds['train']\n",
        "dev_ids = data_folds['dev']\n",
        "test_ids = data_folds['test']\n",
        "\n",
        "print(\"Starting to extract features and labels...\")\n",
        "(train_ps_orig, train_cs_orig, train_cvp_p_orig, train_cvp_c_orig,\n",
        "  train_of_p_orig, train_of_c_orig, train_labels) = extract_features_and_labels(\n",
        "      train_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "(dev_ps_orig, dev_cs_orig, dev_cvp_p_orig, dev_cvp_c_orig,\n",
        "   dev_of_p_orig, dev_of_c_orig, dev_labels) = extract_features_and_labels(\n",
        "     dev_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "(test_ps_orig, test_cs_orig, test_cvp_p_orig, test_cvp_c_orig,\n",
        " test_of_p_orig, test_of_c_orig, test_labels) = extract_features_and_labels(\n",
        "      test_ids, language_sdk, covarep_sdk, openface_sdk, humor_label_sdk)\n",
        "print(\"Feature and label extraction complete.\")\n",
        "\n",
        "print(\"Starting to concatenate multimodal data...\")\n",
        "concatenated_train_audio, concatenated_train_video, concatenated_train_text = concatenate_multimodal_data(\n",
        "        train_cvp_c_orig, train_of_c_orig, train_cs_orig, train_cvp_p_orig, train_of_p_orig, train_ps_orig)\n",
        "concatenated_dev_audio, concatenated_dev_video, concatenated_dev_text = concatenate_multimodal_data(\n",
        "        dev_cvp_c_orig, dev_of_c_orig, dev_cs_orig, dev_cvp_p_orig, dev_of_p_orig, dev_ps_orig)\n",
        "concatenated_test_audio, concatenated_test_video, concatenated_test_text = concatenate_multimodal_data(\n",
        "        test_cvp_c_orig, test_of_c_orig, test_cs_orig, test_cvp_p_orig, test_of_p_orig, test_ps_orig)\n",
        "print(\"Multimodal data concatenation complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdWhItbHFuE3",
        "outputId": "3ed8b1e4-52cf-4563-ce44-8462fcb4849a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data loading complete.\n",
            "Starting to extract features and labels...\n",
            "Feature and label extraction complete.\n",
            "Starting to concatenate multimodal data...\n",
            "Multimodal data concatenation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    MAX_BERT_LEN = 512\n",
        "    # LSTM_HIDDEN_SIZE is now used for text LSTM, audio/video hierarchical LSTMs have their own config\n",
        "    TEXT_LSTM_HIDDEN_SIZE = 256\n",
        "    # Audio/video projection layer input dimension, now is the output dimension of HierarchicalLSTMAggregator\n",
        "    HIER_LSTM_OUTPUT_DIM = SAMPLE_LSTM_HIDDEN_DIM_CONFIG # Use the constant defined above: 512\n",
        "    PROJECTOR_OUTPUT_DIM = 1024 # Projection layer output / fusion head input dimension (unchanged)\n",
        "\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    NUM_CLASSES = 2\n",
        "    BATCH_SIZE = 16 # Note: if memory is insufficient, BATCH_SIZE can be reduced\n",
        "    LEARNING_RATE = 5e-5\n",
        "    BERT_LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 4\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "    HIER_LSTM_DROPOUT = 0.3 # Dropout for hierarchical LSTM\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"MAX_BERT_LEN set to: {MAX_BERT_LEN}\")\n",
        "    print(f\"Text LSTM_HIDDEN_SIZE set to: {TEXT_LSTM_HIDDEN_SIZE}\")\n",
        "    print(f\"Hierarchical LSTM output dim (projector input): {HIER_LSTM_OUTPUT_DIM}\")\n",
        "    print(f\"Projector output dim (fusion head AV input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"ATTENTION_TOKEN_DIM set to: {ATTENTION_TOKEN_DIM}\")\n",
        "    print(f\"NUM_ATTENTION_TOKENS_PER_MODAL set to: {NUM_ATTENTION_TOKENS_PER_MODAL} (processed modality feature dimension: {NUM_ATTENTION_TOKENS_PER_MODAL*ATTENTION_TOKEN_DIM})\")\n",
        "\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AVT', 'audio': True,  'video': True,  'text': True},\n",
        "        # Can add other configurations for testing\n",
        "        # {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        # {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    print(\"Initializing BERT model and tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    bert_feature_extractor_global = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = bert_feature_extractor_global.config.hidden_size\n",
        "\n",
        "    print(\"Freezing BERT parameters...\") # BERT parameters will not be updated\n",
        "    for param in bert_feature_extractor_global.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"BERT parameters frozen.\")\n",
        "\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = CustomFeatureDataset(\n",
        "        concatenated_train_audio, concatenated_train_video, concatenated_train_text, train_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDataset(\n",
        "        concatenated_dev_audio, concatenated_dev_video, concatenated_dev_text, dev_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    test_dataset = CustomFeatureDataset(\n",
        "        concatenated_test_audio, concatenated_test_video, concatenated_test_text, test_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    print(\"Creating data loaders with custom collate_fn...\")\n",
        "    # DataLoader now uses custom_collate_fn\n",
        "    # drop_last=True might be more stable for training contrastive loss, avoiding last batch being too small\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True if BATCH_SIZE > 1 else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "\n",
        "    for config_idx, model_config in enumerate(modality_configurations):\n",
        "        config_name = model_config['name']\n",
        "        active_modalities_tuple = tuple(m for m in ['audio', 'video', 'text'] if model_config[m])\n",
        "        print(f\"\\n--- [{config_idx+1}/{len(modality_configurations)}] Starting processing for model configuration: {config_name} (Active: {active_modalities_tuple}) ---\")\n",
        "\n",
        "        print(f\"Initializing model components for configuration {config_name}...\")\n",
        "        # 1. Initialize trainable hierarchical LSTM aggregators\n",
        "        current_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_AUDIO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "        current_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 2. Initialize projection layers (input dimension is the output dimension of hierarchical LSTM)\n",
        "        current_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "        current_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "        # 3. Initialize fusion head (input dimension is the output dimension of projection layers)\n",
        "        current_clf_head = MultimodalFusionLSTMHead(\n",
        "            projected_audio_video_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len=MAX_BERT_LEN,\n",
        "            lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE, # Text LSTM\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            num_classes=NUM_CLASSES,\n",
        "            active_modalities=active_modalities_tuple\n",
        "        ).to(DEVICE)\n",
        "        print(f\"Model components for {config_name} initialized.\")\n",
        "\n",
        "        # Parameters for BERT will have a specific learning rate, other parameters will use the main LEARNING_RATE\n",
        "        # Since BERT parameters have requires_grad=False, they won't be updated by the optimizer,\n",
        "        # but they are listed here for completeness or if fine-tuning BERT was an option.\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': bert_feature_extractor_global.parameters(), 'lr': BERT_LEARNING_RATE}, # BERT params (frozen)\n",
        "            # Group all other trainable parameters\n",
        "            {'params': list(current_audio_hier_lstm.parameters()) + \\\n",
        "                       list(current_video_hier_lstm.parameters()) + \\\n",
        "                       list(current_audio_projector.parameters()) + \\\n",
        "                       list(current_video_projector.parameters()) + \\\n",
        "                       list(current_clf_head.parameters()), 'lr': LEARNING_RATE} # Main learning rate for these\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters) # Default lr=LEARNING_RATE will be overridden for specific groups\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs, Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_model_state_for_config = {\n",
        "            'epoch': 0,\n",
        "            'audio_hier_lstm_state_dict': None, 'video_hier_lstm_state_dict': None,\n",
        "            'audio_projector_state_dict': None, 'video_projector_state_dict': None,\n",
        "            'bert_state_dict': None,  # Added this line\n",
        "            'clf_head_state_dict': None, 'best_val_accuracy': 0.0\n",
        "        }\n",
        "        num_training_steps_per_epoch = len(train_loader)\n",
        "        if num_training_steps_per_epoch == 0 and NUM_EPOCHS > 0 : # Handle case where train_loader is empty\n",
        "            print(f\"Warning: train_loader for {config_name} is empty. Scheduler will not be effective.\")\n",
        "            scheduler = None # Or do not create scheduler\n",
        "        elif NUM_EPOCHS > 0:\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1) # E.g., 10% of steps for warmup\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader is empty. Aborting training for {config_name}.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train(\n",
        "                    current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                    current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                    train_loader, optimizer, scheduler, bce_criterion, contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS, CONTRASTIVE_LOSS_WEIGHT,\n",
        "                    model_config, bert_tokenizer_global\n",
        "                )\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy = validate_or_test(\n",
        "                        current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                        current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                        val_loader, bce_criterion, DEVICE, epoch, NUM_EPOCHS,\n",
        "                        model_config, bert_tokenizer_global, mode=\"Val\"\n",
        "                    )\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f}.\")\n",
        "                        best_model_state_for_config['epoch'] = epoch\n",
        "                        best_model_state_for_config['audio_hier_lstm_state_dict'] = copy.deepcopy(current_audio_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['video_hier_lstm_state_dict'] = copy.deepcopy(current_video_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['audio_projector_state_dict'] = copy.deepcopy(current_audio_projector.state_dict())\n",
        "                        best_model_state_for_config['video_projector_state_dict'] = copy.deepcopy(current_video_projector.state_dict())\n",
        "                        best_model_state_for_config['clf_head_state_dict'] = copy.deepcopy(current_clf_head.state_dict())\n",
        "                        best_model_state_for_config['best_val_accuracy'] = best_val_accuracy_for_config\n",
        "                        best_model_state_for_config['bert_state_dict'] = copy.deepcopy(bert_feature_extractor_global.state_dict()) # Save BERT state too\n",
        "\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for configuration {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (at epoch {best_model_state_for_config['epoch']+1})\")\n",
        "\n",
        "        print(f\"\\nStarting testing phase for configuration {config_name}...\")\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader is empty. Skipping testing for {config_name}.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'test_acc': 0.0, 'test_loss': 0.0, 'best_epoch': best_model_state_for_config['epoch']+1 if best_val_accuracy_for_config > 0 else 'N/A'}\n",
        "        else:\n",
        "            # Re-initialize models for testing and load best weights\n",
        "            test_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_AUDIO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "            test_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_VIDEO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "\n",
        "            test_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "            test_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "            test_clf_head = MultimodalFusionLSTMHead(\n",
        "                projected_audio_video_dim=PROJECTOR_OUTPUT_DIM, bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "                max_bert_len=MAX_BERT_LEN, lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE,\n",
        "                attention_token_dim=ATTENTION_TOKEN_DIM, num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "                num_classes=NUM_CLASSES, active_modalities=active_modalities_tuple).to(DEVICE)\n",
        "\n",
        "            # Initialize a separate BERT model for testing to load its specific state\n",
        "            test_bert = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "            # BERT parameters for test_bert should also be frozen if they were during training\n",
        "            for param in test_bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            epoch_for_log = None\n",
        "            if best_model_state_for_config['clf_head_state_dict'] is not None:\n",
        "                test_audio_hier_lstm.load_state_dict(best_model_state_for_config['audio_hier_lstm_state_dict'])\n",
        "                test_video_hier_lstm.load_state_dict(best_model_state_for_config['video_hier_lstm_state_dict'])\n",
        "                test_audio_projector.load_state_dict(best_model_state_for_config['audio_projector_state_dict'])\n",
        "                test_video_projector.load_state_dict(best_model_state_for_config['video_projector_state_dict'])\n",
        "                test_clf_head.load_state_dict(best_model_state_for_config['clf_head_state_dict'])\n",
        "                if best_model_state_for_config['bert_state_dict'] is not None:\n",
        "                     test_bert.load_state_dict(best_model_state_for_config['bert_state_dict'])\n",
        "                print(f\"Loaded best model from Epoch {best_model_state_for_config['epoch']+1} ({config_name}) for testing.\")\n",
        "                epoch_for_log = best_model_state_for_config['epoch']\n",
        "            else:\n",
        "                print(f\"No best validation model state found for {config_name}. Using final model from training for testing.\")\n",
        "                test_audio_hier_lstm.load_state_dict(current_audio_hier_lstm.state_dict())\n",
        "                test_video_hier_lstm.load_state_dict(current_video_hier_lstm.state_dict())\n",
        "                test_audio_projector.load_state_dict(current_audio_projector.state_dict())\n",
        "                test_video_projector.load_state_dict(current_video_projector.state_dict())\n",
        "                test_clf_head.load_state_dict(current_clf_head.state_dict())\n",
        "                test_bert.load_state_dict(bert_feature_extractor_global.state_dict()) # Use the state of the global BERT\n",
        "                epoch_for_log = NUM_EPOCHS - 1 if NUM_EPOCHS > 0 else None\n",
        "\n",
        "            test_loss, test_accuracy, test_f1= validate_or_test(\n",
        "                test_audio_hier_lstm,\n",
        "                test_video_hier_lstm,\n",
        "                test_audio_projector,\n",
        "                test_video_projector,\n",
        "                test_bert, # Pass the test_bert instance with loaded weights\n",
        "                test_clf_head,\n",
        "                test_loader,\n",
        "                bce_criterion,\n",
        "                DEVICE,\n",
        "                epoch=epoch_for_log,\n",
        "                num_epochs=NUM_EPOCHS, # or best_model_state_for_config['epoch']+1\n",
        "                current_modality_config=model_config,\n",
        "                tokenizer_for_padding=bert_tokenizer_global,\n",
        "                mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for configuration {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config,\n",
        "                'test_acc': test_accuracy, 'test_loss': test_loss,\n",
        "                'test_f1': test_f1,\n",
        "                'best_epoch': best_model_state_for_config['epoch']+1 if best_model_state_for_config['clf_head_state_dict'] is not None else (NUM_EPOCHS if NUM_EPOCHS > 0 else 'N/A')\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy (Epoch {results['best_epoch']}): {results['val_acc']:.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results['test_f1']:.4f}\")\n",
        "        print(f\"  Test Set Accuracy: {results['test_acc']:.4f}\")\n",
        "        print(f\"  Test Set Loss: {results['test_loss']:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a0cb7e9084634f10a3d31a4360b5e370",
            "cac396aef2424601b87a6fb29b0b65a3",
            "d7316e7a391c405c8790dc31bc2fad9f",
            "3a853dad383e41a7b274377205763274",
            "5ab007aecce44b2dac2923432587cfde",
            "0526f919f08e43ccb23ed512609311cc",
            "7a43439fd77047059d8bca6ad6ec2fd5",
            "232dae75cf1b46718c4cb7aba34795c5",
            "0a05209e107b40a0b63646e9d4285052",
            "b5b7e58894cc4cd587c70a7b46b6cac7",
            "66023d065d2d4affa1a66244f4061c71",
            "efe56728c62147b5af4eec8cfd4f069c",
            "88f7193094e44df88d670f6d26cc551b",
            "207248e7f77e42d28e5a9bbfac4a7064",
            "bea8d481d9984082a8f14fbeadb0d521",
            "2d2a4b02690c4f90a9cfa73b0a7ed250",
            "a139747606714a84829270b1a276573c",
            "0f02bd73e91c45539a972f99bc72caa0",
            "1c8a0a8cdd964eeeb61e4b4b9dda6e37",
            "01fe332c6f4e4dfaad57e5a6c9090f6d",
            "bd47a81344944b4985b76e22c873ecf3",
            "646f3d0e247142de9890c91c6125b74f",
            "0be11616d52c416fbf66de0058803813",
            "97ec601c610146fe9bff6012e460884a",
            "67a7f81647284832aceaa37fec45e172",
            "57915fbb1d64499dbe4b2b4e6a7839ce",
            "9daa31d568de4416947c5c784f2f4997",
            "84c862485b7449bd8ffe2655bff3f053",
            "665e40ca60594a4eaeec719f12392d1f",
            "72446547dd0d460eb0be1c7b54c0ff2e",
            "6a618e9e3004447e964e87b2ae89cfed",
            "c942f6ebe781455ea8c02fae0925879c",
            "27e5fda0b4534609a25e31b206f3e9bd",
            "20e6c9df99a947518c5cba0226a7ebc4",
            "9ad63f0b36154aeeafb4d28fc9ebcade",
            "8d4af11ceb984e3799fa26e6fdd73d9a",
            "51e4620136da4f0089c305338b3e4925",
            "d3efc42f8a984fe8973e25cdd814d1f9",
            "2efd85fa3fcb4261a19546892ad5c330",
            "ad978cc6b6b34ae0ad2b300a36c455ae",
            "af58486e62e24602a85c80857ce9d492",
            "c85aa58ae3034353aa3112f188c1392a",
            "ebed43d64316477ba9713a9726d92aff",
            "86ea3b6bc2cd424b98f236752bcafe52",
            "76f7f91a579445b0aa2c6dc38f0a197f",
            "55a3fab26ac541f0a2744c34c6f79a25",
            "bc22ad6c81034bf698dad2bf77f65c97",
            "389f6323fe694d289cd3a6ada5139c7f",
            "8074991d595e468c924fad7851de02f6",
            "64d0c158c86f432291bd55408dc42a7e",
            "bcbdad6dbb7e4b3b837ecad1e0cb8e84",
            "d7f414cd74c948629fc01cc77c836b24",
            "2bfcd93f374f447b8d88ed8c0465a580",
            "23b9939ac2db404eada3ec478ef168e0",
            "27f01ff9898c452aa95d40c538952476"
          ]
        },
        "id": "7NYGkOfiGTTb",
        "outputId": "a7118377-8a55-41f4-8f8f-9ebdf717b416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "MAX_BERT_LEN set to: 512\n",
            "Text LSTM_HIDDEN_SIZE set to: 256\n",
            "Hierarchical LSTM output dim (projector input): 512\n",
            "Projector output dim (fusion head AV input): 1024\n",
            "ATTENTION_TOKEN_DIM set to: 32\n",
            "NUM_ATTENTION_TOKENS_PER_MODAL set to: 16 (processed modality feature dimension: 512)\n",
            "Initializing BERT model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0cb7e9084634f10a3d31a4360b5e370"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efe56728c62147b5af4eec8cfd4f069c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be11616d52c416fbf66de0058803813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20e6c9df99a947518c5cba0226a7ebc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76f7f91a579445b0aa2c6dc38f0a197f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing BERT parameters...\n",
            "BERT parameters frozen.\n",
            "Creating datasets...\n",
            "Creating data loaders with custom collate_fn...\n",
            "\n",
            "--- [1/1] Starting processing for model configuration: AVT (Active: ('audio', 'video', 'text')) ---\n",
            "Initializing model components for configuration AVT...\n",
            "Model components for AVT initialized.\n",
            "Starting training for AVT... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT) Train Avg Loss: 0.9978, BCE: 0.6695, SimCLR: 10.9421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT) Val Avg BCE: 0.6318, Accuracy: 0.6571, F1: 0.6842\n",
            "Epoch 1 (AVT): New best validation accuracy: 0.6571.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT) Train Avg Loss: 0.9247, BCE: 0.6114, SimCLR: 10.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT) Val Avg BCE: 0.6155, Accuracy: 0.6582, F1: 0.6315\n",
            "Epoch 2 (AVT): New best validation accuracy: 0.6582.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Train Avg Loss: 0.8571, BCE: 0.5613, SimCLR: 9.8610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Val Avg BCE: 0.6166, Accuracy: 0.6847, F1: 0.6950\n",
            "Epoch 3 (AVT): New best validation accuracy: 0.6847.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT) Train Avg Loss: 0.7322, BCE: 0.4454, SimCLR: 9.5593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT) Val Avg BCE: 0.6693, Accuracy: 0.6714, F1: 0.6492\n",
            "Training for configuration AVT complete. Best validation accuracy for this config: 0.6847 (at epoch 3)\n",
            "\n",
            "Starting testing phase for configuration AVT...\n",
            "Loaded best model from Epoch 3 (AVT) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Test Avg BCE: 0.5897, Accuracy: 0.6841, F1: 0.6940\n",
            "Final test results for configuration AVT -> Avg BCE Loss: 0.5897, Accuracy: 0.6841\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AVT\n",
            "  Best Validation Accuracy (Epoch 3): 0.6847\n",
            "  Test Set F1 Score: 0.6940\n",
            "  Test Set Accuracy: 0.6841\n",
            "  Test Set Loss: 0.5897\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    MAX_BERT_LEN = 512\n",
        "    # LSTM_HIDDEN_SIZE is now used for text LSTM, audio/video hierarchical LSTMs have their own config\n",
        "    TEXT_LSTM_HIDDEN_SIZE = 256\n",
        "    # Audio/video projection layer input dimension, now is the output dimension of HierarchicalLSTMAggregator\n",
        "    HIER_LSTM_OUTPUT_DIM = SAMPLE_LSTM_HIDDEN_DIM_CONFIG # Use the constant defined above: 512\n",
        "    PROJECTOR_OUTPUT_DIM = 1024 # Projection layer output / fusion head input dimension (unchanged)\n",
        "\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    NUM_CLASSES = 2\n",
        "    BATCH_SIZE = 16 # Note: if memory is insufficient, BATCH_SIZE can be reduced\n",
        "    LEARNING_RATE = 5e-5\n",
        "    BERT_LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 4\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "    HIER_LSTM_DROPOUT = 0.3 # Dropout for hierarchical LSTM\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"MAX_BERT_LEN set to: {MAX_BERT_LEN}\")\n",
        "    print(f\"Text LSTM_HIDDEN_SIZE set to: {TEXT_LSTM_HIDDEN_SIZE}\")\n",
        "    print(f\"Hierarchical LSTM output dim (projector input): {HIER_LSTM_OUTPUT_DIM}\")\n",
        "    print(f\"Projector output dim (fusion head AV input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"ATTENTION_TOKEN_DIM set to: {ATTENTION_TOKEN_DIM}\")\n",
        "    print(f\"NUM_ATTENTION_TOKENS_PER_MODAL set to: {NUM_ATTENTION_TOKENS_PER_MODAL} (processed modality feature dimension: {NUM_ATTENTION_TOKENS_PER_MODAL*ATTENTION_TOKEN_DIM})\")\n",
        "\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        {'name': 'AT',  'audio': True,  'video': False,  'text': True},\n",
        "        {'name': 'VT',  'audio': False,  'video': True,  'text': True},\n",
        "        {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "        {'name': 'V',   'audio': False, 'video': True, 'text': False},\n",
        "        {'name': 'A',   'audio': True, 'video': False, 'text': False},\n",
        "\n",
        "\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    print(\"Initializing BERT model and tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    bert_feature_extractor_global = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = bert_feature_extractor_global.config.hidden_size\n",
        "\n",
        "    print(\"Freezing BERT parameters...\") # BERT parameters will not be updated\n",
        "    for param in bert_feature_extractor_global.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"BERT parameters frozen.\")\n",
        "\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = CustomFeatureDataset(\n",
        "        concatenated_train_audio, concatenated_train_video, concatenated_train_text, train_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDataset(\n",
        "        concatenated_dev_audio, concatenated_dev_video, concatenated_dev_text, dev_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    test_dataset = CustomFeatureDataset(\n",
        "        concatenated_test_audio, concatenated_test_video, concatenated_test_text, test_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    print(\"Creating data loaders with custom collate_fn...\")\n",
        "    # DataLoader now uses custom_collate_fn\n",
        "    # drop_last=True might be more stable for training contrastive loss, avoiding last batch being too small\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True if BATCH_SIZE > 1 else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "\n",
        "    for config_idx, model_config in enumerate(modality_configurations):\n",
        "        config_name = model_config['name']\n",
        "        active_modalities_tuple = tuple(m for m in ['audio', 'video', 'text'] if model_config[m])\n",
        "        print(f\"\\n--- [{config_idx+1}/{len(modality_configurations)}] Starting processing for model configuration: {config_name} (Active: {active_modalities_tuple}) ---\")\n",
        "\n",
        "        print(f\"Initializing model components for configuration {config_name}...\")\n",
        "        # 1. Initialize trainable hierarchical LSTM aggregators\n",
        "        current_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_AUDIO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "        current_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 2. Initialize projection layers (input dimension is the output dimension of hierarchical LSTM)\n",
        "        current_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "        current_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "        # 3. Initialize fusion head (input dimension is the output dimension of projection layers)\n",
        "        current_clf_head = MultimodalFusionLSTMHead(\n",
        "            projected_audio_video_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len=MAX_BERT_LEN,\n",
        "            lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE, # Text LSTM\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            num_classes=NUM_CLASSES,\n",
        "            active_modalities=active_modalities_tuple\n",
        "        ).to(DEVICE)\n",
        "        print(f\"Model components for {config_name} initialized.\")\n",
        "\n",
        "        # Parameters for BERT will have a specific learning rate, other parameters will use the main LEARNING_RATE\n",
        "        # Since BERT parameters have requires_grad=False, they won't be updated by the optimizer,\n",
        "        # but they are listed here for completeness or if fine-tuning BERT was an option.\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': bert_feature_extractor_global.parameters(), 'lr': BERT_LEARNING_RATE}, # BERT params (frozen)\n",
        "            # Group all other trainable parameters\n",
        "            {'params': list(current_audio_hier_lstm.parameters()) + \\\n",
        "                       list(current_video_hier_lstm.parameters()) + \\\n",
        "                       list(current_audio_projector.parameters()) + \\\n",
        "                       list(current_video_projector.parameters()) + \\\n",
        "                       list(current_clf_head.parameters()), 'lr': LEARNING_RATE} # Main learning rate for these\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters) # Default lr=LEARNING_RATE will be overridden for specific groups\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs, Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_model_state_for_config = {\n",
        "            'epoch': 0,\n",
        "            'audio_hier_lstm_state_dict': None, 'video_hier_lstm_state_dict': None,\n",
        "            'audio_projector_state_dict': None, 'video_projector_state_dict': None,\n",
        "            'bert_state_dict': None,  # Added this line\n",
        "            'clf_head_state_dict': None, 'best_val_accuracy': 0.0\n",
        "        }\n",
        "        num_training_steps_per_epoch = len(train_loader)\n",
        "        if num_training_steps_per_epoch == 0 and NUM_EPOCHS > 0 : # Handle case where train_loader is empty\n",
        "            print(f\"Warning: train_loader for {config_name} is empty. Scheduler will not be effective.\")\n",
        "            scheduler = None # Or do not create scheduler\n",
        "        elif NUM_EPOCHS > 0:\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1) # E.g., 10% of steps for warmup\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader is empty. Aborting training for {config_name}.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train(\n",
        "                    current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                    current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                    train_loader, optimizer, scheduler, bce_criterion, contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS, CONTRASTIVE_LOSS_WEIGHT,\n",
        "                    model_config, bert_tokenizer_global\n",
        "                )\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy = validate_or_test(\n",
        "                        current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                        current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                        val_loader, bce_criterion, DEVICE, epoch, NUM_EPOCHS,\n",
        "                        model_config, bert_tokenizer_global, mode=\"Val\"\n",
        "                    )\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f}.\")\n",
        "                        best_model_state_for_config['epoch'] = epoch\n",
        "                        best_model_state_for_config['audio_hier_lstm_state_dict'] = copy.deepcopy(current_audio_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['video_hier_lstm_state_dict'] = copy.deepcopy(current_video_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['audio_projector_state_dict'] = copy.deepcopy(current_audio_projector.state_dict())\n",
        "                        best_model_state_for_config['video_projector_state_dict'] = copy.deepcopy(current_video_projector.state_dict())\n",
        "                        best_model_state_for_config['clf_head_state_dict'] = copy.deepcopy(current_clf_head.state_dict())\n",
        "                        best_model_state_for_config['best_val_accuracy'] = best_val_accuracy_for_config\n",
        "                        best_model_state_for_config['bert_state_dict'] = copy.deepcopy(bert_feature_extractor_global.state_dict()) # Save BERT state too\n",
        "\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for configuration {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (at epoch {best_model_state_for_config['epoch']+1})\")\n",
        "\n",
        "        print(f\"\\nStarting testing phase for configuration {config_name}...\")\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader is empty. Skipping testing for {config_name}.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'test_acc': 0.0, 'test_loss': 0.0, 'best_epoch': best_model_state_for_config['epoch']+1 if best_val_accuracy_for_config > 0 else 'N/A'}\n",
        "        else:\n",
        "            # Re-initialize models for testing and load best weights\n",
        "            test_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_AUDIO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "            test_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_VIDEO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "\n",
        "            test_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "            test_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "            test_clf_head = MultimodalFusionLSTMHead(\n",
        "                projected_audio_video_dim=PROJECTOR_OUTPUT_DIM, bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "                max_bert_len=MAX_BERT_LEN, lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE,\n",
        "                attention_token_dim=ATTENTION_TOKEN_DIM, num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "                num_classes=NUM_CLASSES, active_modalities=active_modalities_tuple).to(DEVICE)\n",
        "\n",
        "            # Initialize a separate BERT model for testing to load its specific state\n",
        "            test_bert = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "            # BERT parameters for test_bert should also be frozen if they were during training\n",
        "            for param in test_bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            epoch_for_log = None\n",
        "            if best_model_state_for_config['clf_head_state_dict'] is not None:\n",
        "                test_audio_hier_lstm.load_state_dict(best_model_state_for_config['audio_hier_lstm_state_dict'])\n",
        "                test_video_hier_lstm.load_state_dict(best_model_state_for_config['video_hier_lstm_state_dict'])\n",
        "                test_audio_projector.load_state_dict(best_model_state_for_config['audio_projector_state_dict'])\n",
        "                test_video_projector.load_state_dict(best_model_state_for_config['video_projector_state_dict'])\n",
        "                test_clf_head.load_state_dict(best_model_state_for_config['clf_head_state_dict'])\n",
        "                if best_model_state_for_config['bert_state_dict'] is not None:\n",
        "                     test_bert.load_state_dict(best_model_state_for_config['bert_state_dict'])\n",
        "                print(f\"Loaded best model from Epoch {best_model_state_for_config['epoch']+1} ({config_name}) for testing.\")\n",
        "                epoch_for_log = best_model_state_for_config['epoch']\n",
        "            else:\n",
        "                print(f\"No best validation model state found for {config_name}. Using final model from training for testing.\")\n",
        "                test_audio_hier_lstm.load_state_dict(current_audio_hier_lstm.state_dict())\n",
        "                test_video_hier_lstm.load_state_dict(current_video_hier_lstm.state_dict())\n",
        "                test_audio_projector.load_state_dict(current_audio_projector.state_dict())\n",
        "                test_video_projector.load_state_dict(current_video_projector.state_dict())\n",
        "                test_clf_head.load_state_dict(current_clf_head.state_dict())\n",
        "                test_bert.load_state_dict(bert_feature_extractor_global.state_dict()) # Use the state of the global BERT\n",
        "                epoch_for_log = NUM_EPOCHS - 1 if NUM_EPOCHS > 0 else None\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test(\n",
        "                test_audio_hier_lstm,\n",
        "                test_video_hier_lstm,\n",
        "                test_audio_projector,\n",
        "                test_video_projector,\n",
        "                test_bert, # Pass the test_bert instance with loaded weights\n",
        "                test_clf_head,\n",
        "                test_loader,\n",
        "                bce_criterion,\n",
        "                DEVICE,\n",
        "                epoch=epoch_for_log,\n",
        "                num_epochs=NUM_EPOCHS, # or best_model_state_for_config['epoch']+1\n",
        "                current_modality_config=model_config,\n",
        "                tokenizer_for_padding=bert_tokenizer_global,\n",
        "                mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for configuration {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config,\n",
        "                'test_f1': test_f1,\n",
        "                'test_acc': test_accuracy, 'test_loss': test_loss,\n",
        "                'best_epoch': best_model_state_for_config['epoch']+1 if best_model_state_for_config['clf_head_state_dict'] is not None else (NUM_EPOCHS if NUM_EPOCHS > 0 else 'N/A')\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy (Epoch {results['best_epoch']}): {results['val_acc']:.4f}\")\n",
        "        print(f\"  Test Set Accuracy: {results['test_acc']:.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results['test_f1']:.4f}\")\n",
        "        print(f\"  Test Set Loss: {results['test_loss']:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuZN9F2nJRH2",
        "outputId": "fcd34a4e-cff8-4c98-ffee-07b4c993e38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "MAX_BERT_LEN set to: 512\n",
            "Text LSTM_HIDDEN_SIZE set to: 256\n",
            "Hierarchical LSTM output dim (projector input): 512\n",
            "Projector output dim (fusion head AV input): 1024\n",
            "ATTENTION_TOKEN_DIM set to: 32\n",
            "NUM_ATTENTION_TOKENS_PER_MODAL set to: 16 (processed modality feature dimension: 512)\n",
            "Initializing BERT model and tokenizer...\n",
            "Freezing BERT parameters...\n",
            "BERT parameters frozen.\n",
            "Creating datasets...\n",
            "Creating data loaders with custom collate_fn...\n",
            "\n",
            "--- [1/6] Starting processing for model configuration: AV (Active: ('audio', 'video')) ---\n",
            "Initializing model components for configuration AV...\n",
            "Model components for AV initialized.\n",
            "Starting training for AV... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Train Avg Loss: 0.8028, BCE: 0.6930, SimCLR: 3.6603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AV) Val Avg BCE: 0.6925, Accuracy: 0.5020, F1: 0.0722\n",
            "Epoch 1 (AV): New best validation accuracy: 0.5020.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Train Avg Loss: 0.7740, BCE: 0.6878, SimCLR: 2.8742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Val Avg BCE: 0.6870, Accuracy: 0.5571, F1: 0.5241\n",
            "Epoch 2 (AV): New best validation accuracy: 0.5571.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Train Avg Loss: 0.7578, BCE: 0.6794, SimCLR: 2.6142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AV) Val Avg BCE: 0.6882, Accuracy: 0.5561, F1: 0.6181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Train Avg Loss: 0.7510, BCE: 0.6739, SimCLR: 2.5674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AV) Val Avg BCE: 0.6882, Accuracy: 0.5500, F1: 0.5353\n",
            "Training for configuration AV complete. Best validation accuracy for this config: 0.5571 (at epoch 2)\n",
            "\n",
            "Starting testing phase for configuration AV...\n",
            "Loaded best model from Epoch 2 (AV) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AV) Test Avg BCE: 0.6833, Accuracy: 0.5684, F1: 0.5498\n",
            "Final test results for configuration AV -> Avg BCE Loss: 0.6833, Accuracy: 0.5684\n",
            "\n",
            "--- [2/6] Starting processing for model configuration: AT (Active: ('audio', 'text')) ---\n",
            "Initializing model components for configuration AT...\n",
            "Model components for AT initialized.\n",
            "Starting training for AT... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Train Avg Loss: 0.7828, BCE: 0.6746, SimCLR: 3.6068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AT) Val Avg BCE: 0.6754, Accuracy: 0.5724, F1: 0.3901\n",
            "Epoch 1 (AT): New best validation accuracy: 0.5724.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Train Avg Loss: 0.7247, BCE: 0.6204, SimCLR: 3.4755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Val Avg BCE: 0.6104, Accuracy: 0.6816, F1: 0.6892\n",
            "Epoch 2 (AT): New best validation accuracy: 0.6816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Train Avg Loss: 0.6665, BCE: 0.5653, SimCLR: 3.3746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AT) Val Avg BCE: 0.6458, Accuracy: 0.6510, F1: 0.5799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Train Avg Loss: 0.5663, BCE: 0.4654, SimCLR: 3.3617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AT) Val Avg BCE: 0.6582, Accuracy: 0.6816, F1: 0.6518\n",
            "Training for configuration AT complete. Best validation accuracy for this config: 0.6816 (at epoch 2)\n",
            "\n",
            "Starting testing phase for configuration AT...\n",
            "Loaded best model from Epoch 2 (AT) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AT) Test Avg BCE: 0.5975, Accuracy: 0.6861, F1: 0.6911\n",
            "Final test results for configuration AT -> Avg BCE Loss: 0.5975, Accuracy: 0.6861\n",
            "\n",
            "--- [3/6] Starting processing for model configuration: VT (Active: ('video', 'text')) ---\n",
            "Initializing model components for configuration VT...\n",
            "Model components for VT initialized.\n",
            "Starting training for VT... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Train Avg Loss: 0.7719, BCE: 0.6636, SimCLR: 3.6105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Val Avg BCE: 0.6369, Accuracy: 0.6500, F1: 0.6594\n",
            "Epoch 1 (VT): New best validation accuracy: 0.6500.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Train Avg Loss: 0.7080, BCE: 0.6025, SimCLR: 3.5170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Val Avg BCE: 0.6155, Accuracy: 0.6694, F1: 0.7268\n",
            "Epoch 2 (VT): New best validation accuracy: 0.6694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Train Avg Loss: 0.6444, BCE: 0.5390, SimCLR: 3.5127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Val Avg BCE: 0.6270, Accuracy: 0.6939, F1: 0.6982\n",
            "Epoch 3 (VT): New best validation accuracy: 0.6939.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Train Avg Loss: 0.5281, BCE: 0.4207, SimCLR: 3.5809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Val Avg BCE: 0.7385, Accuracy: 0.6898, F1: 0.6813\n",
            "Training for configuration VT complete. Best validation accuracy for this config: 0.6939 (at epoch 3)\n",
            "\n",
            "Starting testing phase for configuration VT...\n",
            "Loaded best model from Epoch 3 (VT) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Test Avg BCE: 0.5917, Accuracy: 0.7032, F1: 0.7059\n",
            "Final test results for configuration VT -> Avg BCE Loss: 0.5917, Accuracy: 0.7032\n",
            "\n",
            "--- [4/6] Starting processing for model configuration: T (Active: ('text',)) ---\n",
            "Initializing model components for configuration T...\n",
            "Model components for T initialized.\n",
            "Starting training for T... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Train Avg Loss: 0.6761, BCE: 0.6761, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Val Avg BCE: 0.6526, Accuracy: 0.6224, F1: 0.5616\n",
            "Epoch 1 (T): New best validation accuracy: 0.6224.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Train Avg Loss: 0.6215, BCE: 0.6215, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Val Avg BCE: 0.6123, Accuracy: 0.6929, F1: 0.6894\n",
            "Epoch 2 (T): New best validation accuracy: 0.6929.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Train Avg Loss: 0.5823, BCE: 0.5823, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Val Avg BCE: 0.5998, Accuracy: 0.6888, F1: 0.6859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Train Avg Loss: 0.5208, BCE: 0.5208, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Val Avg BCE: 0.6193, Accuracy: 0.6806, F1: 0.6688\n",
            "Training for configuration T complete. Best validation accuracy for this config: 0.6929 (at epoch 2)\n",
            "\n",
            "Starting testing phase for configuration T...\n",
            "Loaded best model from Epoch 2 (T) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Test Avg BCE: 0.6066, Accuracy: 0.6680, F1: 0.6562\n",
            "Final test results for configuration T -> Avg BCE Loss: 0.6066, Accuracy: 0.6680\n",
            "\n",
            "--- [5/6] Starting processing for model configuration: V (Active: ('video',)) ---\n",
            "Initializing model components for configuration V...\n",
            "Model components for V initialized.\n",
            "Starting training for V... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Train Avg Loss: 0.6913, BCE: 0.6913, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Val Avg BCE: 0.6922, Accuracy: 0.5173, F1: 0.2972\n",
            "Epoch 1 (V): New best validation accuracy: 0.5173.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Train Avg Loss: 0.6875, BCE: 0.6875, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Val Avg BCE: 0.6931, Accuracy: 0.5224, F1: 0.3276\n",
            "Epoch 2 (V): New best validation accuracy: 0.5224.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Train Avg Loss: 0.6758, BCE: 0.6758, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Val Avg BCE: 0.6903, Accuracy: 0.5347, F1: 0.4584\n",
            "Epoch 3 (V): New best validation accuracy: 0.5347.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Train Avg Loss: 0.6681, BCE: 0.6681, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Val Avg BCE: 0.6924, Accuracy: 0.5520, F1: 0.5588\n",
            "Epoch 4 (V): New best validation accuracy: 0.5520.\n",
            "Training for configuration V complete. Best validation accuracy for this config: 0.5520 (at epoch 4)\n",
            "\n",
            "Starting testing phase for configuration V...\n",
            "Loaded best model from Epoch 4 (V) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Test Avg BCE: 0.6684, Accuracy: 0.6076, F1: 0.6154\n",
            "Final test results for configuration V -> Avg BCE Loss: 0.6684, Accuracy: 0.6076\n",
            "\n",
            "--- [6/6] Starting processing for model configuration: A (Active: ('audio',)) ---\n",
            "Initializing model components for configuration A...\n",
            "Model components for A initialized.\n",
            "Starting training for A... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n",
            "Epoch 1 (A): New best validation accuracy: 0.4959.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n",
            "Training for configuration A complete. Best validation accuracy for this config: 0.4959 (at epoch 1)\n",
            "\n",
            "Starting testing phase for configuration A...\n",
            "Loaded best model from Epoch 1 (A) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Test Avg BCE: 0.6931, Accuracy: 0.5070, F1: 0.0000\n",
            "Final test results for configuration A -> Avg BCE Loss: 0.6931, Accuracy: 0.5070\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AV\n",
            "  Best Validation Accuracy (Epoch 2): 0.5571\n",
            "  Test Set Accuracy: 0.5684\n",
            "  Test Set F1 Score: 0.5498\n",
            "  Test Set Loss: 0.6833\n",
            "------------------------------\n",
            "Configuration: AT\n",
            "  Best Validation Accuracy (Epoch 2): 0.6816\n",
            "  Test Set Accuracy: 0.6861\n",
            "  Test Set F1 Score: 0.6911\n",
            "  Test Set Loss: 0.5975\n",
            "------------------------------\n",
            "Configuration: VT\n",
            "  Best Validation Accuracy (Epoch 3): 0.6939\n",
            "  Test Set Accuracy: 0.7032\n",
            "  Test Set F1 Score: 0.7059\n",
            "  Test Set Loss: 0.5917\n",
            "------------------------------\n",
            "Configuration: T\n",
            "  Best Validation Accuracy (Epoch 2): 0.6929\n",
            "  Test Set Accuracy: 0.6680\n",
            "  Test Set F1 Score: 0.6562\n",
            "  Test Set Loss: 0.6066\n",
            "------------------------------\n",
            "Configuration: V\n",
            "  Best Validation Accuracy (Epoch 4): 0.5520\n",
            "  Test Set Accuracy: 0.6076\n",
            "  Test Set F1 Score: 0.6154\n",
            "  Test Set Loss: 0.6684\n",
            "------------------------------\n",
            "Configuration: A\n",
            "  Best Validation Accuracy (Epoch 1): 0.4959\n",
            "  Test Set Accuracy: 0.5070\n",
            "  Test Set F1 Score: 0.0000\n",
            "  Test Set Loss: 0.6931\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    MAX_BERT_LEN = 512\n",
        "    # LSTM_HIDDEN_SIZE is now used for text LSTM, audio/video hierarchical LSTMs have their own config\n",
        "    TEXT_LSTM_HIDDEN_SIZE = 256\n",
        "    # Audio/video projection layer input dimension, now is the output dimension of HierarchicalLSTMAggregator\n",
        "    HIER_LSTM_OUTPUT_DIM = SAMPLE_LSTM_HIDDEN_DIM_CONFIG # Use the constant defined above: 512\n",
        "    PROJECTOR_OUTPUT_DIM = 1024 # Projection layer output / fusion head input dimension (unchanged)\n",
        "\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    NUM_CLASSES = 2\n",
        "    BATCH_SIZE = 16 # Note: if memory is insufficient, BATCH_SIZE can be reduced\n",
        "    LEARNING_RATE = 5e-5\n",
        "    BERT_LEARNING_RATE = 8e-5\n",
        "    NUM_EPOCHS = 4\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "    HIER_LSTM_DROPOUT = 0.3 # Dropout for hierarchical LSTM\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"MAX_BERT_LEN set to: {MAX_BERT_LEN}\")\n",
        "    print(f\"Text LSTM_HIDDEN_SIZE set to: {TEXT_LSTM_HIDDEN_SIZE}\")\n",
        "    print(f\"Hierarchical LSTM output dim (projector input): {HIER_LSTM_OUTPUT_DIM}\")\n",
        "    print(f\"Projector output dim (fusion head AV input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"ATTENTION_TOKEN_DIM set to: {ATTENTION_TOKEN_DIM}\")\n",
        "    print(f\"NUM_ATTENTION_TOKENS_PER_MODAL set to: {NUM_ATTENTION_TOKENS_PER_MODAL} (processed modality feature dimension: {NUM_ATTENTION_TOKENS_PER_MODAL*ATTENTION_TOKEN_DIM})\")\n",
        "\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        {'name': 'AT',  'audio': True,  'video': False,  'text': True},\n",
        "        {'name': 'VT',  'audio': False,  'video': True,  'text': True},\n",
        "        {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "        {'name': 'V',   'audio': False, 'video': True, 'text': False},\n",
        "        {'name': 'A',   'audio': True, 'video': False, 'text': False},\n",
        "\n",
        "\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    print(\"Initializing BERT model and tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    bert_feature_extractor_global = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = bert_feature_extractor_global.config.hidden_size\n",
        "\n",
        "    print(\"Freezing BERT parameters...\") # BERT parameters will not be updated\n",
        "    for param in bert_feature_extractor_global.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"BERT parameters frozen.\")\n",
        "\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = CustomFeatureDataset(\n",
        "        concatenated_train_audio, concatenated_train_video, concatenated_train_text, train_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDataset(\n",
        "        concatenated_dev_audio, concatenated_dev_video, concatenated_dev_text, dev_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    test_dataset = CustomFeatureDataset(\n",
        "        concatenated_test_audio, concatenated_test_video, concatenated_test_text, test_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    print(\"Creating data loaders with custom collate_fn...\")\n",
        "    # DataLoader now uses custom_collate_fn\n",
        "    # drop_last=True might be more stable for training contrastive loss, avoiding last batch being too small\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True if BATCH_SIZE > 1 else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "\n",
        "    for config_idx, model_config in enumerate(modality_configurations):\n",
        "        config_name = model_config['name']\n",
        "        active_modalities_tuple = tuple(m for m in ['audio', 'video', 'text'] if model_config[m])\n",
        "        print(f\"\\n--- [{config_idx+1}/{len(modality_configurations)}] Starting processing for model configuration: {config_name} (Active: {active_modalities_tuple}) ---\")\n",
        "\n",
        "        print(f\"Initializing model components for configuration {config_name}...\")\n",
        "        # 1. Initialize trainable hierarchical LSTM aggregators\n",
        "        current_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_AUDIO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "        current_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 2. Initialize projection layers (input dimension is the output dimension of hierarchical LSTM)\n",
        "        current_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "        current_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "        # 3. Initialize fusion head (input dimension is the output dimension of projection layers)\n",
        "        current_clf_head = MultimodalFusionLSTMHead(\n",
        "            projected_audio_video_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len=MAX_BERT_LEN,\n",
        "            lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE, # Text LSTM\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            num_classes=NUM_CLASSES,\n",
        "            active_modalities=active_modalities_tuple\n",
        "        ).to(DEVICE)\n",
        "        print(f\"Model components for {config_name} initialized.\")\n",
        "\n",
        "        # Parameters for BERT will have a specific learning rate, other parameters will use the main LEARNING_RATE\n",
        "        # Since BERT parameters have requires_grad=False, they won't be updated by the optimizer,\n",
        "        # but they are listed here for completeness or if fine-tuning BERT was an option.\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': bert_feature_extractor_global.parameters(), 'lr': BERT_LEARNING_RATE}, # BERT params (frozen)\n",
        "            # Group all other trainable parameters\n",
        "            {'params': list(current_audio_hier_lstm.parameters()) + \\\n",
        "                       list(current_video_hier_lstm.parameters()) + \\\n",
        "                       list(current_audio_projector.parameters()) + \\\n",
        "                       list(current_video_projector.parameters()) + \\\n",
        "                       list(current_clf_head.parameters()), 'lr': LEARNING_RATE} # Main learning rate for these\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters) # Default lr=LEARNING_RATE will be overridden for specific groups\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs, Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_model_state_for_config = {\n",
        "            'epoch': 0,\n",
        "            'audio_hier_lstm_state_dict': None, 'video_hier_lstm_state_dict': None,\n",
        "            'audio_projector_state_dict': None, 'video_projector_state_dict': None,\n",
        "            'bert_state_dict': None,  # Added this line\n",
        "            'clf_head_state_dict': None, 'best_val_accuracy': 0.0\n",
        "        }\n",
        "        num_training_steps_per_epoch = len(train_loader)\n",
        "        if num_training_steps_per_epoch == 0 and NUM_EPOCHS > 0 : # Handle case where train_loader is empty\n",
        "            print(f\"Warning: train_loader for {config_name} is empty. Scheduler will not be effective.\")\n",
        "            scheduler = None # Or do not create scheduler\n",
        "        elif NUM_EPOCHS > 0:\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1) # E.g., 10% of steps for warmup\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader is empty. Aborting training for {config_name}.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train(\n",
        "                    current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                    current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                    train_loader, optimizer, scheduler, bce_criterion, contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS, CONTRASTIVE_LOSS_WEIGHT,\n",
        "                    model_config, bert_tokenizer_global\n",
        "                )\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy = validate_or_test(\n",
        "                        current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                        current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                        val_loader, bce_criterion, DEVICE, epoch, NUM_EPOCHS,\n",
        "                        model_config, bert_tokenizer_global, mode=\"Val\"\n",
        "                    )\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f}.\")\n",
        "                        best_model_state_for_config['epoch'] = epoch\n",
        "                        best_model_state_for_config['audio_hier_lstm_state_dict'] = copy.deepcopy(current_audio_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['video_hier_lstm_state_dict'] = copy.deepcopy(current_video_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['audio_projector_state_dict'] = copy.deepcopy(current_audio_projector.state_dict())\n",
        "                        best_model_state_for_config['video_projector_state_dict'] = copy.deepcopy(current_video_projector.state_dict())\n",
        "                        best_model_state_for_config['clf_head_state_dict'] = copy.deepcopy(current_clf_head.state_dict())\n",
        "                        best_model_state_for_config['best_val_accuracy'] = best_val_accuracy_for_config\n",
        "                        best_model_state_for_config['bert_state_dict'] = copy.deepcopy(bert_feature_extractor_global.state_dict()) # Save BERT state too\n",
        "\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for configuration {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (at epoch {best_model_state_for_config['epoch']+1})\")\n",
        "\n",
        "        print(f\"\\nStarting testing phase for configuration {config_name}...\")\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader is empty. Skipping testing for {config_name}.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'test_acc': 0.0, 'test_loss': 0.0, 'best_epoch': best_model_state_for_config['epoch']+1 if best_val_accuracy_for_config > 0 else 'N/A'}\n",
        "        else:\n",
        "            # Re-initialize models for testing and load best weights\n",
        "            test_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_AUDIO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "            test_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_VIDEO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "\n",
        "            test_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "            test_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "            test_clf_head = MultimodalFusionLSTMHead(\n",
        "                projected_audio_video_dim=PROJECTOR_OUTPUT_DIM, bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "                max_bert_len=MAX_BERT_LEN, lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE,\n",
        "                attention_token_dim=ATTENTION_TOKEN_DIM, num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "                num_classes=NUM_CLASSES, active_modalities=active_modalities_tuple).to(DEVICE)\n",
        "\n",
        "            # Initialize a separate BERT model for testing to load its specific state\n",
        "            test_bert = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "            # BERT parameters for test_bert should also be frozen if they were during training\n",
        "            for param in test_bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            epoch_for_log = None\n",
        "            if best_model_state_for_config['clf_head_state_dict'] is not None:\n",
        "                test_audio_hier_lstm.load_state_dict(best_model_state_for_config['audio_hier_lstm_state_dict'])\n",
        "                test_video_hier_lstm.load_state_dict(best_model_state_for_config['video_hier_lstm_state_dict'])\n",
        "                test_audio_projector.load_state_dict(best_model_state_for_config['audio_projector_state_dict'])\n",
        "                test_video_projector.load_state_dict(best_model_state_for_config['video_projector_state_dict'])\n",
        "                test_clf_head.load_state_dict(best_model_state_for_config['clf_head_state_dict'])\n",
        "                if best_model_state_for_config['bert_state_dict'] is not None:\n",
        "                     test_bert.load_state_dict(best_model_state_for_config['bert_state_dict'])\n",
        "                print(f\"Loaded best model from Epoch {best_model_state_for_config['epoch']+1} ({config_name}) for testing.\")\n",
        "                epoch_for_log = best_model_state_for_config['epoch']\n",
        "            else:\n",
        "                print(f\"No best validation model state found for {config_name}. Using final model from training for testing.\")\n",
        "                test_audio_hier_lstm.load_state_dict(current_audio_hier_lstm.state_dict())\n",
        "                test_video_hier_lstm.load_state_dict(current_video_hier_lstm.state_dict())\n",
        "                test_audio_projector.load_state_dict(current_audio_projector.state_dict())\n",
        "                test_video_projector.load_state_dict(current_video_projector.state_dict())\n",
        "                test_clf_head.load_state_dict(current_clf_head.state_dict())\n",
        "                test_bert.load_state_dict(bert_feature_extractor_global.state_dict()) # Use the state of the global BERT\n",
        "                epoch_for_log = NUM_EPOCHS - 1 if NUM_EPOCHS > 0 else None\n",
        "\n",
        "            test_loss, test_accuracy, test_f1 = validate_or_test(\n",
        "                test_audio_hier_lstm,\n",
        "                test_video_hier_lstm,\n",
        "                test_audio_projector,\n",
        "                test_video_projector,\n",
        "                test_bert, # Pass the test_bert instance with loaded weights\n",
        "                test_clf_head,\n",
        "                test_loader,\n",
        "                bce_criterion,\n",
        "                DEVICE,\n",
        "                epoch=epoch_for_log,\n",
        "                num_epochs=NUM_EPOCHS, # or best_model_state_for_config['epoch']+1\n",
        "                current_modality_config=model_config,\n",
        "                tokenizer_for_padding=bert_tokenizer_global,\n",
        "                mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for configuration {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config,\n",
        "                'test_f1': test_f1,\n",
        "                'test_acc': test_accuracy, 'test_loss': test_loss,\n",
        "                'best_epoch': best_model_state_for_config['epoch']+1 if best_model_state_for_config['clf_head_state_dict'] is not None else (NUM_EPOCHS if NUM_EPOCHS > 0 else 'N/A')\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy (Epoch {results['best_epoch']}): {results['val_acc']:.4f}\")\n",
        "        print(f\"  Test Set Accuracy: {results['test_acc']:.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results['test_f1']:.4f}\")\n",
        "        print(f\"  Test Set Loss: {results['test_loss']:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "80e7e774711f4111a3b92c2a85f37db2",
            "b84446b04db9451b91e64ccfcb260113",
            "7a4308e945244b4eb5a8faf8da4e08ab",
            "dc4983f4bc194c7a913a7152d8030a81",
            "5efaa63fb4f34267936a55746d914e5c",
            "e45ad75cc1644f3faa1bd467b961fa27",
            "f31d4da2a93e484db5ab0b0a2d5a2a6c",
            "5388cf6a3c274c3cb2ff04b8416ebbe2",
            "5d635e1897ef482d89e28cc79759ee07",
            "462ec7d1e73b4abcbd089113a99e78c5",
            "fac942e4072442a59a9b51a8071da28c",
            "71aaed2087b7453ea2663a25e28ee895",
            "8b249a08d5d84b35861336a1b215a8d3",
            "d8d2006bf7f54b4fb0c49558f65c4a6f",
            "fbe6006eb5744462b6254816efb31cb3",
            "65bdce2918bd4da386626546e975bad6",
            "7a6e174bdd2e47968367d151bcba0abc",
            "8e74bc3560a04c2b879121eef986b644",
            "f37b9debfa974b94bbf6d7f3c99d2b12",
            "ccb45754122d4c0980d9be64ce804b1d",
            "1670855f79bd4478b0c833f3fa6231ec",
            "2a77be1c348b42b1948ab76b95a24809",
            "b800fe7d7b884fcdab6b83ddad97bec4",
            "319cb0bcde3e4984a8d35a24a7045393",
            "99835fc5f7d34e44a2a2df58a172318b",
            "0c3140606a1946d08fced86ac5c02cfb",
            "d1fdc3fd39824a9cbd33836262721c8a",
            "7cb90eeb85de4c20b6afb9aba52a9e4c",
            "61bf6ff388f64718a4ae764ad7e75ccb",
            "cd8f78913eb046979960f32f9f4d7fd2",
            "5891e362a41441eca652b183a9fa51b5",
            "41271c656b2e4efeb0b5299907ff810f",
            "6019dad1df5349e7b456d5fa3d48e175",
            "ff602ac182a342c89eea873a02efab6e",
            "578e50bfbe8b4d1987a96d119308150e",
            "4c09f424cc1a4c4aab274c51403538e6",
            "c11c5f1c280740779d1a3d678b605d7a",
            "d585ed4a581a41d5b205ef545bc71cb1",
            "68a6c379616049a5ac17400155e0a73f",
            "1352ee6ca7ae4927bd23bee507a0210f",
            "590bdf64ce4c47fa9e3ce4bbcc12fb85",
            "44c2229b2e5749d18350db16bad1bd51",
            "8f277baac84a4b0b83e8c4c635a5e739",
            "a43c9d2d4bf44a88962ee04a8fd9ee08",
            "37f6a0c41926433b9afbe7c973bc65a2",
            "1324cacaef55401f97608293ce6613a5",
            "0c702f48cce94aed9100ec67fdb2e937",
            "39ce899ada6443d78a6a4b2fb9b3c660",
            "8df2e77b894d47b69043393140ac5324",
            "8ebae6d5ee6345c090ca1e1740801190",
            "631ad8da3b2e4986be617b510031295a",
            "81f2771611e94c76b975e8e1eebe579d",
            "aea217685ac24e279a7be37da3ac653d",
            "2252b13bf2f8446893a21219960f1097",
            "296cf0b1352d4f60aa624e676b9f58c7"
          ]
        },
        "id": "yjrXgQ4cigJB",
        "outputId": "ea202183-ead9-41e7-e999-986c2c108158"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "MAX_BERT_LEN set to: 512\n",
            "Text LSTM_HIDDEN_SIZE set to: 256\n",
            "Hierarchical LSTM output dim (projector input): 512\n",
            "Projector output dim (fusion head AV input): 1024\n",
            "ATTENTION_TOKEN_DIM set to: 32\n",
            "NUM_ATTENTION_TOKENS_PER_MODAL set to: 16 (processed modality feature dimension: 512)\n",
            "Initializing BERT model and tokenizer...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80e7e774711f4111a3b92c2a85f37db2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71aaed2087b7453ea2663a25e28ee895",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b800fe7d7b884fcdab6b83ddad97bec4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff602ac182a342c89eea873a02efab6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f6a0c41926433b9afbe7c973bc65a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing BERT parameters...\n",
            "BERT parameters frozen.\n",
            "Creating datasets...\n",
            "Creating data loaders with custom collate_fn...\n",
            "\n",
            "--- [1/6] Starting processing for model configuration: AV (Active: ('audio', 'video')) ---\n",
            "Initializing model components for configuration AV...\n",
            "Model components for AV initialized.\n",
            "Starting training for AV... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 (AV) Train Avg Loss: 0.7981, BCE: 0.6931, SimCLR: 3.4984\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 (AV) Val Avg BCE: 0.6914, Accuracy: 0.5112, F1: 0.4108\n",
            "Epoch 1 (AV): New best validation accuracy: 0.5112.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 (AV) Train Avg Loss: 0.7652, BCE: 0.6876, SimCLR: 2.5889\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 (AV) Val Avg BCE: 0.6888, Accuracy: 0.5459, F1: 0.5251\n",
            "Epoch 2 (AV): New best validation accuracy: 0.5459.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 (AV) Train Avg Loss: 0.7574, BCE: 0.6821, SimCLR: 2.5106\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 (AV) Val Avg BCE: 0.6858, Accuracy: 0.5602, F1: 0.5852\n",
            "Epoch 3 (AV): New best validation accuracy: 0.5602.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 (AV) Train Avg Loss: 0.7501, BCE: 0.6758, SimCLR: 2.4772\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 (AV) Val Avg BCE: 0.6874, Accuracy: 0.5694, F1: 0.5520\n",
            "Epoch 4 (AV): New best validation accuracy: 0.5694.\n",
            "Training for configuration AV complete. Best validation accuracy for this config: 0.5694 (at epoch 4)\n",
            "\n",
            "Starting testing phase for configuration AV...\n",
            "Loaded best model from Epoch 4 (AV) for testing.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 (AV) Test Avg BCE: 0.6796, Accuracy: 0.5865, F1: 0.5802\n",
            "Final test results for configuration AV -> Avg BCE Loss: 0.6796, Accuracy: 0.5865\n",
            "\n",
            "--- [2/6] Starting processing for model configuration: AT (Active: ('audio', 'text')) ---\n",
            "Initializing model components for configuration AT...\n",
            "Model components for AT initialized.\n",
            "Starting training for AT... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 (AT) Train Avg Loss: 0.7727, BCE: 0.6649, SimCLR: 3.5941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 (AT) Val Avg BCE: 0.6222, Accuracy: 0.6633, F1: 0.6989\n",
            "Epoch 1 (AT): New best validation accuracy: 0.6633.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 (AT) Train Avg Loss: 0.7015, BCE: 0.6016, SimCLR: 3.3302\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 (AT) Val Avg BCE: 0.6003, Accuracy: 0.6724, F1: 0.6992\n",
            "Epoch 2 (AT): New best validation accuracy: 0.6724.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 (AT) Train Avg Loss: 0.6215, BCE: 0.5268, SimCLR: 3.1595\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 (AT) Val Avg BCE: 0.6090, Accuracy: 0.6786, F1: 0.6872\n",
            "Epoch 3 (AT): New best validation accuracy: 0.6786.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 (AT) Train Avg Loss: 0.4739, BCE: 0.3794, SimCLR: 3.1511\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 (AT) Val Avg BCE: 0.6893, Accuracy: 0.6653, F1: 0.6540\n",
            "Training for configuration AT complete. Best validation accuracy for this config: 0.6786 (at epoch 3)\n",
            "\n",
            "Starting testing phase for configuration AT...\n",
            "Loaded best model from Epoch 3 (AT) for testing.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 (AT) Test Avg BCE: 0.5992, Accuracy: 0.6740, F1: 0.6754\n",
            "Final test results for configuration AT -> Avg BCE Loss: 0.5992, Accuracy: 0.6740\n",
            "\n",
            "--- [3/6] Starting processing for model configuration: VT (Active: ('video', 'text')) ---\n",
            "Initializing model components for configuration VT...\n",
            "Model components for VT initialized.\n",
            "Starting training for VT... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Train Avg Loss: 0.7726, BCE: 0.6639, SimCLR: 3.6239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (VT) Val Avg BCE: 0.6356, Accuracy: 0.6429, F1: 0.6204\n",
            "Epoch 1 (VT): New best validation accuracy: 0.6429.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Train Avg Loss: 0.7074, BCE: 0.6016, SimCLR: 3.5250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (VT) Val Avg BCE: 0.6324, Accuracy: 0.6602, F1: 0.6353\n",
            "Epoch 2 (VT): New best validation accuracy: 0.6602.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Train Avg Loss: 0.6332, BCE: 0.5268, SimCLR: 3.5484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Val Avg BCE: 0.6252, Accuracy: 0.6724, F1: 0.6545\n",
            "Epoch 3 (VT): New best validation accuracy: 0.6724.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Train Avg Loss: 0.4783, BCE: 0.3693, SimCLR: 3.6346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (VT) Val Avg BCE: 0.6957, Accuracy: 0.6684, F1: 0.6625\n",
            "Training for configuration VT complete. Best validation accuracy for this config: 0.6724 (at epoch 3)\n",
            "\n",
            "Starting testing phase for configuration VT...\n",
            "Loaded best model from Epoch 3 (VT) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (VT) Test Avg BCE: 0.5952, Accuracy: 0.6670, F1: 0.6490\n",
            "Final test results for configuration VT -> Avg BCE Loss: 0.5952, Accuracy: 0.6670\n",
            "\n",
            "--- [4/6] Starting processing for model configuration: T (Active: ('text',)) ---\n",
            "Initializing model components for configuration T...\n",
            "Model components for T initialized.\n",
            "Starting training for T... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Train Avg Loss: 0.6748, BCE: 0.6748, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (T) Val Avg BCE: 0.6620, Accuracy: 0.5888, F1: 0.7017\n",
            "Epoch 1 (T): New best validation accuracy: 0.5888.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Train Avg Loss: 0.6211, BCE: 0.6211, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Val Avg BCE: 0.5991, Accuracy: 0.6857, F1: 0.7072\n",
            "Epoch 2 (T): New best validation accuracy: 0.6857.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Train Avg Loss: 0.5769, BCE: 0.5769, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (T) Val Avg BCE: 0.6004, Accuracy: 0.6827, F1: 0.6757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Train Avg Loss: 0.5141, BCE: 0.5141, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (T) Val Avg BCE: 0.6048, Accuracy: 0.6857, F1: 0.6630\n",
            "Training for configuration T complete. Best validation accuracy for this config: 0.6857 (at epoch 2)\n",
            "\n",
            "Starting testing phase for configuration T...\n",
            "Loaded best model from Epoch 2 (T) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (T) Test Avg BCE: 0.5979, Accuracy: 0.6700, F1: 0.6906\n",
            "Final test results for configuration T -> Avg BCE Loss: 0.5979, Accuracy: 0.6700\n",
            "\n",
            "--- [5/6] Starting processing for model configuration: V (Active: ('video',)) ---\n",
            "Initializing model components for configuration V...\n",
            "Model components for V initialized.\n",
            "Starting training for V... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Train Avg Loss: 0.6928, BCE: 0.6928, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (V) Val Avg BCE: 0.6950, Accuracy: 0.4959, F1: 0.0000\n",
            "Epoch 1 (V): New best validation accuracy: 0.4959.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Train Avg Loss: 0.6870, BCE: 0.6870, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (V) Val Avg BCE: 0.6907, Accuracy: 0.5133, F1: 0.1790\n",
            "Epoch 2 (V): New best validation accuracy: 0.5133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Train Avg Loss: 0.6828, BCE: 0.6828, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (V) Val Avg BCE: 0.6928, Accuracy: 0.5347, F1: 0.3596\n",
            "Epoch 3 (V): New best validation accuracy: 0.5347.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Train Avg Loss: 0.6815, BCE: 0.6815, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Val Avg BCE: 0.6926, Accuracy: 0.5418, F1: 0.4380\n",
            "Epoch 4 (V): New best validation accuracy: 0.5418.\n",
            "Training for configuration V complete. Best validation accuracy for this config: 0.5418 (at epoch 4)\n",
            "\n",
            "Starting testing phase for configuration V...\n",
            "Loaded best model from Epoch 4 (V) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (V) Test Avg BCE: 0.6787, Accuracy: 0.5976, F1: 0.5204\n",
            "Final test results for configuration V -> Avg BCE Loss: 0.6787, Accuracy: 0.5976\n",
            "\n",
            "--- [6/6] Starting processing for model configuration: A (Active: ('audio',)) ---\n",
            "Initializing model components for configuration A...\n",
            "Model components for A initialized.\n",
            "Starting training for A... Total 4 epochs, Batch size 16, Learning rate 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n",
            "Epoch 1 (A): New best validation accuracy: 0.4959.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Train Avg Loss: 0.6931, BCE: 0.6931, SimCLR: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (A) Val Avg BCE: 0.6931, Accuracy: 0.4959, F1: 0.0000\n",
            "Training for configuration A complete. Best validation accuracy for this config: 0.4959 (at epoch 1)\n",
            "\n",
            "Starting testing phase for configuration A...\n",
            "Loaded best model from Epoch 1 (A) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (A) Test Avg BCE: 0.6931, Accuracy: 0.5070, F1: 0.0000\n",
            "Final test results for configuration A -> Avg BCE Loss: 0.6931, Accuracy: 0.5070\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AV\n",
            "  Best Validation Accuracy (Epoch 4): 0.5694\n",
            "  Test Set Accuracy: 0.5865\n",
            "  Test Set F1 Score: 0.5802\n",
            "  Test Set Loss: 0.6796\n",
            "------------------------------\n",
            "Configuration: AT\n",
            "  Best Validation Accuracy (Epoch 3): 0.6786\n",
            "  Test Set Accuracy: 0.6740\n",
            "  Test Set F1 Score: 0.6754\n",
            "  Test Set Loss: 0.5992\n",
            "------------------------------\n",
            "Configuration: VT\n",
            "  Best Validation Accuracy (Epoch 3): 0.6724\n",
            "  Test Set Accuracy: 0.6670\n",
            "  Test Set F1 Score: 0.6490\n",
            "  Test Set Loss: 0.5952\n",
            "------------------------------\n",
            "Configuration: T\n",
            "  Best Validation Accuracy (Epoch 2): 0.6857\n",
            "  Test Set Accuracy: 0.6700\n",
            "  Test Set F1 Score: 0.6906\n",
            "  Test Set Loss: 0.5979\n",
            "------------------------------\n",
            "Configuration: V\n",
            "  Best Validation Accuracy (Epoch 4): 0.5418\n",
            "  Test Set Accuracy: 0.5976\n",
            "  Test Set F1 Score: 0.5204\n",
            "  Test Set Loss: 0.6787\n",
            "------------------------------\n",
            "Configuration: A\n",
            "  Best Validation Accuracy (Epoch 1): 0.4959\n",
            "  Test Set Accuracy: 0.5070\n",
            "  Test Set F1 Score: 0.0000\n",
            "  Test Set Loss: 0.6931\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BERT_MODEL_NAME_FOR_MAIN = \"bert-base-uncased\"\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    MAX_BERT_LEN = 512\n",
        "    # LSTM_HIDDEN_SIZE is now used for text LSTM, audio/video hierarchical LSTMs have their own config\n",
        "    TEXT_LSTM_HIDDEN_SIZE = 256\n",
        "    # Audio/video projection layer input dimension, now is the output dimension of HierarchicalLSTMAggregator\n",
        "    HIER_LSTM_OUTPUT_DIM = SAMPLE_LSTM_HIDDEN_DIM_CONFIG # Use the constant defined above: 512\n",
        "    PROJECTOR_OUTPUT_DIM = 1024 # Projection layer output / fusion head input dimension (unchanged)\n",
        "\n",
        "    ATTENTION_TOKEN_DIM = 32\n",
        "    NUM_ATTENTION_TOKENS_PER_MODAL = 16\n",
        "    NUM_CLASSES = 2\n",
        "    BATCH_SIZE = 16 # Note: if memory is insufficient, BATCH_SIZE can be reduced\n",
        "    LEARNING_RATE = 8e-5\n",
        "    BERT_LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 4\n",
        "    TEMPERATURE_CONTRASTIVE = 0.5\n",
        "    CONTRASTIVE_LOSS_WEIGHT = 0.03\n",
        "    HIER_LSTM_DROPOUT = 0.3 # Dropout for hierarchical LSTM\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"MAX_BERT_LEN set to: {MAX_BERT_LEN}\")\n",
        "    print(f\"Text LSTM_HIDDEN_SIZE set to: {TEXT_LSTM_HIDDEN_SIZE}\")\n",
        "    print(f\"Hierarchical LSTM output dim (projector input): {HIER_LSTM_OUTPUT_DIM}\")\n",
        "    print(f\"Projector output dim (fusion head AV input): {PROJECTOR_OUTPUT_DIM}\")\n",
        "    print(f\"ATTENTION_TOKEN_DIM set to: {ATTENTION_TOKEN_DIM}\")\n",
        "    print(f\"NUM_ATTENTION_TOKENS_PER_MODAL set to: {NUM_ATTENTION_TOKENS_PER_MODAL} (processed modality feature dimension: {NUM_ATTENTION_TOKENS_PER_MODAL*ATTENTION_TOKEN_DIM})\")\n",
        "\n",
        "\n",
        "    modality_configurations = [\n",
        "        {'name': 'AVT', 'audio': True,  'video': True,  'text': True},\n",
        "        # Can add other configurations for testing\n",
        "        # {'name': 'AV',  'audio': True,  'video': True,  'text': False},\n",
        "        # {'name': 'T',   'audio': False, 'video': False, 'text': True},\n",
        "    ]\n",
        "    all_models_results = {}\n",
        "\n",
        "    print(\"Initializing BERT model and tokenizer...\")\n",
        "    bert_tokenizer_global = AutoTokenizer.from_pretrained(BERT_MODEL_NAME_FOR_MAIN)\n",
        "    bert_feature_extractor_global = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "    BERT_HIDDEN_SIZE_ACTUAL = bert_feature_extractor_global.config.hidden_size\n",
        "\n",
        "    print(\"Freezing BERT parameters...\") # BERT parameters will not be updated\n",
        "    for param in bert_feature_extractor_global.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"BERT parameters frozen.\")\n",
        "\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = CustomFeatureDataset(\n",
        "        concatenated_train_audio, concatenated_train_video, concatenated_train_text, train_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    dev_dataset = CustomFeatureDataset(\n",
        "        concatenated_dev_audio, concatenated_dev_video, concatenated_dev_text, dev_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    test_dataset = CustomFeatureDataset(\n",
        "        concatenated_test_audio, concatenated_test_video, concatenated_test_text, test_labels,\n",
        "        bert_tokenizer=bert_tokenizer_global, max_bert_len=MAX_BERT_LEN\n",
        "    )\n",
        "    print(\"Creating data loaders with custom collate_fn...\")\n",
        "    # DataLoader now uses custom_collate_fn\n",
        "    # drop_last=True might be more stable for training contrastive loss, avoiding last batch being too small\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True if BATCH_SIZE > 1 else False)\n",
        "    val_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    bce_criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    contrastive_loss_fn = ContrastiveLossELI5(temperature=TEMPERATURE_CONTRASTIVE).to(DEVICE)\n",
        "\n",
        "    for config_idx, model_config in enumerate(modality_configurations):\n",
        "        config_name = model_config['name']\n",
        "        active_modalities_tuple = tuple(m for m in ['audio', 'video', 'text'] if model_config[m])\n",
        "        print(f\"\\n--- [{config_idx+1}/{len(modality_configurations)}] Starting processing for model configuration: {config_name} (Active: {active_modalities_tuple}) ---\")\n",
        "\n",
        "        print(f\"Initializing model components for configuration {config_name}...\")\n",
        "        # 1. Initialize trainable hierarchical LSTM aggregators\n",
        "        current_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_AUDIO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "        current_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "            word_dim=_VIDEO_WORD_DIM_CONST,\n",
        "            sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "            sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, # = SAMPLE_LSTM_HIDDEN_DIM_CONFIG\n",
        "            dropout_rate=HIER_LSTM_DROPOUT\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 2. Initialize projection layers (input dimension is the output dimension of hierarchical LSTM)\n",
        "        current_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "        current_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "        # 3. Initialize fusion head (input dimension is the output dimension of projection layers)\n",
        "        current_clf_head = MultimodalFusionLSTMHead(\n",
        "            projected_audio_video_dim=PROJECTOR_OUTPUT_DIM,\n",
        "            bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "            max_bert_len=MAX_BERT_LEN,\n",
        "            lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE, # Text LSTM\n",
        "            attention_token_dim=ATTENTION_TOKEN_DIM,\n",
        "            num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "            num_classes=NUM_CLASSES,\n",
        "            active_modalities=active_modalities_tuple\n",
        "        ).to(DEVICE)\n",
        "        print(f\"Model components for {config_name} initialized.\")\n",
        "\n",
        "        # Parameters for BERT will have a specific learning rate, other parameters will use the main LEARNING_RATE\n",
        "        # Since BERT parameters have requires_grad=False, they won't be updated by the optimizer,\n",
        "        # but they are listed here for completeness or if fine-tuning BERT was an option.\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': bert_feature_extractor_global.parameters(), 'lr': BERT_LEARNING_RATE}, # BERT params (frozen)\n",
        "            # Group all other trainable parameters\n",
        "            {'params': list(current_audio_hier_lstm.parameters()) + \\\n",
        "                       list(current_video_hier_lstm.parameters()) + \\\n",
        "                       list(current_audio_projector.parameters()) + \\\n",
        "                       list(current_video_projector.parameters()) + \\\n",
        "                       list(current_clf_head.parameters()), 'lr': LEARNING_RATE} # Main learning rate for these\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters) # Default lr=LEARNING_RATE will be overridden for specific groups\n",
        "\n",
        "        print(f\"Starting training for {config_name}... Total {NUM_EPOCHS} epochs, Batch size {BATCH_SIZE}, Learning rate {LEARNING_RATE}\")\n",
        "        best_val_accuracy_for_config = 0.0\n",
        "        best_model_state_for_config = {\n",
        "            'epoch': 0,\n",
        "            'audio_hier_lstm_state_dict': None, 'video_hier_lstm_state_dict': None,\n",
        "            'audio_projector_state_dict': None, 'video_projector_state_dict': None,\n",
        "            'bert_state_dict': None,  # Added this line\n",
        "            'clf_head_state_dict': None, 'best_val_accuracy': 0.0\n",
        "        }\n",
        "        num_training_steps_per_epoch = len(train_loader)\n",
        "        if num_training_steps_per_epoch == 0 and NUM_EPOCHS > 0 : # Handle case where train_loader is empty\n",
        "            print(f\"Warning: train_loader for {config_name} is empty. Scheduler will not be effective.\")\n",
        "            scheduler = None # Or do not create scheduler\n",
        "        elif NUM_EPOCHS > 0:\n",
        "            total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS\n",
        "            num_warmup_steps = int(total_training_steps * 0.1) # E.g., 10% of steps for warmup\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=num_warmup_steps,\n",
        "                                                        num_training_steps=total_training_steps)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Training data loader is empty. Aborting training for {config_name}.\")\n",
        "        else:\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train(\n",
        "                    current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                    current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                    train_loader, optimizer, scheduler, bce_criterion, contrastive_loss_fn, DEVICE, epoch, NUM_EPOCHS, CONTRASTIVE_LOSS_WEIGHT,\n",
        "                    model_config, bert_tokenizer_global\n",
        "                )\n",
        "                if len(val_loader) > 0:\n",
        "                    val_accuracy = validate_or_test(\n",
        "                        current_audio_hier_lstm, current_video_hier_lstm,\n",
        "                        current_audio_projector, current_video_projector, bert_feature_extractor_global, current_clf_head,\n",
        "                        val_loader, bce_criterion, DEVICE, epoch, NUM_EPOCHS,\n",
        "                        model_config, bert_tokenizer_global, mode=\"Val\"\n",
        "                    )\n",
        "                    if val_accuracy > best_val_accuracy_for_config:\n",
        "                        best_val_accuracy_for_config = val_accuracy\n",
        "                        print(f\"Epoch {epoch+1} ({config_name}): New best validation accuracy: {best_val_accuracy_for_config:.4f}.\")\n",
        "                        best_model_state_for_config['epoch'] = epoch\n",
        "                        best_model_state_for_config['audio_hier_lstm_state_dict'] = copy.deepcopy(current_audio_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['video_hier_lstm_state_dict'] = copy.deepcopy(current_video_hier_lstm.state_dict())\n",
        "                        best_model_state_for_config['audio_projector_state_dict'] = copy.deepcopy(current_audio_projector.state_dict())\n",
        "                        best_model_state_for_config['video_projector_state_dict'] = copy.deepcopy(current_video_projector.state_dict())\n",
        "                        best_model_state_for_config['clf_head_state_dict'] = copy.deepcopy(current_clf_head.state_dict())\n",
        "                        best_model_state_for_config['best_val_accuracy'] = best_val_accuracy_for_config\n",
        "                        best_model_state_for_config['bert_state_dict'] = copy.deepcopy(bert_feature_extractor_global.state_dict()) # Save BERT state too\n",
        "\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1} ({config_name}): Validation data loader is empty. Skipping validation.\")\n",
        "            print(f\"Training for configuration {config_name} complete. Best validation accuracy for this config: {best_val_accuracy_for_config:.4f} (at epoch {best_model_state_for_config['epoch']+1})\")\n",
        "\n",
        "        print(f\"\\nStarting testing phase for configuration {config_name}...\")\n",
        "        if len(test_loader) == 0:\n",
        "            print(f\"Test data loader is empty. Skipping testing for {config_name}.\")\n",
        "            all_models_results[config_name] = {'val_acc': best_val_accuracy_for_config, 'test_acc': 0.0, 'test_loss': 0.0, 'best_epoch': best_model_state_for_config['epoch']+1 if best_val_accuracy_for_config > 0 else 'N/A'}\n",
        "        else:\n",
        "            # Re-initialize models for testing and load best weights\n",
        "            test_audio_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_AUDIO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "            test_video_hier_lstm = HierarchicalLSTMAggregator(\n",
        "                word_dim=_VIDEO_WORD_DIM_CONST, sentence_lstm_hidden_dim=SENTENCE_LSTM_HIDDEN_DIM_CONFIG,\n",
        "                sample_lstm_hidden_dim=HIER_LSTM_OUTPUT_DIM, dropout_rate=HIER_LSTM_DROPOUT).to(DEVICE)\n",
        "\n",
        "            test_audio_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "            test_video_projector = nn.Linear(HIER_LSTM_OUTPUT_DIM, PROJECTOR_OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "            test_clf_head = MultimodalFusionLSTMHead(\n",
        "                projected_audio_video_dim=PROJECTOR_OUTPUT_DIM, bert_hidden_size=BERT_HIDDEN_SIZE_ACTUAL,\n",
        "                max_bert_len=MAX_BERT_LEN, lstm_hidden_size=TEXT_LSTM_HIDDEN_SIZE,\n",
        "                attention_token_dim=ATTENTION_TOKEN_DIM, num_attention_tokens_per_modal=NUM_ATTENTION_TOKENS_PER_MODAL,\n",
        "                num_classes=NUM_CLASSES, active_modalities=active_modalities_tuple).to(DEVICE)\n",
        "\n",
        "            # Initialize a separate BERT model for testing to load its specific state\n",
        "            test_bert = AutoModel.from_pretrained(BERT_MODEL_NAME_FOR_MAIN).to(DEVICE)\n",
        "            # BERT parameters for test_bert should also be frozen if they were during training\n",
        "            for param in test_bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            epoch_for_log = None\n",
        "            if best_model_state_for_config['clf_head_state_dict'] is not None:\n",
        "                test_audio_hier_lstm.load_state_dict(best_model_state_for_config['audio_hier_lstm_state_dict'])\n",
        "                test_video_hier_lstm.load_state_dict(best_model_state_for_config['video_hier_lstm_state_dict'])\n",
        "                test_audio_projector.load_state_dict(best_model_state_for_config['audio_projector_state_dict'])\n",
        "                test_video_projector.load_state_dict(best_model_state_for_config['video_projector_state_dict'])\n",
        "                test_clf_head.load_state_dict(best_model_state_for_config['clf_head_state_dict'])\n",
        "                if best_model_state_for_config['bert_state_dict'] is not None:\n",
        "                     test_bert.load_state_dict(best_model_state_for_config['bert_state_dict'])\n",
        "                print(f\"Loaded best model from Epoch {best_model_state_for_config['epoch']+1} ({config_name}) for testing.\")\n",
        "                epoch_for_log = best_model_state_for_config['epoch']\n",
        "            else:\n",
        "                print(f\"No best validation model state found for {config_name}. Using final model from training for testing.\")\n",
        "                test_audio_hier_lstm.load_state_dict(current_audio_hier_lstm.state_dict())\n",
        "                test_video_hier_lstm.load_state_dict(current_video_hier_lstm.state_dict())\n",
        "                test_audio_projector.load_state_dict(current_audio_projector.state_dict())\n",
        "                test_video_projector.load_state_dict(current_video_projector.state_dict())\n",
        "                test_clf_head.load_state_dict(current_clf_head.state_dict())\n",
        "                test_bert.load_state_dict(bert_feature_extractor_global.state_dict()) # Use the state of the global BERT\n",
        "                epoch_for_log = NUM_EPOCHS - 1 if NUM_EPOCHS > 0 else None\n",
        "\n",
        "            test_loss, test_accuracy, test_f1= validate_or_test(\n",
        "                test_audio_hier_lstm,\n",
        "                test_video_hier_lstm,\n",
        "                test_audio_projector,\n",
        "                test_video_projector,\n",
        "                test_bert, # Pass the test_bert instance with loaded weights\n",
        "                test_clf_head,\n",
        "                test_loader,\n",
        "                bce_criterion,\n",
        "                DEVICE,\n",
        "                epoch=epoch_for_log,\n",
        "                num_epochs=NUM_EPOCHS, # or best_model_state_for_config['epoch']+1\n",
        "                current_modality_config=model_config,\n",
        "                tokenizer_for_padding=bert_tokenizer_global,\n",
        "                mode=\"Test\"\n",
        "            )\n",
        "            print(f\"Final test results for configuration {config_name} -> Avg BCE Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
        "            all_models_results[config_name] = {\n",
        "                'val_acc': best_val_accuracy_for_config,\n",
        "                'test_acc': test_accuracy, 'test_loss': test_loss,\n",
        "                'test_f1': test_f1,\n",
        "                'best_epoch': best_model_state_for_config['epoch']+1 if best_model_state_for_config['clf_head_state_dict'] is not None else (NUM_EPOCHS if NUM_EPOCHS > 0 else 'N/A')\n",
        "            }\n",
        "\n",
        "    print(\"\\n\\n--- Final Results Summary for All Model Configurations ---\")\n",
        "    for config_name, results in all_models_results.items():\n",
        "        print(f\"Configuration: {config_name}\")\n",
        "        print(f\"  Best Validation Accuracy (Epoch {results['best_epoch']}): {results['val_acc']:.4f}\")\n",
        "        print(f\"  Test Set F1 Score: {results['test_f1']:.4f}\")\n",
        "        print(f\"  Test Set Accuracy: {results['test_acc']:.4f}\")\n",
        "        print(f\"  Test Set Loss: {results['test_loss']:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    print(\"All operations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSUopWBTurNj",
        "outputId": "92214139-e26c-4e70-86df-2fc6491d93fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "MAX_BERT_LEN set to: 512\n",
            "Text LSTM_HIDDEN_SIZE set to: 256\n",
            "Hierarchical LSTM output dim (projector input): 512\n",
            "Projector output dim (fusion head AV input): 1024\n",
            "ATTENTION_TOKEN_DIM set to: 32\n",
            "NUM_ATTENTION_TOKENS_PER_MODAL set to: 16 (processed modality feature dimension: 512)\n",
            "Initializing BERT model and tokenizer...\n",
            "Freezing BERT parameters...\n",
            "BERT parameters frozen.\n",
            "Creating datasets...\n",
            "Creating data loaders with custom collate_fn...\n",
            "\n",
            "--- [1/1] Starting processing for model configuration: AVT (Active: ('audio', 'video', 'text')) ---\n",
            "Initializing model components for configuration AVT...\n",
            "Model components for AVT initialized.\n",
            "Starting training for AVT... Total 4 epochs, Batch size 16, Learning rate 8e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT) Train Avg Loss: 0.9848, BCE: 0.6578, SimCLR: 10.8991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (AVT) Val Avg BCE: 0.6267, Accuracy: 0.6480, F1: 0.6205\n",
            "Epoch 1 (AVT): New best validation accuracy: 0.6480.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT) Train Avg Loss: 0.9061, BCE: 0.6004, SimCLR: 10.1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (AVT) Val Avg BCE: 0.5992, Accuracy: 0.6776, F1: 0.6617\n",
            "Epoch 2 (AVT): New best validation accuracy: 0.6776.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Train Avg Loss: 0.8261, BCE: 0.5398, SimCLR: 9.5409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Val Avg BCE: 0.6024, Accuracy: 0.6857, F1: 0.6771\n",
            "Epoch 3 (AVT): New best validation accuracy: 0.6857.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT) Train Avg Loss: 0.6911, BCE: 0.4113, SimCLR: 9.3259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (AVT) Val Avg BCE: 0.6628, Accuracy: 0.6806, F1: 0.6667\n",
            "Training for configuration AVT complete. Best validation accuracy for this config: 0.6857 (at epoch 3)\n",
            "\n",
            "Starting testing phase for configuration AVT...\n",
            "Loaded best model from Epoch 3 (AVT) for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (AVT) Test Avg BCE: 0.5838, Accuracy: 0.6962, F1: 0.6899\n",
            "Final test results for configuration AVT -> Avg BCE Loss: 0.5838, Accuracy: 0.6962\n",
            "\n",
            "\n",
            "--- Final Results Summary for All Model Configurations ---\n",
            "Configuration: AVT\n",
            "  Best Validation Accuracy (Epoch 3): 0.6857\n",
            "  Test Set F1 Score: 0.6899\n",
            "  Test Set Accuracy: 0.6962\n",
            "  Test Set Loss: 0.5838\n",
            "------------------------------\n",
            "All operations complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}
